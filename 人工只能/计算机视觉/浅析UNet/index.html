<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>浅析UNet | sevenboy</title><meta name="keywords" content="人工只能,计算机视觉,语义分割"><meta name="author" content="sevenboy"><meta name="copyright" content="sevenboy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这是自己在学习计算机视觉，做语义分割的时候学习到的UNet的时候做的课程笔记，便于查看">
<meta property="og:type" content="article">
<meta property="og:title" content="浅析UNet">
<meta property="og:url" content="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/index.html">
<meta property="og:site_name" content="sevenboy">
<meta property="og:description" content="这是自己在学习计算机视觉，做语义分割的时候学习到的UNet的时候做的课程笔记，便于查看">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/11.jpg">
<meta property="article:published_time" content="2021-09-26T14:48:44.000Z">
<meta property="article:modified_time" content="2021-09-26T15:06:04.587Z">
<meta property="article:author" content="sevenboy">
<meta property="article:tag" content="人工只能">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="语义分割">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/11.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '浅析UNet',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-09-26 23:06:04'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/image/myself.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/11.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">sevenboy</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">浅析UNet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-26T14:48:44.000Z" title="发表于 2021-09-26 22:48:44">2021-09-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-26T15:06:04.587Z" title="更新于 2021-09-26 23:06:04">2021-09-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/">人工只能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="浅析UNet"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>研究一个深度学习算法，可以先看网络结构，看懂网络结构后，再Loss计算方法、训练方法等。本文主要针对UNet的网络结构进行讲解</p>
<p>卷积神经网络被大规模的应用在分类任务中，输出的结果是整个图像的类标签。但是UNet是像素级分类，输出的则是每个像素点的类别，且不同类别的像素会显示不同颜色，UNet常常用在生物医学图像上，而该任务中图片数据往往较少。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。</p>
<ul>
<li><strong>优点</strong><ul>
<li>输出结果可以定位出目标类别的位置；</li>
<li>由于输入的训练数据是patches，这样就相当于进行了数据增强，从而解决了生物医学图像数量少的问题，数据增强有利于模型的训练</li>
</ul>
</li>
<li><strong>缺点</strong><ul>
<li>训练过程较慢，网络必须训练每个patches，由于每个patches具有较多的重叠部分，这样持续训练patches，就会导致相当多的图片特征被多次训练，造成资源的浪费，导致训练时间加长且效率会低下。但是也会认为网络对这个特征进行多次训练，会对这个特征影响十分深刻，从而准确率得到改进。但是这里你拿一张图片复制100次去训练，很可能会出现过拟合的现象，对于这张图片确实十分敏感，但是拿另外一张图片来就可能识别不出了啦</li>
<li>定位准确性和获取上下文信息不可兼得，大的patches需要更多的max-pooling，这样会减少定位准确性，因为最大池化会丢失目标像素和周围像素之间的空间关系，而小patches只能看到很小的局部信息，包含的背景信息不够。</li>
</ul>
</li>
</ul>
<h4 id="网络结构原理"><a href="#网络结构原理" class="headerlink" title="网络结构原理"></a>网络结构原理</h4><p>UNet网络结构，最主要的两个特点是：U型网络结构和Skip Connection跳层连接。</p>
<p>UNet网络结构分为三个部分，原理图如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/1.jpg" alt=""></p>
<ul>
<li><p>第一部分是<strong>主干特征提取部分</strong>，我们可以利用<strong>主干部分</strong>获得一个又一个的<strong>特征层</strong>，Unet的主干特征提取部分与VGG相似，为卷积和最大池化的堆叠。<strong>利用主干特征提取部分我们可以获得五个初步有效特征层</strong>，在第二步中，我们会利用这五个有效特征层可以进行特征融合。</p>
<ul>
<li><p>下采样</p>
</li>
<li><p>左边特征提取网络：使用conv和pooling，就是每次向下采样之前都会进行两次的卷积操作，然后向下采样，然后再进行两次卷积操作，以此往复，向下连续采样五次</p>
</li>
</ul>
</li>
<li><p>第二部分是<strong>加强特征提取部分</strong>，我们可以利用主干部分获取到的<strong>五个初步有效特征层</strong>进行上采样，并且进行特征融合，获得一个最终的，融合了<strong>所有特征的有效特征层</strong>。</p>
<ul>
<li><p>上采样</p>
</li>
<li><p>右边网络为特征融合网络：使用上采样产生的特征图与左侧特征图进行concatenate操作</p>
</li>
<li><p>Skip Connection中间四条灰色的平行线，Skip Connection就是在上采样的过程中，融合下采样过过程中的feature map。Skip Connection用到的融合的操作也很简单，就是将feature map的通道进行叠加，俗称Concat。</p>
</li>
<li><p>Concat操作也很好理解，举个例子：一本大小为10cm<em>10cm，厚度为3cm的书A，和一本大小为10cm</em>10cm，厚度为4cm的书B。将书A和书B，边缘对齐地摞在一起。这样就得到了，大小为10cm*10cm厚度为7cm的一摞书（就是直接把书叠起来的意思）</p>
</li>
<li><p>对于feature map，一个大小为<strong>256 <em> 256 </em> 64</strong>的feature map，即feature map的w（宽）为256，h（高）为256，c（通道数）为64。和一个大小为<strong>256 <em> 256 </em> 32</strong>的feature map进行Concat融合，就会得到一个大小为<strong>256 <em> 256 </em> 96</strong>的feature map。</p>
<p>在实际使用中，Concat融合的两个feature map的大小不一定相同，例如<strong>256 <em> 256 </em> 64</strong>的feature map和<strong>240 <em> 240 </em> 32</strong>的feature map进行Concat。</p>
<p>这种时候，就有两种办法：</p>
<ul>
<li><p>第一种：将大<strong>256 <em> 256 </em> 64</strong>的feature map进行裁剪，裁剪为<strong>240 <em> 240 </em> 64</strong>的feature map，比如上下左右，各舍弃8 pixel，裁剪后再进行Concat，得到<strong>240 <em> 240 </em> 96</strong>的feature map。</p>
</li>
<li><p>第二种：将小<strong>240 <em> 240 </em> 32</strong>的feature map进行padding操作，padding为<strong>256 <em> 256 </em> 32</strong>的feature map，比如上下左右，各补8 pixel，padding后再进行Concat，得到<strong>256 <em> 256 </em> 96</strong>的feature map。</p>
</li>
</ul>
<p>UNet采用的Concat方案就是第二种，将小的feature map进行padding，padding的方式是补0，一种常规的常量填充。</p>
</li>
</ul>
</li>
<li><p>第三部分是<strong>预测部分</strong>，我们会利用<strong>最终获得的最后一个有效特征层</strong>对每一个特征点进行分类，相当于对每一个像素点进行分类。<strong>（将最后特征层调整通道数，也就是我们要分类个数）</strong></p>
<ul>
<li>最后再经过两次卷积操作，生成特征图，再用两个卷积核大小为<strong>1*1</strong>的卷积做分类得到最后的两张heatmap，例如第一张表示第一类的得分，第二张表示第二类的得分heatmap，然后作为softmax函数的输入，算出概率比较大的softmax，然后再进行loss，反向传播计算。</li>
</ul>
</li>
</ul>
<h4 id="网络代码实现"><a href="#网络代码实现" class="headerlink" title="网络代码实现"></a>网络代码实现</h4><p>按照UNet的网络结构分parts去实现Unet结构，<strong>采取一种搭积木的方式，先定义各个独立的模块，最后组合拼接就可以！</strong></p>
<h5 id="DoubleConv模块"><a href="#DoubleConv模块" class="headerlink" title="DoubleConv模块"></a>DoubleConv模块</h5><p>如下图所示模块，连续的两个卷积的操作，在整个UNet网络中，主干特征提取网络和加强特征网络中各自使用了五次，每一层都会采取这个操作，故可以提取出来：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/2.jpg" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class DoubleConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.double_conv(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><p>nn.Sequential 是一个时许的容器，会将里面的 modle 逐一执行，执行顺序为：<strong>卷积-&gt;BN-&gt;ReLU-&gt;卷积-&gt;BN-&gt;ReLU。</strong></p>
</li>
<li><p>in_channels, out_channels，输入输出通道定义为参数，增强扩展使用</p>
</li>
<li><p>卷积 nn.Conv2d 的输出：</p>
<ul>
<li><p><strong>nn. Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,dilation=1, groups=1, bias=True, padding_mode= ‘zeros’ )</strong></p>
<ul>
<li>in_channels:输入的四维张量[N, C, H, W]中的C，也就是说输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。</li>
<li>out_channels:也很好理解，即期望的四维输出张量的channels数，不再多说。</li>
<li>kernel_size:卷积核的大小，一般我们会使用5x5、3x3这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）。<br>stride = 1:卷积核在图像窗口上每次平移的间隔，即所谓的步长。这个概念和Tensorflow等其他框架没什么区别，不再多言。</li>
<li>padding:这是Pytorch与Tensorflow在卷积层实现上最大的差别，padding也就是指图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。需要注意的是这里的填充包括图像的上下左右，以padding=1为例，若原始图像大小为32 <em> 32，那么padding后的图像大小就变成了34 </em> 34，而不是33*33。<br>Pytorch不同于Tensorflow的地方在于，Tensorflow提供的是padding的模式，比如same、valid，且不同模式对应了不同的输出图像尺寸计算公式。而Pytorch则需要手动输入padding的数量，当然，Pytorch这种实现好处就在于输出图像尺寸计算公式是唯一的，</li>
<li>dilation:这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）。更形象和直观的图示可以观察Github上的Dilated convolution animations，展示了dilation=2的情况。</li>
<li>groups:决定了是否采用分组卷积，groups参数可以参考groups参数详解</li>
<li>bias:即是否要添加偏置参数作为可学习参数的一个，默认为True。</li>
<li>padding_mode:即padding的模式，默认采用零填充。</li>
</ul>
</li>
<li><p>输出通道就是 out_channels</p>
</li>
<li><p>输出的 <strong>X * X</strong> 计算公式：</p>
<script type="math/tex; mode=display">
O = （I - K + 2P）/ S +1</script></li>
</ul>
</li>
</ul>
<pre><code>- I 为输入feature map的大小，O为输出feature map的大小，K为卷积核的大小，P为padding的大小，S为步长
</code></pre><h5 id="Down（下采样模块）"><a href="#Down（下采样模块）" class="headerlink" title="Down（下采样模块）"></a>Down（下采样模块）</h5><p>UNet的下采样模块有着4次的下采样过程，过程如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/3.jpg" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Down(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.maxpool_conv(x)</span><br></pre></td></tr></table></figure>
<ul>
<li>代码很简单，就是一个maxpool池化层，进行下采样，然后接一个DoubleConv模块。</li>
<li>到这里，左边的网络完成！！</li>
</ul>
<h5 id="Up（上采样模块）"><a href="#Up（上采样模块）" class="headerlink" title="Up（上采样模块）"></a>Up（上采样模块）</h5><p>上采样模块就是出来常规的上采样操作以外，还需要进行特征融合，</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/4.jpg" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Up(nn.Module):</span><br><span class="line">   </span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)</span><br><span class="line">        self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    def forward(self, x1, x2):</span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        # input is CHW</span><br><span class="line">        diffY = x2.size()[2] - x1.size()[2]</span><br><span class="line">        diffX = x2.size()[3] - x1.size()[3]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,</span><br><span class="line">                        diffY // 2, diffY - diffY // 2])</span><br><span class="line">     </span><br><span class="line">        x = torch.cat([x2, x1], dim=1)</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>初始化函数里定义的上采样方法（反卷积）以及卷积采用DoubleConv</p>
</li>
<li><p>反卷积，顾名思义，就是反着卷积。卷积是让featuer map越来越小，反卷积就是让feature map越来越大，</p>
<p>下面蓝色为原始图片，周围白色的虚线方块为padding结果，通常为0，上面绿色为卷积后的图片。</p>
<p>这个示意图，就是一个从 <strong>2 * 2</strong>的feature map  —-&gt;  <strong>4 * 4 </strong>的feature map过程。</p>
<p>在forward前向传播函数中，x1接收的是<strong>上采样</strong>的数据，x2接收的是<strong>特征融合</strong>的数据。特征融合方法就是，上文提到的，先对小的feature map进行padding，再进行concat。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/6.gif" alt=""></p>
</li>
</ul>
<h5 id="OutConv模块"><a href="#OutConv模块" class="headerlink" title="OutConv模块"></a><strong>OutConv模块</strong></h5><p>用上述的DoubleConv模块、Down模块、Up模块就可以拼出UNet的主体网络结构了。UNet网络的输出需要根据分割数量，整合输出通道。</p>
<p>利用前面的模块，我们可以获取输入进来的图片的特征，此时，我们需要利用特征获得预测结果</p>
<p>利用特征获得预测结果的过程为：</p>
<ul>
<li><strong>利用一个1x1卷积进行通道调整，将最终特征层的通道数调整成num_classes。</strong>  <strong>（即对每一个像素点进行分类）</strong></li>
</ul>
<p>这个过程简单，顺便也包装一下吧</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class OutConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure>
<p>到这里，所有的积木已经完成了，接下来就是搭建的过程了。</p>
<h5 id="UNet模块"><a href="#UNet模块" class="headerlink" title="UNet模块"></a>UNet模块</h5><p>到这里，按照UNet网络结构，设置每个模块的输入输出通道个数以及调用顺序，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from nets.net_of_me.unet_parts import *</span><br><span class="line">class UNet(nn.Module):</span><br><span class="line">    def __init__(self, n_channels, n_classes, bilinear=False):</span><br><span class="line">        super(UNet, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, 64)</span><br><span class="line">        self.down1 = Down(64, 128)</span><br><span class="line">        self.down2 = Down(128, 256)</span><br><span class="line">        self.down3 = Down(256, 512)</span><br><span class="line">        self.down4 = Down(512, 1024)</span><br><span class="line">        self.up1 = Up(1024, 512, bilinear)</span><br><span class="line">        self.up2 = Up(512, 256, bilinear)</span><br><span class="line">        self.up3 = Up(256, 128, bilinear)</span><br><span class="line">        self.up4 = Up(128, 64, bilinear)</span><br><span class="line">        self.outc = OutConv(64, n_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        return logits</span><br></pre></td></tr></table></figure>
<h4 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练网络的代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.backends.cudnn as cudnn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">from nets.unet_training import CE_Loss, Dice_loss, LossHistory</span><br><span class="line">from utils.dataloader import DeeplabDataset, deeplab_dataset_collate</span><br><span class="line">from utils.metrics import f_score</span><br><span class="line"></span><br><span class="line">def get_lr(optimizer):</span><br><span class="line">    for param_group in optimizer.param_groups:</span><br><span class="line">        return param_group[&#x27;lr&#x27;]</span><br><span class="line"></span><br><span class="line">def fit_one_epoch(net,epoch,epoch_size,epoch_size_val,gen,genval,Epoch,cuda):</span><br><span class="line">    net = net.train()</span><br><span class="line">    total_loss = 0</span><br><span class="line">    total_f_score = 0</span><br><span class="line"></span><br><span class="line">    val_toal_loss = 0</span><br><span class="line">    val_total_f_score = 0</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(gen):</span><br><span class="line">        if iteration &gt;= epoch_size:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        #进行训练</span><br><span class="line">        outputs = net(imgs)</span><br><span class="line">        loss    = CE_Loss(outputs, pngs, num_classes = NUM_CLASSES)</span><br><span class="line">        if dice_loss:</span><br><span class="line">            main_dice = Dice_loss(outputs, labels)</span><br><span class="line">            loss      = loss + main_dice</span><br><span class="line"></span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">        waste_time = time.time() - start_time #训练epoch需要的时间</span><br><span class="line">        start_time = time.time()</span><br><span class="line"></span><br><span class="line">        if (iteration % 50 == 0):</span><br><span class="line">            print(&quot;epoch = &#123;&#125; and loss = &#123;&#125; and waste_time = &#123;&#125;&quot;.format(epoch,loss.item(),waste_time))</span><br><span class="line">            #写入日志文件</span><br><span class="line">            with open(&quot;log/train_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch,loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Training&#x27;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Validation&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(genval):</span><br><span class="line">        if iteration &gt;= epoch_size_val:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line">			# 开始训练</span><br><span class="line">            outputs = net(imgs)</span><br><span class="line">            #计算损失函数</span><br><span class="line">            val_loss = CE_Loss(outputs, pngs, num_classes=NUM_CLASSES)</span><br><span class="line">            if dice_loss:</span><br><span class="line">                main_dice = Dice_loss(outputs, labels)</span><br><span class="line">                val_loss = val_loss + main_dice</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">            val_toal_loss += val_loss.item()</span><br><span class="line">            val_total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">            if (iteration % 50 == 0):</span><br><span class="line">                print(&quot;epoch = &#123;&#125; and val_loss = &#123;&#125; &quot;.format(epoch, val_loss.item()))</span><br><span class="line">                # 写入日志文件</span><br><span class="line">                with open(&quot;log/val_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                    f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch, val_loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Validation&#x27;)</span><br><span class="line">    print(&#x27;Epoch:&#x27; + str(epoch + 1) + &#x27;/&#x27; + str(Epoch))</span><br><span class="line">    print(&#x27;Total Loss: %.4f || Val Loss: %.4f &#x27; % (total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line">    print(&#x27;Saving state, iter:&#x27;, str(epoch + 1))</span><br><span class="line">    torch.save(model.state_dict(), &#x27;model/Epoch%d-Total_Loss%.4f-%.4f.pth&#x27; % ((epoch + 1), total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    #------------------------------#</span><br><span class="line">    #   输入图片的大小</span><br><span class="line">    #------------------------------#</span><br><span class="line">    inputs_size = [512,512,3]</span><br><span class="line">    #---------------------#</span><br><span class="line">    #   分类个数+1</span><br><span class="line">    #   2+1</span><br><span class="line">    #---------------------#</span><br><span class="line">    NUM_CLASSES = 21</span><br><span class="line">    #   Cuda的使用</span><br><span class="line">    #-------------------------------#</span><br><span class="line">    Cuda = True</span><br><span class="line">    #linux服务器</span><br><span class="line">    dataset_path = &quot;/data/xwd/pro_datas/VOCdevkit/VOC2007&quot;</span><br><span class="line"></span><br><span class="line">    #网络</span><br><span class="line">    model = UNet(n_channels=inputs_size[-1], n_classes=NUM_CLASSES).train()</span><br><span class="line"></span><br><span class="line">    if Cuda:</span><br><span class="line">        net = torch.nn.DataParallel(model)</span><br><span class="line">        cudnn.benchmark = True</span><br><span class="line">        net = net.cuda()</span><br><span class="line"></span><br><span class="line">    # 打开训练数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/train.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        train_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    # 打开验证数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/val.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        val_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    if True:</span><br><span class="line">        lr = 1e-4</span><br><span class="line">        Init_Epoch = 0</span><br><span class="line">        Interval_Epoch = 5</span><br><span class="line">        Batch_size = 4</span><br><span class="line"></span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr)  #优化器</span><br><span class="line">        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.92) #学习率的调整</span><br><span class="line">        </span><br><span class="line">        #封装数据</span><br><span class="line">        train_dataset = DeeplabDataset(train_lines, inputs_size, NUM_CLASSES, True, dataset_path)</span><br><span class="line">        val_dataset = DeeplabDataset(val_lines, inputs_size, NUM_CLASSES, False, dataset_path)</span><br><span class="line">        gen = DataLoader(train_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                         drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line">        gen_val = DataLoader(val_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                             drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line"></span><br><span class="line">        epoch_size = len(train_lines) // Batch_size</span><br><span class="line">        epoch_size_val = len(val_lines) // Batch_size</span><br><span class="line"></span><br><span class="line">        if epoch_size == 0 or epoch_size_val == 0:</span><br><span class="line">            raise ValueError(&quot;数据集过小，无法进行训练，请扩充数据集。&quot;)</span><br><span class="line"></span><br><span class="line">        for epoch in range(Init_Epoch, Interval_Epoch):</span><br><span class="line">            fit_one_epoch(model, epoch, epoch_size, epoch_size_val, gen, gen_val, Interval_Epoch, Cuda)</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>封装数据集</strong>的办法主要采用：自定义类继承Dataset，下面展示的是他的伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># ================================================================== #</span><br><span class="line">#                Input pipeline for custom dataset                 #</span><br><span class="line"># ================================================================== </span><br><span class="line"># You should build your custom dataset as below.</span><br><span class="line">class CustomDataset(torch.utils.data.Dataset):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Initialize file paths or a list of file names. </span><br><span class="line">        pass</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span><br><span class="line">        # 2. Preprocess the data (e.g. torchvision.Transform).</span><br><span class="line">        # 3. Return a data pair (e.g. image and label).</span><br><span class="line">        pass</span><br><span class="line">    def __len__(self):</span><br><span class="line">        # You should change 0 to the total size of your dataset.</span><br><span class="line">        return 0 </span><br><span class="line"># You can then use the prebuilt data loader. </span><br><span class="line">custom_dataset = CustomDataset()</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,</span><br><span class="line">                                           batch_size=64, </span><br><span class="line">                                           shuffle=True)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>init</strong>函数是这个类的初始化函数，根据指定的图片路径，读取所有图片数据，</li>
<li><strong>len</strong>函数可以返回数据的多少，这个类实例化后，通过len()函数调用。</li>
<li><strong>getitem</strong>函数是数据获取函数，在这个函数里你可以写数据怎么读，怎么处理，并且可以一些数据预处理、数据增强都可以在这里进行</li>
</ul>
<p>下面的是自定义的这个方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">class DeeplabDataset(Dataset):</span><br><span class="line">    def __init__(self,train_lines,image_size,num_classes,random_data,dataset_path):</span><br><span class="line">        super(DeeplabDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.train_lines    = train_lines</span><br><span class="line">        self.train_batches  = len(train_lines)</span><br><span class="line">        self.image_size     = image_size</span><br><span class="line">        self.num_classes    = num_classes</span><br><span class="line">        self.random_data    = random_data</span><br><span class="line">        self.dataset_path   = dataset_path</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.train_batches</span><br><span class="line"></span><br><span class="line">    def rand(self, a=0, b=1):</span><br><span class="line">        return np.random.rand() * (b - a) + a</span><br><span class="line"></span><br><span class="line">    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=1.5, val=1.5):</span><br><span class="line">        label = Image.fromarray(np.array(label))</span><br><span class="line"></span><br><span class="line">        h, w = input_shape</span><br><span class="line">        # resize image</span><br><span class="line">        rand_jit1 = rand(1-jitter,1+jitter)</span><br><span class="line">        rand_jit2 = rand(1-jitter,1+jitter)</span><br><span class="line">        new_ar = w/h * rand_jit1/rand_jit2</span><br><span class="line"></span><br><span class="line">        scale = rand(0.5,1.5)</span><br><span class="line">        if new_ar &lt; 1:</span><br><span class="line">            nh = int(scale*h)</span><br><span class="line">            nw = int(nh*new_ar)</span><br><span class="line">        else:</span><br><span class="line">            nw = int(scale*w)</span><br><span class="line">            nh = int(nw/new_ar)</span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        label = label.resize((nw,nh), Image.NEAREST)</span><br><span class="line">        label = label.convert(&quot;L&quot;)</span><br><span class="line">        </span><br><span class="line">        # flip image or not</span><br><span class="line">        flip = rand()&lt;.5</span><br><span class="line">        if flip: </span><br><span class="line">            image = image.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">            label = label.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">        </span><br><span class="line">        # place image</span><br><span class="line">        dx = int(rand(0, w-nw))</span><br><span class="line">        dy = int(rand(0, h-nh))</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, (w,h), (128,128,128))</span><br><span class="line">        new_label = Image.new(&#x27;L&#x27;, (w,h), (0))</span><br><span class="line">        new_image.paste(image, (dx, dy))</span><br><span class="line">        new_label.paste(label, (dx, dy))</span><br><span class="line">        image = new_image</span><br><span class="line">        label = new_label</span><br><span class="line"></span><br><span class="line">        # distort image</span><br><span class="line">        hue = rand(-hue, hue)</span><br><span class="line">        sat = rand(1, sat) if rand()&lt;.5 else 1/rand(1, sat)</span><br><span class="line">        val = rand(1, val) if rand()&lt;.5 else 1/rand(1, val)</span><br><span class="line">        x = cv2.cvtColor(np.array(image,np.float32)/255, cv2.COLOR_RGB2HSV)</span><br><span class="line">        x[..., 0] += hue*360</span><br><span class="line">        x[..., 0][x[..., 0]&gt;1] -= 1</span><br><span class="line">        x[..., 0][x[..., 0]&lt;0] += 1</span><br><span class="line">        x[..., 1] *= sat</span><br><span class="line">        x[..., 2] *= val</span><br><span class="line">        x[x[:,:, 0]&gt;360, 0] = 360</span><br><span class="line">        x[:, :, 1:][x[:, :, 1:]&gt;1] = 1</span><br><span class="line">        x[x&lt;0] = 0</span><br><span class="line">        image_data = cv2.cvtColor(x, cv2.COLOR_HSV2RGB)*255</span><br><span class="line">        return image_data,label</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        if index == 0:</span><br><span class="line">            shuffle(self.train_lines)  </span><br><span class="line">        annotation_line = self.train_lines[index]</span><br><span class="line">        name = annotation_line.split()[0]</span><br><span class="line">        # 从文件中读取图像</span><br><span class="line">        jpg = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;JPEGImages&quot;), name + &quot;.jpg&quot;))</span><br><span class="line">        png = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;SegmentationClass&quot;), name + &quot;.png&quot;))</span><br><span class="line"></span><br><span class="line">        if self.random_data:</span><br><span class="line">            jpg, png = self.get_random_data(jpg,png,(int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        else:</span><br><span class="line">            jpg, png = letterbox_image(jpg, png, (int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        png = np.array(png)</span><br><span class="line">        png[png &gt;= self.num_classes] = self.num_classes</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        #   转化成one_hot的形式</span><br><span class="line">        #   在这里需要+1是因为voc数据集有些标签具有白边部分</span><br><span class="line">        #   我们需要将白边部分进行忽略，+1的目的是方便忽略。</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        seg_labels = np.eye(self.num_classes+1)[png.reshape([-1])]</span><br><span class="line">        seg_labels = seg_labels.reshape((int(self.image_size[1]),int(self.image_size[0]),self.num_classes+1))</span><br><span class="line">        jpg = np.transpose(np.array(jpg),[2,0,1])/255</span><br><span class="line">        return jpg, png, seg_labels</span><br></pre></td></tr></table></figure>
<p>我这边设置的epoch并不算很大，采用3090的显卡也是运行了一段时间是时间，可以看到网络，loss实在逐渐在收敛的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/7.jpg" alt=""></p>
<p>采用训练好的模型进行预测，看看结果如何：</p>
<p>这边采用的是在网络上copy的图片预处理和后续处理的代码，本人目前对图片处理还是比较菜，把别人的代码贴在这里，最后给出自己的预测结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line">import colorsys</span><br><span class="line">import copy</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from PIL import Image</span><br><span class="line">from torch import nn</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">#-------------------------------------------#</span><br><span class="line">#   使用自己训练好的模型预测需要修改2个参数</span><br><span class="line">#   model_path和num_classes都需要修改！</span><br><span class="line">#   如果出现shape不匹配</span><br><span class="line">#   一定要注意训练时的model_path和num_classes数的修改</span><br><span class="line">#--------------------------------------------#</span><br><span class="line">class Unet(object):</span><br><span class="line">    _defaults = &#123;</span><br><span class="line">        &quot;model_path&quot;        : &#x27;model\Epoch2-Total_Loss1.0039-0.8573.pth&#x27;, #保存的训练模型的路径</span><br><span class="line">        &quot;model_image_size&quot;  : (512, 512, 3), #输入图片的大小</span><br><span class="line">        &quot;num_classes&quot;       : 21, </span><br><span class="line">        &quot;cuda&quot;              : True,</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        #   blend参数用于控制是否</span><br><span class="line">        #   让识别结果和原图混合</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        &quot;blend&quot;             : True</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   初始化UNET</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        self.__dict__.update(self._defaults)</span><br><span class="line">        self.device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">        self.generate()</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   获得所有的分类</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def generate(self):</span><br><span class="line">        self.net = UNet(n_channels=self.model_image_size[-1],n_classes=self.num_classes).eval()</span><br><span class="line"></span><br><span class="line">        # 加载本地的模型参数</span><br><span class="line">        state_dict = torch.load(self.model_path,self.device)</span><br><span class="line">        self.net.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line">        if self.cuda:</span><br><span class="line">            self.net = nn.DataParallel(self.net) #可以调用多个GPU，帮助加速训练</span><br><span class="line">            self.net = self.net.to(self.device)</span><br><span class="line"></span><br><span class="line">        print(&#x27;&#123;&#125; model loaded.&#x27;.format(self.model_path))</span><br><span class="line"></span><br><span class="line">        if self.num_classes &lt;= 21:</span><br><span class="line">            self.colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),                              (0, 128, 128),  (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), </span><br><span class="line">                           (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), </span><br><span class="line">                           (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64,128),(128, 64, 12)]</span><br><span class="line">        else:</span><br><span class="line">            # 画框设置不同的颜色</span><br><span class="line">            hsv_tuples = [(x / len(self.class_names), 1., 1.)</span><br><span class="line">                        for x in range(len(self.class_names))]</span><br><span class="line">            self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))</span><br><span class="line">            self.colors = list(</span><br><span class="line">                map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),</span><br><span class="line">                    self.colors))</span><br><span class="line"></span><br><span class="line">    def letterbox_image(self ,image, size):</span><br><span class="line">        image = image.convert(&quot;RGB&quot;)</span><br><span class="line">        iw, ih = image.size</span><br><span class="line">        w, h = size</span><br><span class="line">        scale = min(w/iw, h/ih)</span><br><span class="line">        nw = int(iw*scale)</span><br><span class="line">        nh = int(ih*scale)</span><br><span class="line"></span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, size, (128,128,128))</span><br><span class="line">        new_image.paste(image, ((w-nw)//2, (h-nh)//2))</span><br><span class="line">        return new_image,nw,nh</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   检测图片，处理图片</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def detect_image(self, image):</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        image = image.convert(&#x27;RGB&#x27;)</span><br><span class="line">        </span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   对输入图像进行一个备份，后面用于绘图</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        old_img = copy.deepcopy(image)</span><br><span class="line"></span><br><span class="line">        orininal_h = np.array(image).shape[0]</span><br><span class="line">        orininal_w = np.array(image).shape[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   进行不失真的resize，添加灰条，进行图像归一化</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        image, nw, nh = self.letterbox_image(image,(self.model_image_size[1],self.model_image_size[0]))</span><br><span class="line">        a = np.array(image).shape</span><br><span class="line">        images = [np.array(image)/255]</span><br><span class="line"></span><br><span class="line">        images = np.transpose(images,(0,3,1,2))</span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   图片传入网络进行预测</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            images = torch.from_numpy(images).type(torch.FloatTensor) #转化为tensor</span><br><span class="line">            if self.cuda:</span><br><span class="line">                #images =images.cuda()</span><br><span class="line">                images = images.cpu()</span><br><span class="line"></span><br><span class="line">            pr = self.net(images)</span><br><span class="line"></span><br><span class="line">            pr = pr[0]</span><br><span class="line">            pr1 = pr[1]</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            #   取出每一个像素点的种类</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy().argmax(axis=-1)</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            #   将灰条部分截取掉</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            pr = pr[int((self.model_image_size[0]-nh)//2):int((self.model_image_size[0]-nh)//2+nh), </span><br><span class="line">                    int((self.model_image_size[1]-nw)//2):int((self.model_image_size[1]-nw)//2+nw)]</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   创建一副新图，并根据每个像素点的种类赋予颜色</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        seg_img = np.zeros((np.shape(pr)[0],np.shape(pr)[1],3))</span><br><span class="line">        for c in range(self.num_classes):</span><br><span class="line">            seg_img[:,:,0] += ((pr[:,: ] == c )*( self.colors[c][0] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,1] += ((pr[:,: ] == c )*( self.colors[c][1] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,2] += ((pr[:,: ] == c )*( self.colors[c][2] )).astype(&#x27;uint8&#x27;)</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片转换成Image的形式</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        image = Image.fromarray(np.uint8(seg_img)).resize((orininal_w,orininal_h))</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片和原图片混合</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        if self.blend:</span><br><span class="line">             image = Image.blend(old_img,image,0.7)      </span><br><span class="line">        return image,  old_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    unet = Unet()</span><br><span class="line">    while True:</span><br><span class="line">        img = input(&#x27;Input image filename:&#x27;)</span><br><span class="line">        try:</span><br><span class="line">            image = Image.open(img)</span><br><span class="line">        except:</span><br><span class="line">            print(&#x27;Open Error! Try again!&#x27;)</span><br><span class="line">            continue</span><br><span class="line">        else:</span><br><span class="line">            r_image,old_image= unet.detect_image(image)</span><br><span class="line">            old_image.show()</span><br><span class="line">            r_image.show()</span><br></pre></td></tr></table></figure>
<p>调用这个函数，得到的预测结果如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/8.jpg" alt=""></p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/9.jpg" alt=""></p>
<h4 id="语义分割的MIOU指标"><a href="#语义分割的MIOU指标" class="headerlink" title="语义分割的MIOU指标"></a>语义分割的MIOU指标</h4><p>语义分割的标准度量。其计算所有类别交集和并集之比的平均值.，语义分割说到底也还是一个分割任务，既然是一个分割的任务，预测的结果往往就是四种情况：</p>
<ul>
<li><p>true positive（TP）：预测正确, 预测结果是正类, 真实是正类 </p>
</li>
<li><p>false positive（FP）：预测错误, 预测结果是正类, 真实是负类</p>
</li>
<li><p>true negative（TN）：预测错误, 预测结果是负类, 真实是正类</p>
</li>
<li><p>false negative（FN）：预测正确, 预测结果是负类, 真实是负类  </p>
</li>
</ul>
<p>mIOU 的定义：计算真实值和预测值两个集合的交集和并集之比。这个比例可以变形为TP（交集）比上TP、FP、FN之和（并集）。即：mIOU=TP/(FP+FN+TP)。</p>
<p>计算公式：</p>
<script type="math/tex; mode=display">
MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{p_{ii}}{\sum_{j=0}^kp_{ij} + \sum_{j=0}^kp_{ji} - p_{ii}}</script><p>等价于：</p>
<script type="math/tex; mode=display">
MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{TP}{FN+FP+TP}</script><p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是基于全局的评价。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/10.jpg" alt=""></p>
<p><code>MIoU</code>：计算两圆交集（橙色部分）与两圆并集（红色+橙色+黄色）之间的比例，理想情况下两圆重合，比例为1。</p>
<p>计算本网络的MIoU可以采样训练好的模型进行计算，计算的结果比例越接近1效果越好。</p>
<p>代码实现后续把，hhhhhhhhhhhhhhhhhhhhh。。。。。。。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">sevenboy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/">https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sevenboy.online" target="_blank">sevenboy</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/">人工只能</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">语义分割</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/11.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SOTR-Segmenting-Objects-with-Transformers</div></div></a></div><div class="next-post pull-right"><a href="/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/14.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计算机视觉的基本知识介绍</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/人工只能/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/" title="simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/9.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-08</div><div class="title">simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/Loss-in-Deep-Learning/" title="Loss_in_Deep_Learning"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-22</div><div class="title">Loss_in_Deep_Learning</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/Metric-in-Semantic-Segmentation/" title="Metric_in_Semantic_Segmentation"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230326/head.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-25</div><div class="title">Metric_in_Semantic_Segmentation</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/Regularization-Summary/" title="Regularization_Summary"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230329/head1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-29</div><div class="title">Regularization_Summary</div></div></a></div><div><a href="/人工只能/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/" title="SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-03</div><div class="title">SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers</div></div></a></div><div><a href="/人工只能/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/" title="SOTR-Segmenting-Objects-with-Transformers"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-27</div><div class="title">SOTR-Segmenting-Objects-with-Transformers</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">网络结构原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">网络代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#DoubleConv%E6%A8%A1%E5%9D%97"><span class="toc-number">3.1.</span> <span class="toc-text">DoubleConv模块</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Down%EF%BC%88%E4%B8%8B%E9%87%87%E6%A0%B7%E6%A8%A1%E5%9D%97%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">Down（下采样模块）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Up%EF%BC%88%E4%B8%8A%E9%87%87%E6%A0%B7%E6%A8%A1%E5%9D%97%EF%BC%89"><span class="toc-number">3.3.</span> <span class="toc-text">Up（上采样模块）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#OutConv%E6%A8%A1%E5%9D%97"><span class="toc-number">3.4.</span> <span class="toc-text">OutConv模块</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#UNet%E6%A8%A1%E5%9D%97"><span class="toc-number">3.5.</span> <span class="toc-text">UNet模块</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text">训练网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84MIOU%E6%8C%87%E6%A0%87"><span class="toc-number">5.</span> <span class="toc-text">语义分割的MIOU指标</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/11.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2023 By sevenboy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">this is a sunshine body</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://cdn.jsdelivr.net/gh/lete114/CDN/Sum/sakura.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="阳光,向上,好学,积极,热爱,奋斗,拼搏,追求,奋发" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":180,"height":330},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>