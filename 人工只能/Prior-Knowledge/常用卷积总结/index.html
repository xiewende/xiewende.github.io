<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>常用卷积总结 | sevenboy</title><meta name="keywords" content="常规卷积, 分组卷积, 深度可分离卷积, 转置卷积, 空洞卷积"><meta name="author" content="sevenboy"><meta name="copyright" content="sevenboy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这是常用卷积的简单总结">
<meta property="og:type" content="article">
<meta property="og:title" content="常用卷积总结">
<meta property="og:url" content="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="sevenboy">
<meta property="og:description" content="这是常用卷积的简单总结">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/images.png">
<meta property="article:published_time" content="+022023-08-07T13:25:38.000Z">
<meta property="article:modified_time" content="2023-08-07T13:53:34.227Z">
<meta property="article:author" content="sevenboy">
<meta property="article:tag" content="人工只能">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="卷积">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/images.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '常用卷积总结',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2023-08-07 21:53:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/image/myself.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/images.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">sevenboy</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">常用卷积总结</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="+022023-08-07T13:25:38.000Z" title="发表于 22023-08-07 21:25:38">22023-08-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-07T13:53:34.227Z" title="更新于 2023-08-07 21:53:34">2023-08-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/">人工只能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/">Prior Knowledge</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="常用卷积总结"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h4 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h4><ul>
<li><p>输入：$H_{in} \times W_{in} \times C_{in}$，其中 $H_{in}$ 为输入 feature map的高，$W_{in}$ 为宽，$C_{in}$ 为通道数</p>
</li>
<li><p>输出：$H_{out} \times W_{out} \times C_{out}$，其中 $H_{out}$ 为输入 feature map的高，$W_{out}$ 为宽，$C_{out}$ 为通道数</p>
</li>
<li><p>卷积核：$N \times K \times K \times C_k$ ，其中 N 为该卷积层的卷积核个数，$K$ 为卷积核宽与高(默认相等)，$C_k$ 为卷积核通道数</p>
</li>
</ul>
<h4 id="常规卷积"><a href="#常规卷积" class="headerlink" title="常规卷积"></a>常规卷积</h4><p><strong>特点：</strong></p>
<ul>
<li>卷积和通道数与输入 feature map的通道数相等，即 $C_{in} = C_k$</li>
<li>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</li>
</ul>
<p><strong>卷积过程：</strong></p>
<p>卷积核在输入 feature map 中移动，按位点乘后求和即可，通道也会求和。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/1.gif" style="zoom: 50%;" /></p>
<p><strong>函数语法格式，二维卷积最常用的卷积方式，先实例化再使用。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn. Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode= &#x27;zeros&#x27; )</span><br></pre></td></tr></table></figure>
<ul>
<li><p>参数解释</p>
<ul>
<li><strong>in_channels</strong> ：输入的四维张量[N, C, H, W]中的C，也就是说输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。</li>
<li><strong>out_channels</strong>：即是期望的输出四维张量的channels数。</li>
<li><strong>kernel_size</strong> ：卷积核的大小，一般我们会使用 5x5、3x3 这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个<strong>tuple</strong>，而不能写一个列表（list）。</li>
<li><strong>stride = 1</strong>： 卷积核在图像窗口上每次平移的间隔，即所谓的步长。跟Tensorflow框架等的意义一样。</li>
<li><strong>padding=0</strong>：padding也就是指图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。<strong>需要注意的是这里的填充包括图像的上下左右</strong>，以padding=1为例，若原始图像大小为<strong>32*32</strong>，那么padding后的图像大小就变成了 <strong>34*34</strong>，而不是<strong>33*33</strong>。这是Pytorch与Tensorflow在卷积层实现上最大的差别。</li>
<li><strong>dilation=1</strong>：这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）</li>
<li><strong>groups=1</strong>：决定了是否采用分组卷积，默认值为 1 .</li>
<li><strong>bias=True</strong> ：即是否要添加偏置参数作为可学习参数的一个，默认为True。</li>
<li><strong>padding_mode</strong> ：即padding的模式，默认采用零填充。</li>
</ul>
</li>
<li><p>输出图像的计算公式</p>
<ul>
<li><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/4.png" style="zoom:80%;" /></p>
</li>
<li><p>在大多数情况下，<strong><em>大多数情况下的</em> kernel_size、padding</strong>左右两数均相同，且不采用空洞卷积（dilation默认为1），因此只需要记住这种在深度学习课程里学过的公式就好了。</p>
<script type="math/tex; mode=display">
O = \frac{I-K+2P}{S} + 1</script></li>
</ul>
</li>
</ul>
<h4 id="1X1卷积"><a href="#1X1卷积" class="headerlink" title="1X1卷积"></a>1X1卷积</h4><p><strong>特点、作用：</strong></p>
<ul>
<li><p>顾名思义，卷积核大小为 1ｘ1</p>
</li>
<li><p>卷积核通道数与输入 feature map 的通道数相等，即 $C_{in} = C_k$</p>
</li>
<li><p>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</p>
</li>
<li><p>不改变 feature map 的大小，目的是为了改变 channel 数，即 1ｘ1 卷积的使用场景是：不想改变输入 feature map 的宽高，但想改变它的通道数。即可以用于升维或降维。</p>
</li>
<li><p>相比 3ｘ3 等卷积，计算量及参数量都更小，计算量和参数量的计算参考另一篇文章 (22_CNN网络各种层的FLOPs和参数量paras计算)</p>
</li>
<li><p><strong>加入非线性</strong>。1*1的卷积在不同 channels 上进行线性整合，在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/2.png" style="zoom:80%;" /></p>
<p>1x1核的主要目的是应用非线性。在神经网络的每一层之后，我们都可以应用一个激活层。无论是ReLU、PReLU、Sigmoid还是其他，与卷积层不同，激活层是非线性的。非线性层扩展了模型的可能性，这也是通常使“深度”网络优于“宽”网络的原因。为了<strong>在不显著增加参数和计算量的情况下增加非线性层的数量，我们可以应用一个1x1内核并在它之后添加一个激活层。这有助于给网络增加一层深度</strong></p>
<h4 id="分组卷积（Group-Convolution）"><a href="#分组卷积（Group-Convolution）" class="headerlink" title="分组卷积（Group Convolution）"></a>分组卷积（Group Convolution）</h4><p>Group convolution 分组卷积，最早在 AlexNet 中出现，由于当时的硬件资源有限，训练 AlexNet 时卷积操作不能全部放在同一个 GPU 处理，因此作者把 feature maps 分给多个GPU分别进行处理，最后把多个 GPU 的结果进行融合。</p>
<p><strong>卷积过程</strong></p>
<p>​    将输入 feature map 分成 g 组，一个卷积核也相对应地分成 g 组，在对应的组内做卷积。（我们可以理解成分组卷积中使用的 g 组卷积核整体对应于常规卷积中的一个卷积核，只不过是将常规卷积中的一个卷积核分成了 g 组而已）</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/3.png" alt=""></p>
<p><strong>特点、作用：</strong></p>
<ul>
<li><p>输入的 feature map 尺寸：$H_{in}<em>W_{in}</em> \frac{C_{in}}{g}$ ，共有 g 组</p>
</li>
<li><p>卷积核的规格：$N<em>K</em>K<em>\frac{C_{k}}{g}$，共有 N </em> g 组</p>
</li>
<li><p>输出 feature map 规格：$H_{out}<em>W_{out}</em>N<em>g$ ，共生成 N</em>g 个 feature map</p>
</li>
<li><p>当 g=1 时就退化成了上面讲过的常规卷积，当 $g=C_{in}$ 时就是我们下面将要讲述的深度分离卷积。</p>
</li>
<li><p>用常规卷积得到一个输出 feature map 的计算量和参数量便可以得到 g 个输出 feature map，所以分组卷积常用在轻量型高效网络中，因为它可以用少量的参数量和计算量生成大量的 feature map。</p>
</li>
<li><p>优点</p>
<ul>
<li>标准2D卷积参数量：$W \times H \times C_{in} \times C_k$</li>
<li><p>分组卷积参数量：$W \times H \times C_{in}/2 \times C_k/2 \times 2$</p>
</li>
<li><p>group=2,参数量减少到原来的1/2；group=4,参数量减少到原来的1/4；<strong>总结：参数量减少1/g。</strong> </p>
</li>
<li>减少运算量和参数量，相同输入输出大小的情况下，减少为原来的 1/g </li>
</ul>
</li>
<li><p>代码的话很简单，就是 nn.Conv2d 里面的一个参数：group。</p>
</li>
</ul>
<h4 id="可分离卷据（Separable-Convolution）"><a href="#可分离卷据（Separable-Convolution）" class="headerlink" title="可分离卷据（Separable Convolution）"></a>可分离卷据（Separable Convolution）</h4><h5 id="空间可分离卷积"><a href="#空间可分离卷积" class="headerlink" title="空间可分离卷积"></a>空间可分离卷积</h5><p>之所以命名为空间可分离卷积，是因为它主要处理的是卷积核的空间维度：宽度和高度。</p>
<p>空间可分离卷积简单地将卷积核划分为两个较小的卷积核。 最常见的情况是将3x3的卷积核划分为3x1和1x3的卷积核，如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/4.jpg" alt=""></p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/5.jpg" alt=""></p>
<ul>
<li><p><strong>局限性</strong>：并不是所有的卷积核都可以“分离”成两个较小的卷积核，==能够“分离”的是那些卷积核参数大小的行和列有一定倍数关系的==. 这在训练期间变得特别麻烦，因为网络可能采用所有可能的卷积核，它最终只能使用可以分成两个较小卷积核的一小部分。所以实际中用的不多</p>
</li>
<li><p><strong>参数量和计算量更少</strong>：如上图所示，不是用9次乘法进行一次卷积，而是进行两次卷积，每次3次乘法（总共6次），以达到相同的效果。 乘法较少，计算复杂性下降，网络运行速度更快。</p>
</li>
</ul>
<h5 id="深度可分离卷积（Depthwise-Separable-Convolution）"><a href="#深度可分离卷积（Depthwise-Separable-Convolution）" class="headerlink" title="深度可分离卷积（Depthwise Separable Convolution）"></a>深度可分离卷积（Depthwise Separable Convolution）</h5><p>深度可分离卷积的过程分为两个部分：<strong>深度卷积（depthwise convolution）和逐点卷积（pointwise convolution）</strong></p>
<p><strong>（1）深度卷积</strong></p>
<p>深度卷积意在保持输入 feature map 的通道数，即对 feature map 中的<strong>每个通道使用一个规格为 $K<em>K</em>1$ 的卷积核进行卷积，于是输入 feature map 有多少个通道就有多少个这样的卷积核</strong>，深度卷积结束后得到的输出的通道数与输入的相等。</p>
<p>Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积，这个过程产生的feature map通道数和输入的通道数完全一样。</p>
<p>这一步其实就相当于常规卷积中的一个卷积核，只不过不同通道的卷积结果不相加而已，自己体会体会。</p>
<p>Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。<strong>而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/6.png" style="zoom:80%;" /></p>
<p><strong>（2） 逐点卷积</strong></p>
<p>在上一步的基础上，运用 1ｘ1 卷积进行逐点卷积。</p>
<p>使用一个 1ｘ1 卷积核就可以得到输出 feature map 一维的结果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/7.png" style="zoom:80%;" /></p>
<p>如果你要输出 feature map 有 256 维，那么就使用 256 个 1ｘ1 卷积核即可。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/8.png" style="zoom:80%;" /></p>
<ul>
<li><strong>可以理解成常规的卷积分成了两步执行，但是分成两步后参数量和计算量大大减少，网络运行更快</strong></li>
<li>深度分离卷积几乎是构造轻量高效模型的必用结构，如Xception, MobileNet, MobileNet V2, ShuffleNet, ShuffleNet V2, CondenseNet等轻量型网络结构中的必用结构。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/9.png" style="zoom:80%;" /></p>
<h4 id="转置卷积（Transposed-Convolution）"><a href="#转置卷积（Transposed-Convolution）" class="headerlink" title="转置卷积（Transposed Convolution）"></a>转置卷积（Transposed Convolution）</h4><p><strong>转置卷积（Transposed Convolution）</strong> 在语义分割或者对抗神经网络（GAN）中比较常见，<strong>其主要作用就是做上采样。</strong></p>
<ul>
<li><strong>转置卷积不是卷积的逆运算、不是逆运算、不是逆运算（重要的事情说三遍）</strong></li>
<li>转置卷积也是卷积</li>
</ul>
<p>函数语法格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, bias=True)</span><br></pre></td></tr></table></figure>
<p>概述：就是<strong>反卷积</strong>，该函数是用来进行转置卷积的，它主要做了这几件事：<strong>首先</strong>，对输入的feature map进行padding操作，得到新的feature map；<strong>然后</strong>，随机初始化一定尺寸的卷积核；<strong>最后</strong>，用随机初始化的一定尺寸的卷积核在新的feature map上进行卷积操作。卷积核确实是随机初始的，但是后续可以对卷积核进行单独的修改</p>
<p>主要作用就是起到上采样的作用。但转置卷积不是卷积的逆运算（一般卷积操作是不可逆的），它只能恢复到原来的大小（shape），数值与原来不同。转置卷积的运算步骤可以归为以下几步：</p>
<ul>
<li>1、在输入特征图元素间填充 <strong>stride-1 行列 0</strong>（其中 stride 表示转置卷积的步距）</li>
<li><p>2、在输入特征图四周填充 <strong>k-p-1</strong> <strong>行列0</strong>（其中k表示转置卷积的kernel_size大小，p为转置卷积的padding，注意这里的padding和卷积操作中有些不同）</p>
</li>
<li><p>3、将卷积核参数上下、左右翻转 </p>
</li>
<li>4、做正常卷积运算（填充0，步距1）</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/12.png" style="zoom:80%;" /></p>
<p>下图展示了转置卷积中不同 stride 和 padding 的情况：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/16.png" style="zoom: 67%;" /></p>
<p><strong>输出尺寸计算：</strong></p>
<script type="math/tex; mode=display">
output = (input-1) * stride + output\_padding - 2*padding + kernel_size</script><p>不过时常 output_padding=0,</p>
<script type="math/tex; mode=display">
output = (input-1) * stride - 2*padding + kernel_size</script><h4 id="空洞卷积（Dilated-Convolution）"><a href="#空洞卷积（Dilated-Convolution）" class="headerlink" title="空洞卷积（Dilated Convolution）"></a>空洞卷积（Dilated Convolution）</h4><p><strong>空洞卷积也叫扩张卷积或者膨胀卷积，简单来说就是在卷积核元素之间加入一些空格(零)来扩大卷积核的过程。</strong></p>
<p>空洞卷积诞生在图像分割领域，在一般的卷积结构中因为存在 pooling 操作，目的是增大感受野也增加非线性等，但是 pooling 之后特征图的大小减半，而图像分割是 pixel-wise 的，因此后续需要 upsamplng 将变小的特征图恢复到原始大小，这里的 <strong>upsampling 主要是通过转置卷积完成</strong>，但是经过这么多的操作之后会将很多细节丢失，<strong>那么空洞卷积就是来解决这个的，既扩大了感受野，又不用 pooling</strong> 。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/10.png" style="zoom:50%;" /></p>
<p>假设以一个变量a来衡量空洞卷积的扩张系数，则加入空洞之后的实际卷积核尺寸与原始卷积核尺寸之间的关系：<strong>K = K + (k-1)(a-1)</strong></p>
<p>其中<strong>k为原始卷积核大小，a为卷积扩张率(dilation rate)</strong>，K为经过扩展后实际卷积核大小。除此之外，空洞卷积的卷积方式跟常规卷积一样。我们用一个扩展率a来表示卷积核扩张的程度。比如说<strong>a=1,2,4</strong>的时候卷积核核感受野如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/11.jpg" alt=""></p>
<p>在这张图像中，3×3 的红点表示经过卷积后，输出图像是 3×3 像素。尽管所有这三个扩张卷积的输出都是同一尺寸，但模型观察到的感受野有很大的不同。当a=1，原始卷积核size为3 <em> 3，就是常规卷积。a=2时，加入空洞之后的卷积核：size=3+(3-1) </em> (2-1)=5，对应的感受野可计算为：(2 ^(a+2))-1=7。a=3时，卷积核size可以变化到3+(3-1)(4-1)=9，感受野则增长到 (2 ^(a+2))-1=15。<strong>有趣的是，与这些操作相关的参数的数量是相等的。我们「观察」更大的感受野不会有额外的成本</strong>。因此，扩张卷积可用于廉价地增大输出单元的感受野，而不会增大其核大小，这在多个扩张卷积彼此堆叠时尤其有效。</p>
<ul>
<li><strong>扩大感受野</strong>：一般来说，在深度神经网络中增加感受野并且减少计算量的方法是下采样。但是下采样牺牲了空间分辨率和一些输入的信息。空洞卷积一方面增大了感受野可以检测分割大目标，另一方面相较于下采样增大了分辨率可以精确定位目标。</li>
<li><p><strong>捕获多尺度上下文信息</strong>：当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。</p>
</li>
<li><p>代码实现就是控制  nn.Conv2d 里面的一个参数：dilation 。</p>
</li>
</ul>
<h4 id="可变形卷积（Deformable-Convolution）"><a href="#可变形卷积（Deformable-Convolution）" class="headerlink" title="可变形卷积（Deformable Convolution）"></a>可变形卷积（Deformable Convolution）</h4><p><strong>要解决的问题：</strong>传统卷积，只能是死板的正方形感受野，不能定义任意形状的感受野，但感受野的形状不限制更能提取有效信息</p>
<p><strong>目的：使得卷积的感受野</strong>通过训练<strong>可以自适应调整</strong>。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">sevenboy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/">https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sevenboy.online" target="_blank">sevenboy</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/">人工只能</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF/">卷积</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/images.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/image1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">图像处理算法总结</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/人工只能/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/" title="simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/9.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-08</div><div class="title">simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/Loss-in-Deep-Learning/" title="Loss_in_Deep_Learning"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-22</div><div class="title">Loss_in_Deep_Learning</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/Metric-in-Semantic-Segmentation/" title="Metric_in_Semantic_Segmentation"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230326/head.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-25</div><div class="title">Metric_in_Semantic_Segmentation</div></div></a></div><div><a href="/人工只能/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/" title="SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-03</div><div class="title">SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers</div></div></a></div><div><a href="/人工只能/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/" title="SOTR-Segmenting-Objects-with-Transformers"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-27</div><div class="title">SOTR-Segmenting-Objects-with-Transformers</div></div></a></div><div><a href="/人工只能/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/" title="TransFuse:Fusing_Transformers_and_CNNs_for_Medical_Image_Segmentation"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-03</div><div class="title">TransFuse:Fusing_Transformers_and_CNNs_for_Medical_Image_Segmentation</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E7%BA%A6%E5%AE%9A"><span class="toc-number">1.</span> <span class="toc-text">符号约定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%84%E5%8D%B7%E7%A7%AF"><span class="toc-number">2.</span> <span class="toc-text">常规卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1X1%E5%8D%B7%E7%A7%AF"><span class="toc-number">3.</span> <span class="toc-text">1X1卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%EF%BC%88Group-Convolution%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">分组卷积（Group Convolution）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E6%8D%AE%EF%BC%88Separable-Convolution%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">可分离卷据（Separable Convolution）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.1.</span> <span class="toc-text">空间可分离卷积</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF%EF%BC%88Depthwise-Separable-Convolution%EF%BC%89"><span class="toc-number">5.2.</span> <span class="toc-text">深度可分离卷积（Depthwise Separable Convolution）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88Transposed-Convolution%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">转置卷积（Transposed Convolution）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%EF%BC%88Dilated-Convolution%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">空洞卷积（Dilated Convolution）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%EF%BC%88Deformable-Convolution%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">可变形卷积（Deformable Convolution）</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/images.png')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2023 By sevenboy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">this is a sunshine body</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://cdn.jsdelivr.net/gh/lete114/CDN/Sum/sakura.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="阳光,向上,好学,积极,热爱,奋斗,拼搏,追求,奋发" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":180,"height":330},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>