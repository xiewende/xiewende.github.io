<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>SOTR-Segmenting-Objects-with-Transformers | sevenboy</title><meta name="keywords" content="人工只能,计算机视觉,paper,ICCV"><meta name="author" content="sevenboy"><meta name="copyright" content="sevenboy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这是自己在研究生阶段读的第一篇顶会论文，记录一下">
<meta property="og:type" content="article">
<meta property="og:title" content="SOTR-Segmenting-Objects-with-Transformers">
<meta property="og:url" content="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/index.html">
<meta property="og:site_name" content="sevenboy">
<meta property="og:description" content="这是自己在研究生阶段读的第一篇顶会论文，记录一下">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg">
<meta property="article:published_time" content="2021-10-26T16:03:49.000Z">
<meta property="article:modified_time" content="2021-12-03T02:42:33.197Z">
<meta property="article:author" content="sevenboy">
<meta property="article:tag" content="人工只能">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="Semantic Segmentation(ICCV)">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SOTR-Segmenting-Objects-with-Transformers',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-12-03 10:42:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/image/myself.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">sevenboy</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">SOTR-Segmenting-Objects-with-Transformers</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-26T16:03:49.000Z" title="发表于 2021-10-27 00:03:49">2021-10-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-03T02:42:33.197Z" title="更新于 2021-12-03 10:42:33">2021-12-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/">人工只能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/">papers(ICCV)</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="SOTR-Segmenting-Objects-with-Transformers"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="SOTR-Segmenting-Objects-with-Transformers"><a href="#SOTR-Segmenting-Objects-with-Transformers" class="headerlink" title="SOTR: Segmenting Objects with Transformers"></a>SOTR: Segmenting Objects with Transformers</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/first.png" alt=""></p>
<ul>
<li><p>代码：<a target="_blank" rel="noopener" href="https://github.com/easton-cau/SOTR">https://github.com/easton-cau/SOTR</a></p>
</li>
<li><p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.06747">https://arxiv.org/abs/2108.06747</a> </p>
</li>
</ul>
<h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><ul>
<li><p>首先作者研究了实例分割的发展历程，以及各个阶段方法的大概总结，提出个各个阶段的优点与不足的，在实例分割这块，主要的方法就是包括</p>
<p>Top-down instance segmentation 和 Bottom-up instance segmentation</p>
<ul>
<li><strong>Top-down instance segmentation</strong><ul>
<li><strong>proposal-based</strong>方法：基于目标检测，在得到目标检测框之后再在框内做语义分割分割前景背景，由于这种方法需要借助目标检测中的区域提议，因此该方法称为<strong>proposal-based</strong>方法.，这种方法就是遵循先检测后分割的范式的。缺点如下 例如 Mak-RCNN<ul>
<li>1）由于有限的感受野，CNN在高级视觉语义信息中相对缺乏 特征的连贯性来关联实例 , 导致对大对象的次优结果；</li>
<li>2）分割质量和推理速度都严重依赖对象检测 器，在复杂场景中性能较差。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Bottom-up instance segmentation</strong></p>
<ul>
<li><strong>proposal-free方法</strong>：在语义分割图的基础上，将像素聚集到不同的实例上。这类方法的基本思想是利用CNN学到每个像素实例级的特征，接着用一种聚合方法将像素聚合成实例。这种方法通常分两步，一个是分割，一个是聚合。语义分割图获得之后，将像素一步步的聚合到不同的实例中。学习每像素嵌入(per-pixel embedding)和实例感知 特征(instance-aware features)，然后使用后处理技术依次分组，根据嵌入特征(embedding characteristics)将它们转换为实例，只要缺点如下<ul>
<li>不稳定的聚类（例如碎片化和联合掩码）以及对不同场景数据集的泛化能力差，当场景非常复杂并且一张图像中存在密集的物体时，背景像素上不可避免地 会损失大量的计算和时间</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>STOR</strong>：就是一种 (Bottom-up) 自底向上的model模型，也就是基于<strong>proposal-free方法</strong>，有效地学习了位置敏感特征(position-sensitive features )，动态生成实例掩码 (dynamically generates instance masks )，无需后处理分组以及边界框位置和尺度的界限，<strong>SOTR</strong> 以图像为输入，结合 CNN 和 Transformer 模块来 提取特征，并直接对类概率和实例掩码进行预测。</p>
</li>
</ul>
</li>
<li><p><strong>Transformer</strong></p>
<ul>
<li><p>近几年来，由于 Transformer 的崛起，其在cv领域的重要性可想而知，在cv领域上，很多人试图完全 替代 卷积运算 或 将类 CNN 架构与transformer结合用于视觉任务中的特征提取，它可以轻松捕 获全局范围特征(global-range characteristics)并自然地对长距离语义依赖项进行建模</p>
<ul>
<li>self-attention，作为transformers 的关键机制，广泛地聚 合了来自整个输入域的特征和位置信息。因此基于transformer的模型可以更好地区分具有相同语义类 别的重叠实例，这使得它们比CNN更适合高级视觉任务。但是他也有不足：<ul>
<li>（1）典型的attention在提取<strong>(low-level features)</strong>方低级特征方面表现不佳，导致对小对象的错误预测</li>
<li>（2）由于广泛的特征图(feature map)，需要大量的内存和时间，尤其是在训练阶段</li>
<li><strong>（1）的问题可以通过结合 CNN 主干得到有效解决</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>为了降低传统 self-attention 的内存和计算复杂度，我们提出了 <strong>twin attention</strong>（双注意力），一种替代的自注意 力自回归块，通过将全局空间 attention (注意力) 分解为独立的垂直和水平 attention (注意力) 来显着 减少计算和内存。与原始 Transformer 相比，这种精心设计的架构在计算和内存方面具有显着的节省，尤其是在用于密集预测（如实例分割）的大输入上。</p>
</li>
</ul>
</li>
<li><p>我们提出了一种创新的自底向上模型(bottom-up model)，称为 <strong>SOTR</strong>，巧妙地结 合了 CNN 和 Transformer 的优点。具体地说，我们采用Transformer模 型来获 取全局依赖关系并提取高级特征(<strong>high-level features</strong>) 以用于后续的函数头部（<strong>Functional heads</strong>）的预 测。其简化了分割管道，建 立在附加了两个并行子任务的替代 CNN 主干上：（1）通过transformer预测每个实例的类别和（2） 动态生成具有多个的(segmentation mask)分割掩码级上采样模块。 SOTR 可以分别通过特征金字塔 网络 (FPN) 和<strong>twin Transformer</strong>有效地提取较低级别的特征表示并捕获远程上下文依赖关系</p>
</li>
</ul>
<h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><p><strong>SOTR 是一种 CNN-transformer 混合实例分割模型</strong>，可以同时学习 2D 平面信息表示并轻松捕获远程信息。 它遵循直接分割范式，首先将输入特征图划分为     <strong>patches</strong> （补丁），然后在动态分割每个实例的同时预测每个<strong>patches</strong> （补丁）的类别。 具体来说，我们的模型主要由三部分组成：1）从输入图像中提取图像特征的主干，尤其是低级和局部特征，2）一个用于建模全局和语义依赖关系的 <strong>Transformer</strong>  ，它附加了功能头以 分别预测每个<strong>patches</strong> （补丁）的类别和卷积核，以及 3) 一个多级上采样模块，通过在生成特征图和相应的卷积核之间执行动态卷积操作来生成最终的分割掩码(segmentation mask)。 总体框架如<strong>图 2</strong> 所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-2.jpg" alt=""></p>
<p>SOTR 可以分别通过特征金字塔网络 (FPN) 和双Transformer有效地提取较低级别的特征表示并捕获远程上下文依赖关系。同时，与原始Transformer相比，所提出的双Transformer具有时间和资源效率，因为只涉及一行和一列注意力来编码像素。</p>
<ul>
<li><strong>twin attention（双注意力）</strong>机制，用稀疏表示来简化 <strong>attention(注意力)</strong> 矩阵。 我们的策略主要将感受野限制为固定步幅的设计块模式。 它首先计算每列中的 <strong>attention(注意力)</strong>，同时保持不同列中的元素独立。 该策略可以在水平尺度上聚合元素之间的上下文信息（见图 3（1））。 然后，在每一行内执行类似的 <strong>attention(注意力) </strong>以充分利用垂直尺度上的特征交互（如图 3（2）所示）。 两个尺度中的注意力依次连接为最后一个尺度，具有全局感受野，涵盖了两个维度的信息</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-3.png" alt=""></p>
<ul>
<li><strong>作者称之为 Twins</strong>，代码写的很复杂，但其实就是提前将行维度和列维度分别整合到前一个维度再输入 Attention 模块。这样做可以将时间复杂度降低为：</li>
</ul>
<script type="math/tex; mode=display">
O((H \times W)^2) \quad to  \quad O(H \times W^2 + W \times H^2)^1</script><ul>
<li><strong>Functional heads</strong></li>
</ul>
<p>来自 <strong>Transformer</strong> 模块的特征图（feature maps）被输入到不同的<strong>函数头（functional heads）</strong>以进行后续预测。 <strong>类头(class head)</strong> 包括 单个线性层(linear)以输出 $N ×N ×M $ 的分类结果，<strong>其中 M 是类的数量</strong>。 由于每个patch(补丁) 只为一个中心落入patch(补丁) 的单个对象分配一个类别，如 YOLO [32]，我们利用多级预测并在不同特征级别共享头部，以进一步提高不同尺度对象的模型性能和效率 . <strong>核头(kernel head)</strong> 也由一个线性层(linear)组成，与 <strong>类头(class head)</strong>并行输出一个 $N×N×D$ 张量用于后续的 掩码(mask) 生成，其中张量表示具有D个参数的 $ N×N $ 卷积核。 在训练期间，<strong>Focal Loss [26]</strong> 被应用于分类，而这些卷积核的所有监督都来自最终的掩码(mask) 损失。</p>
<ul>
<li><strong>Mask</strong></li>
</ul>
<p>为了构建实例感知和位置敏感分割的掩码特征表示，一种直接的方法是对不同尺度的每个特征图进行预测（[36, 12] 等）。 但是，它会增加时间和资源。 受 <strong>Panoptic FPN [22]</strong> 的启发，我们设计了<strong>多级上采样模块(multi-level upsampling module)</strong>，将来自每个 <strong>FPN 级</strong>和 <strong>transformer</strong> 的特征合并为一个统一的掩码特征。 首先，从 <strong>transformer</strong>模块获得带有位置信息的相对低分辨率特征图P5，并结合<strong>FPN中的P2-P4</strong>执行融合。 对于每个尺度的特征图，操作了 3×3 Conv、Group Norm [39] 和 ReLU 的几个阶段。 然后P3-P5被双线性上采样 2×、4×、8×，分别为  $(\frac{H}{4},\frac{W}{4})$分辨率。 最后，将处理后的 P2-P5 加在一起后，执行逐点卷积和上采样以创建最终统一的 $ H×W$ 特征图。</p>
<p>例如掩码(mask)预测，SOTR 通过对上述统一特征图执行动态卷积运算为每个 patch(补丁) 生成掩码(mask)。 给定来自内核头部(kernel head)的预测卷积核  $K \quad \epsilon \quad K^{N \times N \times D} $  ，每个内核负责相应 patch(补丁)中实例的掩码(mask)生成。 具体操作可以表示如下：</p>
<script type="math/tex; mode=display">
Z^{H \times W \times N^2} = F^{H \times W \times C} * K^{N \times N  \times D}</script><p>其中 <strong>*</strong> 表示卷积操作，Z是最终生成的掩码，维度为 $ H×W×N^2 $。 需要注意的是，D 的取值取决于卷积核的形状，即D等于$λ^2C$，其中 <strong>λ</strong> 为核大小。 最终的实例分割掩码可以由 <strong>Matrix NMS [37]</strong> 生成，每个掩码都由 <strong>Dice Loss [30]</strong> 独立监督。，</p>
<ul>
<li><p>代码的具体实现可以查看github上提供的源代码。参考知乎老哥的讲解：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424036708">https://zhuanlan.zhihu.com/p/424036708</a> </p>
</li>
</ul>
<h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul>
<li>We introduce an innovative CNN-transformer-hybrid instance segmentation  framework, termed SOTR. It can effectively model local connectivity and  longrange dependencies leveraging CNN backbone and transformer encoder in the  input domain to make them highly expressive. What’s more, SOTR considerably  streamlines the overall pipeline by directly segmenting object instances without  relying on box detection. </li>
<li>We devise the twin attention, a new  position-sensitive self-attention mechanism, which is tailored for our  transformer. This well-designed architecture enjoys a significant saving in  computation and memory compared with original transformer, especially on large  inputs for a dense prediction like instance segmentation. </li>
<li>Apart from pure  transformer based models, the proposed SOTR does not need to be pre-trained on  large datasets to generalize inductive biases well. Thus, SOTR is easier applied  to insufficient amounts of data. </li>
<li>The performance of SOTR achieves 40.2% of AP  with the ResNet-101-FPN backbone on the MS COCO benchmark, outperforming most of  state-of-the-art approaches in accuracy. Furthermore, SOTR demonstrates  significantly better performance on medium (59.0%) and large objects (73.0%),  thanks to the extraction of global information by twin transformer.</li>
</ul>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul>
<li>定量的结果：SOTR 在 MS COCO 数据集上表现良好，并超越了最先进的实例分割方法。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-7.jpg" alt=""></p>
<ul>
<li>定性结果：比起其他的分割方法，SOTR具有较好的分割性能</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-4.jpg" alt=""></p>
<ul>
<li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li>
</ul>
<h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>rethingking的话，我觉得，现在transformer那么火爆的时刻，本文其实相对简单，但是其的创新点很突出，很新颖。面对某些具体的任务，了解现在主流的方法并且找出他们各自的优点与不足，然后主要针对这些不足提出新的解决方案。在transformer广泛应用于cv领域后，找到transformer在cv领域后的不足之处，就比如提取low-level farture map特征和计算量内存上面存在不足，所以本文也是比较针对这个方面进行了研究，所以最后才可以提出一种创新的分割框架SOTR</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">sevenboy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/">https://sevenboy.online/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sevenboy.online" target="_blank">sevenboy</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/">人工只能</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/Semantic-Segmentation-ICCV/">Semantic Segmentation(ICCV)</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/9.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer</div></div></a></div><div class="next-post pull-right"><a href="/%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/11.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">浅析UNet</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/人工只能/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/" title="simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/9.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-08</div><div class="title">simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/Loss-in-Deep-Learning/" title="Loss_in_Deep_Learning"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-22</div><div class="title">Loss_in_Deep_Learning</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/Metric-in-Semantic-Segmentation/" title="Metric_in_Semantic_Segmentation"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230326/head.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-25</div><div class="title">Metric_in_Semantic_Segmentation</div></div></a></div><div><a href="/人工只能/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/" title="SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-03</div><div class="title">SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers</div></div></a></div><div><a href="/人工只能/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/" title="TransFuse:Fusing_Transformers_and_CNNs_for_Medical_Image_Segmentation"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-03</div><div class="title">TransFuse:Fusing_Transformers_and_CNNs_for_Medical_Image_Segmentation</div></div></a></div><div><a href="/人工只能/Prior-Knowledge/常用Attention总结/" title="常用Attention总结"><img class="cover" src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230808/image12.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-07</div><div class="title">常用Attention总结</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#SOTR-Segmenting-Objects-with-Transformers"><span class="toc-number">1.</span> <span class="toc-text">SOTR: Segmenting Objects with Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction"><span class="toc-number">1.1.</span> <span class="toc-text">introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Methods-and-Creativity"><span class="toc-number">1.2.</span> <span class="toc-text">Methods and Creativity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#contribution"><span class="toc-number">1.3.</span> <span class="toc-text">contribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments"><span class="toc-number">1.4.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rethingking"><span class="toc-number">1.5.</span> <span class="toc-text">Rethingking</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2023 By sevenboy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">this is a sunshine body</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://cdn.jsdelivr.net/gh/lete114/CDN/Sum/sakura.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="阳光,向上,好学,积极,热爱,奋斗,拼搏,追求,奋发" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":180,"height":330},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>