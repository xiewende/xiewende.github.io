<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>常用卷积总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h4 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h4><ul><li><p>输入：$H_{in} \times W_{in} \times C_{in}$，其中 $H_{in}$ 为输入 feature map的高，$W_{in}$ 为宽，$C_{in}$ 为通道数</p></li><li><p>输出：$H_{out} \times W_{out} \times C_{out}$，其中 $H_{out}$ 为输入 feature map的高，$W_{out}$ 为宽，$C_{out}$ 为通道数</p></li><li><p>卷积核：$N \times K \times K \times C_k$ ，其中 N 为该卷积层的卷积核个数，$K$ 为卷积核宽与高(默认相等)，$C_k$ 为卷积核通道数</p></li></ul><h4 id="常规卷积"><a href="#常规卷积" class="headerlink" title="常规卷积"></a>常规卷积</h4><p><strong>特点：</strong></p><ul><li>卷积和通道数与输入 feature map的通道数相等，即 $C_{in} = C_k$</li><li>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</li></ul><p><strong>卷积过程：</strong></p><p>卷积核在输入 feature map 中移动，按位点乘后求和即可，通道也会求和。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/1.gif" style="zoom: 50%;" /></p><p><strong>函数语法格式，二维卷积最常用的卷积方式，先实例化再使用。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn. Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode= &#x27;zeros&#x27; )</span><br></pre></td></tr></table></figure><ul><li><p>参数解释</p><ul><li><strong>in_channels</strong> ：输入的四维张量[N, C, H, W]中的C，也就是说输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。</li><li><strong>out_channels</strong>：即是期望的输出四维张量的channels数。</li><li><strong>kernel_size</strong> ：卷积核的大小，一般我们会使用 5x5、3x3 这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个<strong>tuple</strong>，而不能写一个列表（list）。</li><li><strong>stride = 1</strong>： 卷积核在图像窗口上每次平移的间隔，即所谓的步长。跟Tensorflow框架等的意义一样。</li><li><strong>padding=0</strong>：padding也就是指图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。<strong>需要注意的是这里的填充包括图像的上下左右</strong>，以padding=1为例，若原始图像大小为<strong>32*32</strong>，那么padding后的图像大小就变成了 <strong>34*34</strong>，而不是<strong>33*33</strong>。这是Pytorch与Tensorflow在卷积层实现上最大的差别。</li><li><strong>dilation=1</strong>：这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）</li><li><strong>groups=1</strong>：决定了是否采用分组卷积，默认值为 1 .</li><li><strong>bias=True</strong> ：即是否要添加偏置参数作为可学习参数的一个，默认为True。</li><li><strong>padding_mode</strong> ：即padding的模式，默认采用零填充。</li></ul></li><li><p>输出图像的计算公式</p><ul><li><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/4.png" style="zoom:80%;" /></p></li><li><p>在大多数情况下，<strong><em>大多数情况下的</em> kernel_size、padding</strong>左右两数均相同，且不采用空洞卷积（dilation默认为1），因此只需要记住这种在深度学习课程里学过的公式就好了。</p><script type="math/tex; mode=display">O = \frac{I-K+2P}{S} + 1</script></li></ul></li></ul><h4 id="1X1卷积"><a href="#1X1卷积" class="headerlink" title="1X1卷积"></a>1X1卷积</h4><p><strong>特点、作用：</strong></p><ul><li><p>顾名思义，卷积核大小为 1ｘ1</p></li><li><p>卷积核通道数与输入 feature map 的通道数相等，即 $C_{in} = C_k$</p></li><li><p>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</p></li><li><p>不改变 feature map 的大小，目的是为了改变 channel 数，即 1ｘ1 卷积的使用场景是：不想改变输入 feature map 的宽高，但想改变它的通道数。即可以用于升维或降维。</p></li><li><p>相比 3ｘ3 等卷积，计算量及参数量都更小，计算量和参数量的计算参考另一篇文章 (22_CNN网络各种层的FLOPs和参数量paras计算)</p></li><li><p><strong>加入非线性</strong>。1*1的卷积在不同 channels 上进行线性整合，在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/2.png" style="zoom:80%;" /></p><p>1x1核的主要目的是应用非线性。在神经网络的每一层之后，我们都可以应用一个激活层。无论是ReLU、PReLU、Sigmoid还是其他，与卷积层不同，激活层是非线性的。非线性层扩展了模型的可能性，这也是通常使“深度”网络优于“宽”网络的原因。为了<strong>在不显著增加参数和计算量的情况下增加非线性层的数量，我们可以应用一个1x1内核并在它之后添加一个激活层。这有助于给网络增加一层深度</strong></p><h4 id="分组卷积（Group-Convolution）"><a href="#分组卷积（Group-Convolution）" class="headerlink" title="分组卷积（Group Convolution）"></a>分组卷积（Group Convolution）</h4><p>Group convolution 分组卷积，最早在 AlexNet 中出现，由于当时的硬件资源有限，训练 AlexNet 时卷积操作不能全部放在同一个 GPU 处理，因此作者把 feature maps 分给多个GPU分别进行处理，最后把多个 GPU 的结果进行融合。</p><p><strong>卷积过程</strong></p><p>​    将输入 feature map 分成 g 组，一个卷积核也相对应地分成 g 组，在对应的组内做卷积。（我们可以理解成分组卷积中使用的 g 组卷积核整体对应于常规卷积中的一个卷积核，只不过是将常规卷积中的一个卷积核分成了 g 组而已）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/3.png" alt=""></p><p><strong>特点、作用：</strong></p><ul><li><p>输入的 feature map 尺寸：$H_{in}<em>W_{in}</em> \frac{C_{in}}{g}$ ，共有 g 组</p></li><li><p>卷积核的规格：$N<em>K</em>K<em>\frac{C_{k}}{g}$，共有 N </em> g 组</p></li><li><p>输出 feature map 规格：$H_{out}<em>W_{out}</em>N<em>g$ ，共生成 N</em>g 个 feature map</p></li><li><p>当 g=1 时就退化成了上面讲过的常规卷积，当 $g=C_{in}$ 时就是我们下面将要讲述的深度分离卷积。</p></li><li><p>用常规卷积得到一个输出 feature map 的计算量和参数量便可以得到 g 个输出 feature map，所以分组卷积常用在轻量型高效网络中，因为它可以用少量的参数量和计算量生成大量的 feature map。</p></li><li><p>优点</p><ul><li>标准2D卷积参数量：$W \times H \times C_{in} \times C_k$</li><li><p>分组卷积参数量：$W \times H \times C_{in}/2 \times C_k/2 \times 2$</p></li><li><p>group=2,参数量减少到原来的1/2；group=4,参数量减少到原来的1/4；<strong>总结：参数量减少1/g。</strong> </p></li><li>减少运算量和参数量，相同输入输出大小的情况下，减少为原来的 1/g </li></ul></li><li><p>代码的话很简单，就是 nn.Conv2d 里面的一个参数：group。</p></li></ul><h4 id="可分离卷据（Separable-Convolution）"><a href="#可分离卷据（Separable-Convolution）" class="headerlink" title="可分离卷据（Separable Convolution）"></a>可分离卷据（Separable Convolution）</h4><h5 id="空间可分离卷积"><a href="#空间可分离卷积" class="headerlink" title="空间可分离卷积"></a>空间可分离卷积</h5><p>之所以命名为空间可分离卷积，是因为它主要处理的是卷积核的空间维度：宽度和高度。</p><p>空间可分离卷积简单地将卷积核划分为两个较小的卷积核。 最常见的情况是将3x3的卷积核划分为3x1和1x3的卷积核，如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/4.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/5.jpg" alt=""></p><ul><li><p><strong>局限性</strong>：并不是所有的卷积核都可以“分离”成两个较小的卷积核，==能够“分离”的是那些卷积核参数大小的行和列有一定倍数关系的==. 这在训练期间变得特别麻烦，因为网络可能采用所有可能的卷积核，它最终只能使用可以分成两个较小卷积核的一小部分。所以实际中用的不多</p></li><li><p><strong>参数量和计算量更少</strong>：如上图所示，不是用9次乘法进行一次卷积，而是进行两次卷积，每次3次乘法（总共6次），以达到相同的效果。 乘法较少，计算复杂性下降，网络运行速度更快。</p></li></ul><h5 id="深度可分离卷积（Depthwise-Separable-Convolution）"><a href="#深度可分离卷积（Depthwise-Separable-Convolution）" class="headerlink" title="深度可分离卷积（Depthwise Separable Convolution）"></a>深度可分离卷积（Depthwise Separable Convolution）</h5><p>深度可分离卷积的过程分为两个部分：<strong>深度卷积（depthwise convolution）和逐点卷积（pointwise convolution）</strong></p><p><strong>（1）深度卷积</strong></p><p>深度卷积意在保持输入 feature map 的通道数，即对 feature map 中的<strong>每个通道使用一个规格为 $K<em>K</em>1$ 的卷积核进行卷积，于是输入 feature map 有多少个通道就有多少个这样的卷积核</strong>，深度卷积结束后得到的输出的通道数与输入的相等。</p><p>Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积，这个过程产生的feature map通道数和输入的通道数完全一样。</p><p>这一步其实就相当于常规卷积中的一个卷积核，只不过不同通道的卷积结果不相加而已，自己体会体会。</p><p>Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。<strong>而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/6.png" style="zoom:80%;" /></p><p><strong>（2） 逐点卷积</strong></p><p>在上一步的基础上，运用 1ｘ1 卷积进行逐点卷积。</p><p>使用一个 1ｘ1 卷积核就可以得到输出 feature map 一维的结果。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/7.png" style="zoom:80%;" /></p><p>如果你要输出 feature map 有 256 维，那么就使用 256 个 1ｘ1 卷积核即可。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/8.png" style="zoom:80%;" /></p><ul><li><strong>可以理解成常规的卷积分成了两步执行，但是分成两步后参数量和计算量大大减少，网络运行更快</strong></li><li>深度分离卷积几乎是构造轻量高效模型的必用结构，如Xception, MobileNet, MobileNet V2, ShuffleNet, ShuffleNet V2, CondenseNet等轻量型网络结构中的必用结构。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/9.png" style="zoom:80%;" /></p><h4 id="转置卷积（Transposed-Convolution）"><a href="#转置卷积（Transposed-Convolution）" class="headerlink" title="转置卷积（Transposed Convolution）"></a>转置卷积（Transposed Convolution）</h4><p><strong>转置卷积（Transposed Convolution）</strong> 在语义分割或者对抗神经网络（GAN）中比较常见，<strong>其主要作用就是做上采样。</strong></p><ul><li><strong>转置卷积不是卷积的逆运算、不是逆运算、不是逆运算（重要的事情说三遍）</strong></li><li>转置卷积也是卷积</li></ul><p>函数语法格式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, bias=True)</span><br></pre></td></tr></table></figure><p>概述：就是<strong>反卷积</strong>，该函数是用来进行转置卷积的，它主要做了这几件事：<strong>首先</strong>，对输入的feature map进行padding操作，得到新的feature map；<strong>然后</strong>，随机初始化一定尺寸的卷积核；<strong>最后</strong>，用随机初始化的一定尺寸的卷积核在新的feature map上进行卷积操作。卷积核确实是随机初始的，但是后续可以对卷积核进行单独的修改</p><p>主要作用就是起到上采样的作用。但转置卷积不是卷积的逆运算（一般卷积操作是不可逆的），它只能恢复到原来的大小（shape），数值与原来不同。转置卷积的运算步骤可以归为以下几步：</p><ul><li>1、在输入特征图元素间填充 <strong>stride-1 行列 0</strong>（其中 stride 表示转置卷积的步距）</li><li><p>2、在输入特征图四周填充 <strong>k-p-1</strong> <strong>行列0</strong>（其中k表示转置卷积的kernel_size大小，p为转置卷积的padding，注意这里的padding和卷积操作中有些不同）</p></li><li><p>3、将卷积核参数上下、左右翻转 </p></li><li>4、做正常卷积运算（填充0，步距1）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/12.png" style="zoom:80%;" /></p><p>下图展示了转置卷积中不同 stride 和 padding 的情况：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/16.png" style="zoom: 67%;" /></p><p><strong>输出尺寸计算：</strong></p><script type="math/tex; mode=display">output = (input-1) * stride + output\_padding - 2*padding + kernel_size</script><p>不过时常 output_padding=0,</p><script type="math/tex; mode=display">output = (input-1) * stride - 2*padding + kernel_size</script><h4 id="空洞卷积（Dilated-Convolution）"><a href="#空洞卷积（Dilated-Convolution）" class="headerlink" title="空洞卷积（Dilated Convolution）"></a>空洞卷积（Dilated Convolution）</h4><p><strong>空洞卷积也叫扩张卷积或者膨胀卷积，简单来说就是在卷积核元素之间加入一些空格(零)来扩大卷积核的过程。</strong></p><p>空洞卷积诞生在图像分割领域，在一般的卷积结构中因为存在 pooling 操作，目的是增大感受野也增加非线性等，但是 pooling 之后特征图的大小减半，而图像分割是 pixel-wise 的，因此后续需要 upsamplng 将变小的特征图恢复到原始大小，这里的 <strong>upsampling 主要是通过转置卷积完成</strong>，但是经过这么多的操作之后会将很多细节丢失，<strong>那么空洞卷积就是来解决这个的，既扩大了感受野，又不用 pooling</strong> 。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/10.png" style="zoom:50%;" /></p><p>假设以一个变量a来衡量空洞卷积的扩张系数，则加入空洞之后的实际卷积核尺寸与原始卷积核尺寸之间的关系：<strong>K = K + (k-1)(a-1)</strong></p><p>其中<strong>k为原始卷积核大小，a为卷积扩张率(dilation rate)</strong>，K为经过扩展后实际卷积核大小。除此之外，空洞卷积的卷积方式跟常规卷积一样。我们用一个扩展率a来表示卷积核扩张的程度。比如说<strong>a=1,2,4</strong>的时候卷积核核感受野如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/11.jpg" alt=""></p><p>在这张图像中，3×3 的红点表示经过卷积后，输出图像是 3×3 像素。尽管所有这三个扩张卷积的输出都是同一尺寸，但模型观察到的感受野有很大的不同。当a=1，原始卷积核size为3 <em> 3，就是常规卷积。a=2时，加入空洞之后的卷积核：size=3+(3-1) </em> (2-1)=5，对应的感受野可计算为：(2 ^(a+2))-1=7。a=3时，卷积核size可以变化到3+(3-1)(4-1)=9，感受野则增长到 (2 ^(a+2))-1=15。<strong>有趣的是，与这些操作相关的参数的数量是相等的。我们「观察」更大的感受野不会有额外的成本</strong>。因此，扩张卷积可用于廉价地增大输出单元的感受野，而不会增大其核大小，这在多个扩张卷积彼此堆叠时尤其有效。</p><ul><li><strong>扩大感受野</strong>：一般来说，在深度神经网络中增加感受野并且减少计算量的方法是下采样。但是下采样牺牲了空间分辨率和一些输入的信息。空洞卷积一方面增大了感受野可以检测分割大目标，另一方面相较于下采样增大了分辨率可以精确定位目标。</li><li><p><strong>捕获多尺度上下文信息</strong>：当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。</p></li><li><p>代码实现就是控制  nn.Conv2d 里面的一个参数：dilation 。</p></li></ul><h4 id="可变形卷积（Deformable-Convolution）"><a href="#可变形卷积（Deformable-Convolution）" class="headerlink" title="可变形卷积（Deformable Convolution）"></a>可变形卷积（Deformable Convolution）</h4><p><strong>要解决的问题：</strong>传统卷积，只能是死板的正方形感受野，不能定义任意形状的感受野，但感受野的形状不限制更能提取有效信息</p><p><strong>目的：使得卷积的感受野</strong>通过训练<strong>可以自适应调整</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 卷积 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见模块的参数量与计算量</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9D%97%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9D%97%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="计算量（FLOPs）和参数量（Parameters）"><a href="#计算量（FLOPs）和参数量（Parameters）" class="headerlink" title="计算量（FLOPs）和参数量（Parameters）"></a>计算量（FLOPs）和参数量（Parameters）</h3><ul><li><strong>FLOPS(全大写)</strong>：是floating point operations per second的缩写，意指每秒浮点运算次数，理解为<strong>计算速度，是一个衡量硬件性能的指标</strong>。</li><li><strong>FLOPs(s小写)</strong>：，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量，可以用来<strong>衡量算法/模型的复杂度</strong>，也就是我们这里要讨论的计算量。</li></ul><ul><li><strong>参数定义（下同）：$C_{int}$为输入通道数，k 为卷积核边长， $C_{out}$为输出通道数，$H_{out}*W_{out}$ 为输出特征图的长宽。</strong></li><li><strong>FLOPs = Paras <em> $H_{out}</em>W_{out}$</strong> ；大致是这样的，参数量是和输入特征图大小无关的。</li></ul><h4 id="标准卷积层"><a href="#标准卷积层" class="headerlink" title="标准卷积层"></a>标准卷积层</h4><ul><li><p>Parameters：</p><ul><li>考虑 bias：$(k^2<em>C_{int}+1)</em>C_{out}$</li><li>不考虑 bias：$(k^2<em>C_{int})</em>C_{out}$</li></ul></li><li><p>FLOPs:</p><ul><li>考虑 bias：$(2<em>C_{int}</em>k^2)<em>C_{out}</em>H_{out}*W_{out}$</li><li>不考虑 bias：$(2<em>C_{int}</em>k^2-1)<em>C_{out}</em>H_{out}*W_{out}$</li></ul></li><li><p>其实卷积层在实现的时候可以选择加bias或者不加，在很多的框架当中是一个可以选择的参数，为了严谨，这里特地提一下。</p></li><li><p>如何理解公式？在不考虑 bias的情况下，我们先计算输出feature map中的一个pixel的计算量，然后乘以feature map的规模大小即可，所以主要分析下面FLOPs中的括号部分：</p><script type="math/tex; mode=display">(2*C_{int}*k^2-1) = C_{int}*k^2 + C_{int}*k^2-1</script><p>可以看成两个部分，<strong>第一项是乘法运算，第二项是加法运算，因为n个数相加，要加n-1次</strong>。所以不考虑 bias，会有一个-1，如果考虑bias，刚刚好中和掉了。✖️️2是因为有$C_{int}<em>k^2$次乘法和$C_{int}</em>k^2$次加法？？</p></li></ul><h4 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h4><p>深度可分离卷积可分为两个部分，第一部分是分通道卷积（深度卷积），另一部分是1*1卷积，两部分相加即可！</p><ul><li><p>Parameters：（不考虑bias）</p><ul><li>深度卷积：$k^2*C_{int}$</li><li>1*1卷积：$(1<em>1</em>C_{int})*C_{out}$</li></ul></li><li><p>FLOPs：（考虑bias）</p><ul><li>深度卷积：$(2<em>k^2 )</em>C_{int}<em>H_{out}</em>W_{out}$</li><li>1*1卷积：$2<em>C_{int}</em>C_{out}<em>H_{out}</em>W_{out}$</li></ul></li></ul><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><ul><li><p>全局池化：针对输入所有值进行一次池化操作，不论是max、sum还是avg，都可以简单地看做是只需要对每个值算一次。</p><ul><li><p>Parameters：池化层没有可学习的参数，故参数量为0；</p></li><li><p>FLOPs：$H_{int}<em>W_{int}</em>C_{int}$</p></li></ul></li><li><p>一般池化（注意池化层的：$C_{out} = C_{int}$）</p><ul><li>Parameters：池化层没有可学习的参数，故参数量为0；-</li><li>FLOPs：$k^2<em>H_{out}</em>W_{out}*C_{out}$</li></ul></li></ul><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><ul><li><p>Parameters：</p><ul><li>考虑bias：$（I+1）*O$</li><li>不考虑bias：$I*O$</li></ul></li><li><p>FLOPs：</p><ul><li>考虑bias：$(2<em>I)</em>O$</li><li>不考虑bias：$(2<em>I-1)</em>O$</li></ul></li><li><p>分析同理，括号内是一个输出神经元的计算量，拓展到O个输出神经元。</p></li><li><p><strong>如果该全连接层的输入是卷积层的输出，需要先将输出展开成一列向量：</strong></p><p>例子： $H <em> W </em> D$  输出到 O，其中参数(包括bias)：$(H <em> W </em> D+1) * O$  </p></li></ul><h4 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h4><ul><li>Sigmoid：根据sigmoid的公式可以知道，每个输入都需要经历4次运算，因此计算量是$H<em>W</em>C*4$（参数含义同ReLU）</li></ul><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><ul><li>Relu：Relu一般都是跟在卷积层的后面，这里假设卷积层的输出为$H<em>W</em>C$，因为ReLU函数的计算只涉及到一个判断，因此计算量就是$H<em>W</em>C$</li></ul><script type="math/tex; mode=display">\sigma(x) = max(0,x)</script>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 参数量与计算量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见代码实现</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h5 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h5><ul><li><p>计算公式：</p><script type="math/tex; mode=display">Softmax(X)_{ij} = \frac{exp(X_{ij})}{\sum_{k}exp(X_{ij})}</script></li><li><p>计算过程：</p><ul><li>对每个项求幂（使用exp；<code>torch.exp()</code>）</li><li>对每一行（某一维度）求最大值，并且该行（维度）的值减去最大值，否则求exp(x)可能会溢出，导致inf的情况；</li><li>对每一行（某一维度）求和，得到每个样本的规范化常数。</li><li>将每一行除以其规范化常数，确保结果的和为1。</li></ul></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># numpy</span><br><span class="line">def softmax(x, axis=1):</span><br><span class="line">    # 计算每行的最大值</span><br><span class="line">    # row_max = x.max(axis=axis)</span><br><span class="line">    # row_max = np.expand_dims(row_max, axis=axis)</span><br><span class="line">    row_max = np.max(x, axis=axis, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    # 每行元素都需要减去对应的最大值，否则求exp(x)会溢出，导致inf情况</span><br><span class="line">    x = x - row_max</span><br><span class="line">    # 计算e的指数次幂</span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line">    x_sum = np.sum(x_exp, axis=axis, keepdims=True)</span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line">    return s</span><br><span class="line"></span><br><span class="line"># pytorch</span><br><span class="line">def softmax1(x, dim=1):</span><br><span class="line">    # 计算每行的最大值</span><br><span class="line">    # 1、不保留后恢复</span><br><span class="line">    # row_max, _ = torch.max(x, dim=dim);</span><br><span class="line">    # row_max = row_max.unsqueeze(dim) # 恢复一个为1维度，方便广播机制</span><br><span class="line">    # 2、保留维度</span><br><span class="line">    row_max, _ = torch.max(x, dim=dim, keepdims=True);</span><br><span class="line">    </span><br><span class="line">    # 每行元素都需要减去对应的最大值，否则求exp(x)会溢出，导致inf情况</span><br><span class="line">    x = x - row_max # 广播机制</span><br><span class="line">  # 计算e的指数次幂</span><br><span class="line">    x_exp = torch.exp(x)</span><br><span class="line">    x_sum = torch.sum(x_exp, dim=dim, keepdims=True)</span><br><span class="line">    s = x_exp / x_sum # 广播机制</span><br><span class="line">    return s</span><br></pre></td></tr></table></figure></li></ul><h5 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h5><ul><li><p>计算公式：</p><script type="math/tex; mode=display">Sigmoid(X) = \frac{1}{1+e^{-X}}</script></li><li><p>代码：直接套公式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># numpy</span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1.0 / (1 + np.exp(-x))</span><br><span class="line">    </span><br><span class="line"># pytorch</span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1.0 / (1 + torch.exp(-x))</span><br></pre></td></tr></table></figure></li></ul><h5 id="CrossEntropy"><a href="#CrossEntropy" class="headerlink" title="CrossEntropy"></a>CrossEntropy</h5><ul><li><p>计算公式：</p><script type="math/tex; mode=display">Loss = -\frac{1}{N} \sum_{i=0}^{N-1}\sum_{k=0}^{K-1}\,y_{i,k}\,log\, p_{i,k}</script></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># y是one-hot编码</span><br><span class="line">def cross_entropy_error(p,y):</span><br><span class="line">    assert y.shape == p.shape # 判读shape是否一致</span><br><span class="line">    delta=1e-7       #添加一个微小值可以防止负无限大(np.log(0))的发生。</span><br><span class="line">    p = softmax(p)   # 通过 softmax 变为概率分布，并且sum(p) = 1</span><br><span class="line">    # return -np.sum( y * np.log(p+delta) )    多分类</span><br><span class="line">    # return -(y * np.log(p) + (1 - y) * np.log(1 - p))  二分类</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h5 id="mDice"><a href="#mDice" class="headerlink" title="mDice"></a>mDice</h5><ul><li>计算公式：<script type="math/tex; mode=display">Dice = \frac{2 \times \left |  X \cap Y \right |}{\left | X \right | + \left | Y \right | } =            \frac{2 \times 预测正确的结果 }{ 真实结果 + 预测结果 } \qquad\qquad X是标签；Y是预测值</script></li></ul><ul><li><p>代码实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># H*W</span><br><span class="line">def dice_coeff(pred, target):</span><br><span class="line">    smooth = 1.</span><br><span class="line">    num = pred.size(0)</span><br><span class="line">    m1 = pred.view(num, -1)  # Flatten </span><br><span class="line">    m2 = target.view(num, -1)  # Flatten</span><br><span class="line">    intersection = (m1 * m2).sum() # 计算交集</span><br><span class="line">    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># H*W</span><br><span class="line">def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):</span><br><span class="line">    # Average of Dice coefficient for all batches, or for a single mask</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    if input.dim() == 2 and reduce_batch_first:</span><br><span class="line">        raise ValueError(f&#x27;Dice: asked to reduce batch </span><br><span class="line">        but got tensor without batch dimension (shape &#123;input.shape&#125;)&#x27;)</span><br><span class="line"></span><br><span class="line">    if input.dim() == 2 or reduce_batch_first:</span><br><span class="line">        inter = torch.dot(input.reshape(-1), target.reshape(-1))</span><br><span class="line">        sets_sum = torch.sum(input) + torch.sum(target)</span><br><span class="line">        if sets_sum.item() == 0:</span><br><span class="line">            sets_sum = 2 * inter</span><br><span class="line">        return (2 * inter + epsilon) / (sets_sum + epsilon)</span><br><span class="line">    else:</span><br><span class="line">        # compute and average metric for each batch element</span><br><span class="line">        dice = 0</span><br><span class="line">        for i in range(input.shape[0]):</span><br><span class="line">            dice += dice_coeff(input[i, ...], target[i, ...])</span><br><span class="line">        return dice / input.shape[0]</span><br><span class="line"></span><br><span class="line">def multiclass_dice_coeff(input: Tensor, target: Tensor,</span><br><span class="line">  reduce_batch_first: bool = False, epsilon=1e-6):</span><br><span class="line">    # Average of Dice coefficient for all classes</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    dice = 0</span><br><span class="line">    for channel in range(input.shape[1]):</span><br><span class="line">        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], </span><br><span class="line">           reduce_batch_first, epsilon)</span><br><span class="line"></span><br><span class="line">    return dice / input.shape[1]</span><br><span class="line"></span><br><span class="line">def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):</span><br><span class="line"># 在调用的时候，groud-truth若是多类别，需要进行one-hot编码</span><br><span class="line"># 【B,C,H,W】target and input</span><br><span class="line">    # Dice loss (objective to minimize) between 0 and 1</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    fn = multiclass_dice_coeff if multiclass else dice_coeff</span><br><span class="line">    return 1 - fn(input, target, reduce_batch_first=True)</span><br></pre></td></tr></table></figure></li></ul><h5 id="mIoU"><a href="#mIoU" class="headerlink" title="mIoU"></a>mIoU</h5><ul><li><p>公式：简单来说就是： <strong>交集/并集</strong></p><script type="math/tex; mode=display">mIoU = \frac{1}{k+1} \sum_{i=0}^{k} \frac{TP}{FN+FP+TP}</script></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># 输入 pred，target 【B,H,W】</span><br><span class="line"># 第一种方式 比较合适我理解</span><br><span class="line">def iou_mean(pred, target, n_classes = 1):</span><br><span class="line">    # n_classes ：the number of classes in your dataset,not including background</span><br><span class="line">    # for mask and ground-truth label, not probability map</span><br><span class="line">    ious = [] #每个类别的 IoU</span><br><span class="line">    iousSum = 0</span><br><span class="line">    pred = pred.view(-1)</span><br><span class="line">    target = target.view(-1)</span><br><span class="line">    # Ignore IoU for background class (&quot;0&quot;)</span><br><span class="line">    for cls in range(1, n_classes+1):  </span><br><span class="line">      pred_inds = pred == cls</span><br><span class="line">        target_inds = target == cls</span><br><span class="line">        # Cast to long to prevent overflows</span><br><span class="line">        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()  </span><br><span class="line">        union = pred_inds.long().sum().data.cpu().item() + </span><br><span class="line">        target_inds.long().sum().data.cpu().item() - intersection</span><br><span class="line">        if union == 0:</span><br><span class="line">          ious.append(float(&#x27;nan&#x27;))  # If there is no ground truth, do not include in evaluation</span><br><span class="line">        else:</span><br><span class="line">          ious.append (float(intersection) / float(max(union, 1)))</span><br><span class="line">          iousSum += float(intersection) / float(max(union, 1))</span><br><span class="line">       </span><br><span class="line">      return iousSum/n_classes  # mIoU</span><br><span class="line">      </span><br><span class="line"># 第二种方式</span><br><span class="line"># &#x27;K&#x27; classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.</span><br><span class="line">def intersectionAndUnion(output, target, K, ignore_index=255):</span><br><span class="line">    assert output.ndim in [1, 2, 3]</span><br><span class="line">    assert output.shape == target.shape</span><br><span class="line">    output = output.reshape(output.size).copy()</span><br><span class="line">    target = target.reshape(target.size)</span><br><span class="line">    output[np.where(target == ignore_index)[0]] = ignore_index</span><br><span class="line">    intersection = output[np.where(output == target)[0]]</span><br><span class="line">    area_intersection, _ = np.histogram(intersection, bins=np.arange(K + 1))</span><br><span class="line">    area_output, _ = np.histogram(output, bins=np.arange(K + 1))</span><br><span class="line">    area_target, _ = np.histogram(target, bins=np.arange(K + 1))</span><br><span class="line">    area_union = area_output + area_target - area_intersection</span><br><span class="line">    </span><br><span class="line">    ious = area_intersection / area_union+epsilon  # 是一个array，代表每个类别的IoU</span><br><span class="line">    mIoU = np.nanmean(ious)  # mIoU</span><br></pre></td></tr></table></figure></li></ul><h5 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h5><ul><li><p>计算公式：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">impotr torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import functional as F</span><br><span class="line"></span><br><span class="line">class Attention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    An attention layer that allows for downscaling the size of the embedding</span><br><span class="line">    after projection to queries, keys, and values.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        embedding_dim: int,</span><br><span class="line">        num_heads: int,</span><br><span class="line">        downsample_rate: int = 1,</span><br><span class="line">        attn_drop_ratio=0.,</span><br><span class="line">    ):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.internal_dim = embedding_dim // downsample_rate</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop_ratio)</span><br><span class="line">        assert self.internal_dim % num_heads == 0, &quot;num_heads must divide embedding_dim.&quot;</span><br><span class="line">        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)</span><br><span class="line">        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)</span><br><span class="line">        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)</span><br><span class="line">        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)</span><br><span class="line"></span><br><span class="line">    def forward(self, q, k, v): # [B,N,C]</span><br><span class="line">    </span><br><span class="line">        # Input projections</span><br><span class="line">        q = self.q_proj(q) # [B,N,C1]</span><br><span class="line">        k = self.k_proj(k)</span><br><span class="line">        v = self.v_proj(v)</span><br><span class="line"></span><br><span class="line">        # Separate into heads</span><br><span class="line">        b, n, c = q.shape</span><br><span class="line">        q = q.reshape(b, n, self.num_heads, c // self.num_heads).transpose(1, 2) # [B, N_heads, N_tokens, C_per_head]</span><br><span class="line">        k = k.reshape(b, n, self.num_heads, c // self.num_heads).transpose(1, 2) # [B, N_heads, N_tokens, C_per_head]</span><br><span class="line">        v = v.reshape(b, n, self.num_heads, c // self.num_heads).transpose(1, 2) # [B, N_heads, N_tokens, C_per_head]</span><br><span class="line">        </span><br><span class="line">        # Attention</span><br><span class="line">        _, _, _, c_per_head = q.shape</span><br><span class="line">        attn = q @ k.permute(0, 1, 3, 2)  # [B, N_heads, N_tokens, N_tokens]</span><br><span class="line">        attn = attn / math.sqrt(c_per_head)</span><br><span class="line">        attn = torch.softmax(attn, dim=-1)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        # Get output</span><br><span class="line">        out = attn @ v # [B, N_heads, N_tokens, C_per_head]</span><br><span class="line">        out = out.transpose(1, 2).reshape(b, n, self.num_heads * c_per_head)  # [B, N_tokens, C]</span><br><span class="line">      </span><br><span class="line">        out = self.out_proj(out)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 代码实现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像处理算法总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="图像处理的简单总结"><a href="#图像处理的简单总结" class="headerlink" title="图像处理的简单总结"></a>图像处理的简单总结</h3><ul><li><p><strong>图像平滑</strong>：是通过降低图像中像素之间的变化来<strong>减少图像中的噪声和细节，使图像变得更加均匀和模糊</strong>。这在一些情况下很有用，如去除图像中的噪声、减少细节以改善压缩率或进行预处理以准备进行后续分析</p></li><li><p><strong>图像锐化</strong>：增强图像的高频信息，<strong>增强图像的边缘和细节</strong>，使图像看起来更清晰和有深度。</p></li><li><p><strong>图像腐蚀和膨胀</strong>：图像的膨胀和腐蚀是形态学的基本运算</p><ul><li><p><strong>图像腐蚀</strong>：腐蚀就是通过卷积核，将边界部分向内部靠近，逐步腐蚀掉。其目的是缩小图像中的对象，去除对象的小细节，以及分离靠得很近的对象。腐蚀操作也使用结构元素，但与对象像素相邻的条件下，输出图像中的相应位置只有在结构元素的中心与对象像素都相邻的情况下才设置为对象像素。</p><p><strong>腐蚀操作会使对象变得更小，边界更加锐利，去除小的物体或细节。</strong></p></li><li><p><strong>图像膨胀</strong>：膨胀就是通过卷积核，将边界部分向外部靠近，逐步变粗。实际上膨胀就是腐蚀的逆过程。其目的是增加图像中对象的大小，填充对象的空隙，以及连接相邻的对象。它的工作原理是用一个称为结构元素的小矩形或圆形核对原始图像进行滑动运算。如果结构元素的中心与对象像素相邻，那么在输出图像中的相应位置就会设置为对象像素。</p><p><strong>膨胀操作会使对象变得更大，边界更加平滑，而且可以用于连接断开的对象。</strong></p></li><li><p><strong>开运算</strong>：是先腐蚀后膨胀，用于去除小的噪声，去除毛刺，分离紧密相邻的对象。</p></li><li><p><strong>闭运算</strong>：是先膨胀后腐蚀，用于填充对象的空隙，连接相邻的对象。</p></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20231019/7.png" style="zoom:50%;" /></p><h4 id="图像滤波简介"><a href="#图像滤波简介" class="headerlink" title="图像滤波简介"></a>图像滤波简介</h4><p>图像滤波，就是在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像预处理中不可缺少的操作，其处理效果的好坏将直接影响到后面图像处理和分析的有效性和可靠性。</p><p>图像滤波按照图像域可分为两种类型：</p><ul><li><p>领域滤波：其本质就是数字窗口上的数学运算。一般用于图像平滑、图像锐化、特征提取(如纹理测量、边缘检测)等，邻域滤波使用邻域算子，利用给定像素周围像素值以决定此像素最终输出的一种算子。邻域滤波方式又分为线性滤波和非线性滤波，其中线性滤波包括<strong>均值滤波、方框滤波和高斯滤波</strong>等，非线性滤波包括中<strong>值滤波和双边滤波</strong>等。</p></li><li><p>频域滤波：其本质是对像素频率的修改。一般用于降噪、重采样、图像压缩等。按图像频率滤除效果主要分为两种类型：<strong>低通滤波（滤除原图像的高频成分，即模糊图像边缘与细节）和高通滤波（滤除原图像的低频成分，即图像锐化，图像细节增强）。</strong></p><p>高频：变化剧烈的灰度分量，例如边界；低频：变化缓慢的灰度，例如一片大海</p></li></ul><h5 id="均值滤波"><a href="#均值滤波" class="headerlink" title="均值滤波"></a>均值滤波</h5><ul><li><p>均值滤波采用多次<strong>测量取平均</strong>的思想，用每一个像素周围的像素的平均值代替自身。均值滤波是方框滤波归一化后的特殊情况。</p></li><li><p>取卷积核（kernel）区域下的所有像素的平均值来替换中心元素。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230824/1.png" alt=""></p></li><li><p><strong>优点</strong>：能够将受到噪声影响的像素使用该噪声周围的像素值进行修复，对椒盐噪声的滤除比较好。</p></li><li><p><strong>缺点</strong>：不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊。</p></li></ul><h5 id="方框滤波"><a href="#方框滤波" class="headerlink" title="方框滤波"></a>方框滤波</h5><ul><li><p>与均值滤波不同的是，方框滤波不会计算像素的均值。在均值滤波中，滤波结果的像素值是任意一个点的邻域平均值，等于各邻域像素值之和除以邻域面积。在方框滤波中，可以自由选择是否对均值滤波的结果进行归一化，即可以自由选择滤波结果是邻域像素值之和的平均值，还是邻域像素值之和。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230824/2.png" alt=""></p></li><li><p>当normalize=True时，与均值滤波结果相同；</p></li><li><p>当normalize=False时，表示对加和后的结果不进行平均操作，大于255的使用255表示。</p></li></ul><h5 id="高斯滤波"><a href="#高斯滤波" class="headerlink" title="高斯滤波"></a>高斯滤波</h5><ul><li>高斯滤波(Gauss Filter)基于二维高斯核函数。用一个模板（或称卷积、掩模）扫描图像中的每一个像素，用模板确定的邻域内像素的<strong>加权平均灰度值</strong>去替代模板中心像素点的值。高<strong>斯滤波主要用来去除高斯噪声</strong>。</li><li>高斯滤波具有在保持细节的条件下进行噪声滤波的能力，因此广泛应用于图像降噪中，但其效率比均值滤波低。</li></ul><h5 id="中值滤波"><a href="#中值滤波" class="headerlink" title="中值滤波"></a>中值滤波</h5><ul><li><p>中值滤波将待处理的像素与其周围像素（包括自身）从小到大排序，取中值代替该像素。</p></li><li><p>优点：去除脉冲噪声、椒盐噪声的同时又能保留图像边缘细节。</p></li><li>缺点：当卷积核较大时，仍然使得图像变得模糊，而且计算量很大。</li></ul><h5 id="双边滤波"><a href="#双边滤波" class="headerlink" title="双边滤波"></a>双边滤波</h5><ul><li><p>因为高斯滤波把距离设为权重，设计滤波模板作为滤波系数，并且只考虑像素之间的空间位置关系，所以滤波结果丢失了边缘信息。</p></li><li><p>双边滤波器顾名思义比高斯滤波多了一个高斯方差sigma－d，它是基于空间分布的高斯滤波函数，所以在边缘附近，离的较远的像素不会太多影响到边缘上的像素值，这样就保证了边缘附近像素值的保存。但是由于保存了过多的高频信息，对于彩色图像里的高频噪声，双边滤波器不能够干净的滤掉，只能够对于低频信息进行较好的滤波。</p></li></ul><h5 id="低通滤波"><a href="#低通滤波" class="headerlink" title="低通滤波"></a>低通滤波</h5><ul><li><p>规则为低频信息能正常通过，而超过设定临界值的高频信息则被阻隔、减弱。但是阻隔、减弱的幅度则会依据不同的频率以及不同的滤波程序（目的）而改变。</p></li><li><p><strong>低通滤波，通过了低频信息，保留了图像背景和基本内容，图像边缘被阻挡，图像变模糊。</strong></p></li></ul><h5 id="高通滤波"><a href="#高通滤波" class="headerlink" title="高通滤波"></a>高通滤波</h5><ul><li><p>规则为高频信息能正常通过，而低于设定临界值的低频信息则被阻隔、减弱。但是阻隔、减弱的幅度则会依据不同的频率以及不同的滤波程序(目的)而改变。</p></li><li><p><strong>高通滤波，通过了高频信息，提取了图像边缘和噪声。</strong></p></li></ul><h5 id="opencv-python中各种滤波方法的应用"><a href="#opencv-python中各种滤波方法的应用" class="headerlink" title="opencv-python中各种滤波方法的应用"></a>opencv-python中各种滤波方法的应用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line">import cv2 as cv</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">img = cv.imread(&#x27;lenna.jpg&#x27;)</span><br><span class="line"></span><br><span class="line"># 均值滤波</span><br><span class="line">img_blur = cv.blur(img, (3,3)) # (3,3)代表卷积核尺寸，随着尺寸变大，图像会越来越模糊</span><br><span class="line">img_blur = cv.cvtColor(img_blur, cv.COLOR_BGR2RGB) # BGR转化为RGB格式</span><br><span class="line"></span><br><span class="line"># 方框滤波</span><br><span class="line">img_boxFilter1 = cv.boxFilter(img, -1, (3,3), normalize=True) # 当 normalize=True 时，与均值滤波结果相同</span><br><span class="line">img_boxFilter1 = cv.cvtColor(img_boxFilter1, cv.COLOR_BGR2RGB) # BGR转化为RGB格式</span><br><span class="line"></span><br><span class="line">img_boxFilter2 = cv.boxFilter(img, -1, (3,3), normalize=False)</span><br><span class="line">img_boxFilter2 = cv.cvtColor(img_boxFilter2, cv.COLOR_BGR2RGB) # BGR转化为RGB格式</span><br><span class="line"></span><br><span class="line"># 高斯滤波</span><br><span class="line">img_GaussianBlur= cv.GaussianBlur(img, (3,3), 0, 0) # 参数说明：(源图像，核大小，x方向的标准差，y方向的标准差)</span><br><span class="line">img_GaussianBlur = cv.cvtColor(img_GaussianBlur, cv.COLOR_BGR2RGB) # BGR转化为RGB格式</span><br><span class="line"></span><br><span class="line"># 中值滤波</span><br><span class="line">img_medianBlur = cv.medianBlur(img, 3)</span><br><span class="line">img_medianBlur = cv.cvtColor(img_medianBlur, cv.COLOR_BGR2RGB) # BGR转化为RGB格式</span><br><span class="line"></span><br><span class="line"># 双边滤波</span><br><span class="line"># 参数说明：(源图像，核大小，sigmaColor，sigmaSpace)</span><br><span class="line">img_bilateralFilter=cv.bilateralFilter(img, 50, 100, 100)</span><br><span class="line">img_bilateralFilter = cv.cvtColor(img_bilateralFilter, cv.COLOR_BGR2RGB) # BGR转化为RGB格式</span><br><span class="line"></span><br><span class="line">titles = [&#x27;img_blur&#x27;, &#x27;img_boxFilter1&#x27;, &#x27;img_boxFilter2&#x27;,</span><br><span class="line">          &#x27;img_GaussianBlur&#x27;, &#x27;img_medianBlur&#x27;, &#x27;img_bilateralFilter&#x27;]</span><br><span class="line">images = [img_blur, img_boxFilter1, img_boxFilter2, img_GaussianBlur, img_medianBlur, img_bilateralFilter]</span><br><span class="line"></span><br><span class="line"># 低通滤波</span><br><span class="line">def Low_Pass_Filter(srcImg_path):</span><br><span class="line">    #img = cv.imread(&#x27;srcImg_path&#x27;, 0)</span><br><span class="line">    img = np.array(Image.open(srcImg_path))</span><br><span class="line">    img = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">    # 傅里叶变换</span><br><span class="line">    dft = cv.dft(np.float32(img), flags = cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">    fshift = np.fft.fftshift(dft)</span><br><span class="line"></span><br><span class="line">    # 设置低通滤波器</span><br><span class="line">    rows, cols = img.shape</span><br><span class="line">    crow, ccol = int(rows/2), int(cols/2) # 中心位置</span><br><span class="line">    mask = np.zeros((rows, cols, 2), np.uint8)</span><br><span class="line">    mask[crow-30:crow+30, ccol-30:ccol+30] = 1</span><br><span class="line"></span><br><span class="line">    # 掩膜图像和频谱图像乘积</span><br><span class="line">    f = fshift * mask</span><br><span class="line"></span><br><span class="line">    # 傅里叶逆变换</span><br><span class="line">    ishift = np.fft.ifftshift(f)</span><br><span class="line">    iimg = cv.idft(ishift)</span><br><span class="line">    res = cv.magnitude(iimg[:,:,0], iimg[:,:,1])</span><br><span class="line">    </span><br><span class="line">    return res</span><br><span class="line"></span><br><span class="line"># 高通滤波</span><br><span class="line">def High_Pass_Filter(srcImg_path):</span><br><span class="line">    #img = cv.imread(srcImg_path, 0)</span><br><span class="line">    img = np.array(Image.open(srcImg_path))</span><br><span class="line">    img = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">    # 傅里叶变换</span><br><span class="line">    dft = cv.dft(np.float32(img), flags = cv.DFT_COMPLEX_OUTPUT)</span><br><span class="line">    fshift = np.fft.fftshift(dft)</span><br><span class="line"></span><br><span class="line">    # 设置高通滤波器</span><br><span class="line">    rows, cols = img.shape</span><br><span class="line">    crow, ccol = int(rows/2), int(cols/2) # 中心位置</span><br><span class="line">    mask = np.ones((rows, cols, 2), np.uint8)</span><br><span class="line">    mask[crow-30:crow+30, ccol-30:ccol+30] = 0</span><br><span class="line"></span><br><span class="line">    # 掩膜图像和频谱图像乘积</span><br><span class="line">    f = fshift * mask</span><br><span class="line"></span><br><span class="line">    # 傅里叶逆变换</span><br><span class="line">    ishift = np.fft.ifftshift(f)</span><br><span class="line">    iimg = cv.idft(ishift)</span><br><span class="line">    res = cv.magnitude(iimg[:,:,0], iimg[:,:,1])</span><br><span class="line">    return res</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230824/5.png" style="zoom:50%;" /></p><h4 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h4><p>图像边缘是图像最基本的特征，所谓<strong>边缘</strong>(Edge) 是指图像局部特性的不连续性。灰度或结构等信息的突变处称之为<strong>边缘</strong>。例如，灰度级的突变、颜色的突变,、纹理结构的突变等。边缘是一个区域的结束，也是另一个区域的开始，利用该特征可以分割图像。</p><p>边缘通常可以通过<strong>一阶导数</strong>或<strong>二阶导数</strong>检测得到。<strong>一阶导数</strong>是以<strong>最大值</strong>作为对应的边缘的位置，而<strong>二阶导数</strong>则以<strong>过零点</strong>作为对应边缘的位置。</p><ul><li><p>一阶导数的边缘算法：通过模板作为核与图像的每个像素点做卷积和运算，然后选取合适的阈值来提取图像的边缘。</p><p><strong>常见的有Roberts算子、Sobel算子和Prewitt算子。</strong></p></li><li><p>二阶导数的边缘算法：依据于二阶导数过零点，<strong>常见的有拉普拉斯 (Laplacian) 算子</strong>，此类算子对噪声敏感。</p></li><li><p><strong>Canny算子：</strong>其是在满足一定约束条件下推导出来的边缘检测最优化算子。</p></li></ul><h5 id="图像梯度"><a href="#图像梯度" class="headerlink" title="图像梯度"></a>图像梯度</h5><p>图像梯度是边缘检测的基础知识，因此在讲边缘算子之前先复习下图像梯度的知识。</p><p>一幅图像 $f$ 在位置 $(x,y)$ 处的梯度定义如下：</p><script type="math/tex; mode=display">\nabla f \equiv \operatorname{grad}(f) \equiv\left[\begin{array}{l}g_{x} \\g_{y}\end{array}\right] \equiv\left[\begin{array}{l}\frac{\partial f}{\partial x} \\\frac{\partial f}{\partial y}\end{array}\right]</script><p>因为图像是一种离散分布，因此求导其实就是做差分，$g_x$ 和 $g_y$ 定义如下：</p><script type="math/tex; mode=display">g_x = \frac{\partial f(x,y)}{\partial x} = f(x+1,y) - f(x,y) \\g_y = \frac{\partial f(x,y)}{\partial y} = f(x,y+1) - f(x,y)</script><p>其实上面的这两个公式可以使用下图的一维模板对图像 $f(x,y)$ 进行滤波得到。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20231019/6.png" alt=""></p><p>梯度 $\nabla f$ 表示的是图像 $f$ 在位置 $(x,y)$ 处的梯度向量。梯度的方向以及梯度的大小表示如下：</p><p>梯度方向：</p><script type="math/tex; mode=display">\Theta=argtan(\frac{g_y}{g_x})</script><p>梯度大小：</p><script type="math/tex; mode=display">M(x, y)=\operatorname{mag}(\nabla f)=\sqrt{g_{x}^{2}+g_{y}^{2}}</script><p>用于计算梯度偏导数的<strong>滤波器模板</strong>，通常称之为<strong>梯度算子</strong>、<strong>边缘算子</strong>和<strong>边缘检测子</strong>等。</p><h5 id="Roberts算子-一阶"><a href="#Roberts算子-一阶" class="headerlink" title="Roberts算子 (一阶)"></a>Roberts算子 (一阶)</h5><h5 id="Prewitt算子-一阶"><a href="#Prewitt算子-一阶" class="headerlink" title="Prewitt算子 (一阶)"></a>Prewitt算子 (一阶)</h5><h5 id="Sobel算子-一阶"><a href="#Sobel算子-一阶" class="headerlink" title="Sobel算子 (一阶)"></a>Sobel算子 (一阶)</h5><h5 id="拉普拉斯算子-Laplacian-二阶"><a href="#拉普拉斯算子-Laplacian-二阶" class="headerlink" title="拉普拉斯算子 Laplacian (二阶)"></a>拉普拉斯算子 Laplacian (二阶)</h5><h5 id="Canny算子"><a href="#Canny算子" class="headerlink" title="Canny算子"></a>Canny算子</h5><p>Canny提出了一个对于边缘检测算法的评价标准，包括：</p><p>1)    以低的错误率检测边缘，也即意味着需要尽可能准确的捕获图像中尽可能多的边缘。<br>2)    检测到的边缘应精确定位在真实边缘的中心。<br>3)    图像中给定的边缘应只被标记一次，并且在可能的情况下，图像的噪声不应产生假的边缘。</p><p>简单来说就是，检测算法要做到：<strong>边缘要全，位置要准，抵抗噪声的能力要强。</strong> </p><p>该算子求边缘点的<strong>具体算法步骤</strong>如下：</p><ul><li><strong>（1）图像降噪，用高斯滤波器平滑图像</strong>：边缘检测算子受噪声的影响都很大。那么，我们第一步就是想到要先去除噪声，因为噪声就是灰度变化很大的地方，所以容易被识别为伪边缘。</li><li><strong>（2）用一阶偏导有限差分计算梯度幅值和方向</strong>，例如 Sobel </li><li><strong>（3）对梯度幅值进行非极大值抑制</strong>：sobel算子检测出来的边缘太粗了，我们需要<strong>抑制那些梯度不够大的像素点，只保留最大的梯度，从而达到瘦边的目的</strong>。通常灰度变化的地方都比较集中，将局部范围内的梯度方向上，灰度变化最大的保留下来，其它的不保留，这样可以剔除掉一大部分的点。将有多个像素宽的边缘变成一个单像素宽的边缘。即“胖边缘”变成“瘦边缘”。</li><li><strong>（4）用双阈值算法检测和连接边缘</strong>：通过非极大值抑制后，仍然有很多的可能边缘点，进一步的设置一个双阈值，即低阈值（low），高阈值（high）。灰度变化大于high的，设置为<strong>强边缘像素</strong>，低于low的，剔除。在low和high之间的设置为<strong>弱边缘</strong>。对每一个弱边缘进一步判断，如果其领域内有强边缘像素，保留，如果没有，剔除。</li></ul><h4 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h4><p><strong>直方图均衡化</strong>(Histogram Equalization)是一种<strong>增强图像对比度</strong>(Image Contrast)的方法，其主要思想是将一副图像的直方图分布变成近似均匀分布，从而增强图像的对比度。</p><p><strong>直方图均衡化是将原图像通过某种变换，得到一幅灰度直方图为均匀分布的新图像的方法。</strong></p><p>直方图均衡化方法的<strong>基本思想是对在图像中像素个数多的灰度级进行展宽，而对像素个数少的灰度级进行缩减。从而达到清晰图像的目的。</strong></p><p>计算步骤：</p><ul><li>（1）确定图像的灰度级别，彩色图像,需要将其转换为灰度图像,其中的<strong>灰度级一般是0-255</strong></li><li>（2）计算图像归一化后的灰度直方图：统计每个灰度在原始图像上的像素所占总体的比例</li><li>（3）计算每个灰度值对应的累计概率密度，也就是计算累计直方图 $S(i)$</li><li>（4）根据公式求取像素映射关系：$SS(i) = 255 \times S(i)（需要取整）,其中255就是灰度级$</li><li>（5）将原始灰度值带入公式就得到了他对应的新的灰度值。可以看出数量比较少的灰度值就会映射到同一个灰度，而且灰度直方图也会被拉开，对比度提高。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20231019/8.png" style="zoom: 67%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20231019/9.png" style="zoom:67%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 图像处理 </tag>
            
            <tag> Image Processing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习相关算法总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h4 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h4><p>集成学习通过训练多个分类器，然后将其组合起来，从而达到更好的预测性能，提高分类器的泛化能力。</p><p>Baggging 和Boosting都是模型融合的方法，可以将弱分类器融合之后形成一个强分类器，而且融合之后的效果会比最好的弱分类器更好。</p><p>目前集成学习主要有三个主要框架：bagging，boosting，stacking。</p><p><strong>偏差：训练到的模型与真实标签之间的区别。</strong></p><p><strong>方差：每次学习的模型之间差别有多大。</strong></p><p><strong>偏差指的是算法的期望预测与真实值之间的偏差程度，反映了模型本身的拟合能力；</strong></p><p><strong>方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。</strong></p><h5 id="bagging套袋法"><a href="#bagging套袋法" class="headerlink" title="bagging套袋法"></a>bagging套袋法</h5><p>bagging是并行集成学习方法的最著名代表，其算法过程如下：</p><ul><li><p>从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）</p></li><li><p>每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）</p></li><li><p>对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/1.png" alt=""></p></li></ul><h5 id="boosting提升法"><a href="#boosting提升法" class="headerlink" title="boosting提升法"></a>boosting提升法</h5><p>大多数的提升方法都是改变训练数据的概率分布（训练数据中的各个数据点的权值分布），调用弱学习算法得到一个弱分类器，再改变训练数据的概率分布，再调用弱学习算法得到一个弱分类器，如此反复，得到一系列弱分类器。<strong>两个问题？</strong></p><ol><li>是在每一轮如何改变训练数据的概率分布。</li><li>是如何将多个弱分类器组合成一个强分类器。</li></ol><p><strong>关于第一个问题</strong>，AdaBoosting方式每次使用的是全部的样本，每轮训练改变样本的权重。下一轮训练的目标是找到一个函数f 来拟合上一轮的残差。当残差足够小或者达到设置的最大迭代次数则停止。</p><p><strong>至于第二个问题</strong>，采取加权多数表决的方法。Boosting会减小在上一轮训练正确的样本的权重，增大错误样本的权重。（对的残差小，错的残差大）梯度提升的Boosting方式是使用代价函数对上一轮训练出的模型函数f的偏导来拟合残差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/2.png" style="zoom:33%;" /></p><h5 id="bagging和boosting区别"><a href="#bagging和boosting区别" class="headerlink" title="bagging和boosting区别"></a>bagging和boosting区别</h5><p><strong>Bagging是减少方差variance，而Boosting是减少偏差bias</strong></p><p><strong>（1）样本选择上：</strong></p><ul><li><p>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</p></li><li><p>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。</p></li></ul><p><strong>（2）样例权重：</strong></p><ul><li><p>Bagging：使用均匀取样，每个样例的权重相等。</p></li><li><p>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。</p></li></ul><p><strong>（3）预测函数：</strong></p><ul><li><p>Bagging：所有预测函数的权重相等。</p></li><li><p>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。</p></li></ul><p><strong>（4）并行计算：</strong></p><ul><li><p>Bagging：各个预测函数可以并行生成。</p></li><li><p>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</p></li></ul><h5 id="stacking模型融合"><a href="#stacking模型融合" class="headerlink" title="stacking模型融合"></a>stacking模型融合</h5><p><strong>stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。</strong></p><p><strong>Bagging是减少方差variance和偏差bias。Stacking既减少方差又减少偏差。</strong></p><p><strong>Bagging是尽可能的提高模型的随机性，使得模型之间没有强相关性，以此降低融合的方差。</strong></p><p><strong>Boosting是不断递进的优化每个分类器，逐步减少偏差，各分类器直接具有强相关性，因此不会降低方差。</strong></p><h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><p>什么是决策树呢？决策树是一种监督学习方法，既可以用来处理分类问题也可以处理回归问题。</p><p>决策树是一个<strong>有监督分类模型</strong>，本质是选择一个最大信息增益的特征值进行输的分割，直到达到结束条件或叶子节点纯度达到阈值。</p><h5 id="ID3算法（信息增益）"><a href="#ID3算法（信息增益）" class="headerlink" title="ID3算法（信息增益）"></a>ID3算法（信息增益）</h5><ul><li><p>思想：ID3使用信息增益作为特征选择的度量，使用自顶向下的贪心算法遍历决策树空间。</p><ul><li>（1）计算数据集合的<strong>信息熵</strong>，以及各个特征的<strong>条件熵</strong>，最后计算<strong>信息增益</strong>，选择信息增益最大的作为本次划分的节点</li><li>（2）删除上一步使用的特征。更新各个分支的数据集和特征集。</li><li>（3）重复（1）（2）步，直到子集包含单一特征，则为分支叶节点。</li></ul></li><li><p>信息熵：是度量样本集合纯度最常用的一种指标，假定当前样本集合 $D$ 中第  $k$ 类样本所占的比例为 $p_k(k = 1, 2, …, c)$ ，则 $D$ 的信息熵定义为： </p><script type="math/tex; mode=display">Ent(D)= -\sum_{k=1}^{c}p_klog_2 p_k</script><p> $Ent(D)$ 的值越小，则 $D$ 的纯度越高。注意因为 $p_k \le 1$ ，因此 $Ent(D)$ 也是一个大于等于０小于１的值。$p_i = \frac{|C_k|}{|D|}$, 其中Ck表示集合 D 中属于第 k 类样本的样本子集。</p><p>信息熵：</p><script type="math/tex; mode=display">H(D)= -\sum_{k=1}^{c}\frac{|C_k|}{|D|}log_2 \frac{|C_k|}{|D|}</script><p>针对某个特征A，对于数据集D的条件熵：也就是考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重 $\frac{|D^i|}{|D|}$ </p><script type="math/tex; mode=display">H(D|A) =\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}(\sum_{k=1}^{c}\frac{|C_{ik}|}{|D_i|}log_2 \frac{|C_{ik}|}{|D_i|})</script><p>信息增益== 信息熵-条件熵：</p><script type="math/tex; mode=display">Gain(D,a) = H(D) - H(D|A)</script><p><strong>信息增益越大越好，因为其代表着选择该属性进行划分所带来的纯度提升</strong>，因此全部计算当前样本集合 $D$ 中存在不同取值的那些属性的信息增益后，取信息增益最大的那个所对应的属性作为划分属性即可。</p></li><li><p>优缺点</p><ul><li>ID3算法没有进行决策树剪枝，容易发生过拟合</li><li>ID3算法没有给出处理连续数据的方法，只能处理离散特征</li><li>ID3算法不能处理带有缺失值的数据集,需对数据集中的缺失值进行预处理</li><li><strong>信息增益准则对可取值数目较多的特征有所偏好</strong>  <strong>(信息增益反映的给定一个条件以后不确定性减少的程度,必然是分得越细的数据集确定性更高,也就是条件熵越小,信息增益越大)</strong></li></ul></li></ul><h5 id="C4-5算法（信息增益率）"><a href="#C4-5算法（信息增益率）" class="headerlink" title="C4.5算法（信息增益率）"></a>C4.5算法（信息增益率）</h5><p><strong>C4.5主要是克服ID3使用信息增益进行特征划分对取值数据较多特征有偏好的缺点</strong>。使用<strong>信息增益率</strong>进行特征划分。</p><ul><li><p>对ID3算法的改进：</p><ul><li><p>引入剪枝策略，使用悲观剪枝策略进行后剪枝。</p></li><li><p>使用信息增益率代替信息增益，作为特征划分标准</p></li><li><p>连续特征离散化</p></li><li><ul><li>需要处理的样本或样本子集按照连续变量的大小从小到大进行排序</li><li>假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点,根据这个分割点把原来连续的属性分成两类</li></ul></li><li><p>缺失值处理</p></li><li><ul><li>对于具有缺失值的特征，用没有缺失的样本子集所占比重来折算信息增益率，选择划分特征</li><li>选定该划分特征，对于缺失该特征值的样本，将样本以不同的概率划分到不同子节点</li></ul></li></ul></li><li><p>信息增益率，信息增益率表示如下：</p><script type="math/tex; mode=display">Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}</script><script type="math/tex; mode=display">IV(a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2 \frac{|D^v|}{|D|}</script><p> $IV(a) $称为属性 a 的“固有值”。属性 a 的可能取值数目越多，则 $IV(a)$ 的值通常会越大，信<strong>息增益率对可取值较少的特征有所偏好（分母越小，整体越大）</strong>因此一定程度上抵消了信息增益对可取值数目多的属性的偏好。<strong>但是</strong>C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个<strong>启发式方法</strong>：<strong>先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的</strong>。</p></li><li><p>决策树剪枝：</p><ul><li>决策树剪枝是为了防止过拟合</li><li>C4.5采用的是后剪枝策略进行剪枝：在决策树构造完成后，自底向上对非叶节点进行评估，如果将其换成叶节点能提升泛化性能，则将该子树换成叶节点。后剪枝决策树欠拟合风险很小，泛化性能往往优于预剪枝决策树。但训练时间会大很多。</li></ul></li><li><p>C4.5的优缺点</p><ul><li>C4.5只能用于分类</li><li>C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li><li>在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，算法低效。</li><li>对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到右子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。</li></ul></li></ul><h5 id="CART算法（基尼系数）"><a href="#CART算法（基尼系数）" class="headerlink" title="CART算法（基尼系数）"></a>CART算法（基尼系数）</h5><p>相比ID3和C4.5算法，CART算法使用二叉树简化决策树规模，提高生成决策树效率。</p><ul><li><p>CART树在C4.5基础上改进</p><ul><li>CART使用二叉树来代替C4.5的多叉树，提高了生成决策树效率</li><li>C4.5只能用于分类，<strong>CART树可用于分类和回归</strong></li><li>CART 使用基尼（Gini）系数作为变量的不纯度量，减少了大量的对数运算</li><li>CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中</li><li>CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法</li><li>ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征</li></ul></li><li><p>CART是一棵二叉树，采用二元切分法，每次把数据分成两份，分别进入左子树、右子树。并且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子节点多一。相比于ID3和C4.5，CART的应用要多一些，既可以用于分类也可以用于回归。CART分类时，选择基尼指数（Gini）为最好的分类特征，gini描述的是纯度，与信息熵含义类似，CART中每次迭代都会降低基尼系数。基尼值：</p><script type="math/tex; mode=display">Gini(D) = \sum_{k=1}^{c} \sum_{k^, \not =k}p_kp_{k^,} = 1- \sum_{k=1}^{c}p_k^2 = 1-\sum_{k=1}^{c}(\frac{D^k}{D})^2</script><p>直观来看，$Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率，因此$Gini(D)$ 越小，则数据集 $D$ 的纯度越高。</p><p>对于样本D，个数为|D|，根据特征A的某个值a，把D分成|D1|和|D2|，则在特征A的条件下，<strong>样本D的基尼系数表达式为：</strong></p><script type="math/tex; mode=display">Gini\_index(D,A) = \frac{|D^1|}{|D|}Gini(D^1)+ \frac{|D^2|}{|D|}Gini(D^2)</script><p>于是，我们在候选属性集合A中，选择那个使得划分后基尼系数最小的属性作为最优划分属性即可。</p></li><li><p><strong>缺失值处理</strong></p><p>CART 算法使用一种惩罚机制来抑制提升值，从而反映缺失值的影响。为树的每个节点都找到代理分裂器，当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。</p></li><li><p>决策分类树</p><p><strong>CART作为分类树时，特征属性可以是连续类型也可以是离散类型，但观察属性(即标签属性或者分类属性)必须是离散类型</strong>。</p><p>先说分类树，ID3、C4.5在每一次分支时，是穷举每一个特征属性的每一个阈值，找到使得按照特征值&lt;=阈值，和特征值&gt;阈值分成的两个分支的熵最大的特征和阈值。按照该标准分支得到两个新节点，用同样的方法进行分支，直到所有人被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中性别不唯一，则以多数人的性别作为该叶子节点的性别。</p></li><li><p>决策回归树</p><p>回归树总体流程也是类似，不过在每个节点(不一定是叶子节点)都会得到预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分支时穷举每个特征的每个阈值，找最好的分割点，但衡量的标准变成了最小化均方误差，即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方误差越大，通过最小化均方误差找最靠谱的分支依据。分支直到每个叶子节点上的人的年龄都唯一(这太难了)，或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄作为该叶子节点的预测年龄。</p><p>对于决策树建立后做预测的方式，CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p></li><li><p><strong>剪枝策略</strong>：采用一种“<strong>基于代价复杂度的剪枝</strong>”方法进行后剪枝</p></li></ul><h5 id="对比总结"><a href="#对比总结" class="headerlink" title="对比总结"></a>对比总结</h5><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/3.png" alt=""></p><ul><li><strong>Bagging+决策树=随机森林</strong></li><li><strong>Boosting+决策树=GBDT</strong></li></ul><h4 id="随机森林（Bagging）"><a href="#随机森林（Bagging）" class="headerlink" title="随机森林（Bagging）"></a>随机森林（Bagging）</h4><p>随机森林本质上就是构建很多弱决策树，然后整合成森林，来确定最终的预估结果。<strong>Bagging+决策树=随机森林</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/4.jpg" style="zoom:80%;" /></p><p>随机森林的主要特点可以总结为如下2点：<strong>数据随机性选取，待选特征的随机选取</strong>。主要是为了消除过拟合问题。随机森林使用CART树作为弱学习器，生成树的过程中不进行剪枝<strong>，确定最终结果时，分类使用投票机制，回归问题使用平方误差最小化。</strong></p><p>随机森林根据下面步骤来构建：</p><ol><li>M来表示训练样本的个数，N表示特征数目</li><li>输入特征数目n，用于确定决策树一个节点的决策结果；其中n应远小于N</li><li>M个训练样本中，有放回抽样的方式，取样k次，形成训练集，并用未抽到样本做预测，评估误差</li><li>随机选择n个特征，每棵决策树上每个节点的决策基于这些特征确定。根据这n个特征，计算其最佳的分裂方式</li><li>最后根据每棵树，以多胜少方式决定分类</li></ol><p>优点：</p><ul><li>很容易查看模型的输入特征的相对重要性</li><li>可以处理高维数据</li><li>超参数的数量不多，而且它们所代表的含义直观易懂</li><li>随机森林有足够多的树，分类器就不会产生过度拟合模型</li></ul><p>缺点：</p><ul><li>使用大量的树会使算法变得很慢，无法做到实时预测</li><li>对于回归问题，精准度不够</li><li>抗噪声干扰能力弱，无法自动处理异常样本</li><li>模型越深越容易过拟合</li></ul><h4 id="Boosting算法代表"><a href="#Boosting算法代表" class="headerlink" title="Boosting算法代表"></a>Boosting算法代表</h4><h5 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h5><ul><li>特点<ul><li>不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起到不同的作用</li><li>利用基本分类器的线性组合构建最终的分类器</li></ul></li></ul><p>假定给定一个二分类分类的训练数据：</p><script type="math/tex; mode=display">T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}</script><p>其中每个样本点由实例与标记组成。实例 $x_i \in \chi \subseteq R^n$ ，标记 $y_i \in \Upsilon = \{-1,1\}$ ，$\chi$ 是实例空间，$\Upsilon$ 是标记集合。Adaboost 利用以下算法，从训练数据中学习一系列弱分类器，并将这些弱分类器线性组合成为一个强分类器。</p><p>（1）初始化训练数据的权值分布：</p><script type="math/tex; mode=display">D_1 = (w_{11},...,w_{1i},...,w_{1N}),\quad w_{1i}=\frac{1}{N},\quad i=1,2,...,N</script><blockquote><p>假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同。这一假设保证第１步能够在原始数据上学习基本分类器 $G_1(x)$ 。</p></blockquote><p>（2）一共需要学习 M 个基本分类器，则在每一轮 $m = 1,2,…,M$ 顺次地执行下列操作：</p><ul><li>a.<strong>使用当前分布 $D_m$ 加权的训练数据集，得到基本分类器：</strong></li></ul><script type="math/tex; mode=display">G_m(x):\chi\rightarrow \{-1,+1\}</script><ul><li>b.<strong>计算 $G_m(x)$ 在训练数据集上的分类误差率：</strong></li></ul><script type="math/tex; mode=display">e_m = \sum_{i=1}^{N}P(G_m(x_i)\not = y_i) = \sum_{G_{mi}\not = y_i}w_{mi}</script><p>$w_{mi}$ 表示第 m 轮中第 i 个实例的权值，$\sum_{i=1}^{N}w_{mi} = 1$ 。这表明，  $G_{m}(x)$  在加权的训练数据集上的分类误差率是被 $G_{m}(x)$  误分类的样本的权值之和。</p><ul><li><p>c.<strong>计算 $G_{m}(x)$ 的系数：</strong></p><script type="math/tex; mode=display">\alpha_m = \frac{1}{2}ln \frac{1-e_m}{e_m}</script><p>$\alpha_m$ 表示 $G_{m}(x)$ 在最终分类器中的重要性。根据式子可知，$e_m \le \frac{1}{2}$ 时，$\alpha_m \ge 0$ ，并且 $\alpha_m$ 随着 $e_m$ 的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。<strong>这里注意所有 $\alpha_m$ 加起来的和并不等于 1 。</strong> （<u>注意 $e_m$ 是有可能比 $\frac{1}{2}$ 大的，也就是说 $\alpha_m$ 有可能小于 0，《统计学习方法》没有讨论这种情况的处理方法，但在西瓜书中的处理方法是抛弃该分类器，且终止学习过程，哪怕学习到的分类器个数远远没有达到预设的 M</u>）</p></li><li><p>e.<strong>更新训练数据集的权值分布：</strong></p></li></ul><script type="math/tex; mode=display">D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})</script><script type="math/tex; mode=display">w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)), \quad i=1,2,...,N</script><script type="math/tex; mode=display">其中 Z_m是规范因子，Z_m = \sum_{i=1}^{N}w_{mi}exp(-\alpha_my_iG_m(x_i))</script><p>​    $w_{m+1,i}$ 也可以写成分段函数的形式：</p><script type="math/tex; mode=display">w_{m+1,i}= \begin {cases} \frac{w_{mi}}{Z_m}e^{-\alpha_m}, & G_m(x_i) = y_i \\\frac{w_{mi}}{Z_m}e^{\alpha_m}, & G_m(x_i) \not = y_i\end {cases}</script><p>​        也就是说被基本分类器 $G_m(x)$ 误分类样本的权值得以增大，而被正确分类的样本的权值却变小。两者相比可知误分类样本的权值被        放    大 $e^{2\alpha_m} = \frac{1-e_m}{e_m}$ 倍。因此，误分类样本在下一轮学习中起更大的作用。</p><p>（3）经过以上过程后可以得到 M 个基本分类器，构建基本分类器的线性组合：</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^{M}\alpha_m G_m(x)</script><p>​    通过线性组合得到最终的分类器：</p><script type="math/tex; mode=display">G(x) = sign(f(x)) = sign(\sum_{m=1}^{M}\alpha_m G_m(x))</script><p>​    线性组合 $f(x)$ 实现 M 个基本分类器的加权表决，$f(x)$ 的符号决定了实例 $x$ 的类，$f(x)$ 的绝对值表示分类的确信度。</p><h5 id="GBDT算法"><a href="#GBDT算法" class="headerlink" title="GBDT算法"></a>GBDT算法</h5><p>GBDT（Gradient Boosting Decision Tree）梯度迭代树，是Boosting算法的一种。</p><p><strong>GBDT使用的弱学习器必须是CART，且必须是回归树。</strong>GBDT用来做回归预测，当然，通过设置阈值也能用于分类任务。在模型训练时，模型预测样本损失尽可能小。</p><p><strong>GBDT直观理解：模型的每一轮预测和真实值有gap，这个gap称为残差。下一轮对残差进行预测，最后将所有预测结果相加，就可以得到最终结果。</strong>也就是说，模型的结果是一组回归分类树组合：T1，T2，…，Tk，其中Tj学习的是之前j-1课树预测结果的残差。这种思想就像准备考试前的复习，先做一遍习题册，然后把做错的题目挑出来，再做一次，然后把做错的题目挑出来在做一次，经过反复多轮训练，取得最好的成绩。</p><p>GBDT学习的是残差，而本质是在<strong>学习负梯度信息</strong>，由于一般回归使用<strong>平方损失（L2 loss）</strong>，他的导数就是残差，所以GBDT学的是残差。</p><p>GBDT和AdaBoost的模型的表示形式都可以：</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^M \alpha_m G_m(x)</script><p><strong>只是 AdaBoost</strong> 在训练完一个 $G_m(x)$ 后会重新赋值样本的权重：分类错误的样本的权重会增大而分类正确的样本的权重则会减小。这样在训练时 $G_{m+1}(x)$ 会侧重对错误样本的训练，以达到模型性能的提升，</p><p><strong>但是AdaBoost</strong>模型每个基分类器的损失函数优化目标是相同的且独立的，都是最优化当前样本（样本权重）的指数损失。</p><p>GBDT是一个加性模型，但其是通过不断迭代拟合样本真实值与当前分类器预测的残差来逼近真实值的。可以形象地理解如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/5.png" style="zoom:50%;" /></p><p>按照这个思路，第 m 个基分类器的预测结果为：</p><script type="math/tex; mode=display">f_m(x) = f_{m-1}(x)+\alpha_mG_m(x)</script><p>而 $G_m(x)$ 的优化目标就是最小化当前预测结果 $f_{m-1}(x_i)+\alpha G(x_i)$ 与真实值 $y_i$ 之间的差距。</p><script type="math/tex; mode=display">(\alpha_m,G_m(x)) = arg min_{\alpha,G}\sum_{i=1}^{N}L(y_i,\quad f_{m-1}(x_i)+\alpha G(x_i))</script><p>GBDT树的学习过程为：计算伪残差-&gt; 生成基学习器 -&gt; 目标函数优化 -&gt; 更新模型</p><p>其思想类似于数值优化中梯度下降求参方法，参数沿着梯度的负方向以小步长前进，最终逐步逼近参数的局部最优解。在GB中模型每次拟合残差，逐步逼近最终结果。</p><p>如上所述，我们每个 stage 的优化目标是：<strong>（一阶泰勒）</strong></p><script type="math/tex; mode=display">G_m(x) = arg min_{G}\sum_{i=1}^{N}L(y_i,\quad f_{m-1}(x_i)+\alpha_m G_{m}(x_i))</script><script type="math/tex; mode=display">这一块太乱，回来再重新总结一遍。经过一阶泰勒展开得到：</script><script type="math/tex; mode=display">arg min_{G}\sum_{i=1}^{N}L(y_i,\alpha_m G_{m-1}(x_i))+f_{m-1}(x_i)+\frac{\partial L(y_i,f_m(x_i))}{\partial f_m(x_i)}\\</script><p>该函数比较难求解，类似于梯度下降方法，给定 $f_{m-1}(x_i)$ 的一个近似解，$\alpha_mG_m(x)$ 可以看作是 $f_{m-1}(x_i)$ 逼近 $f_m(x_i)$ 的步长和方向。所以：</p><script type="math/tex; mode=display">f_m(x)= f_{m-1}(x)-\alpha_m [\frac{\partial L(y_i,f_m(x_i))}{\partial f_m(x_i)}]_{f_m(x_i)=f_{m-1}(x_i)}</script><script type="math/tex; mode=display">\alpha_m = arg min_{\alpha}\sum_{i=1}^{N}L(y_i,f_{m-1}(x)-\alpha_m [\frac{\partial L(y_i,f_m(x_i))}{\partial f_m(x_i)}]_{f_m(x_i)=f_{m-1}(x_i)} )</script><p>上面的损失函数 L 可以根据问题的不同使用不同的损失函数，而且还可以<strong>加上模型复杂度的正则项等等来尽量避免过拟合</strong>。</p><p><strong>GBDT缺点</strong>：基于残差的gbdt在解决回归问题上不算好的选择，对异常值过于敏感；不适合高纬稀疏特征；不好并行计算。</p><h5 id="XGBoost算法"><a href="#XGBoost算法" class="headerlink" title="XGBoost算法"></a>XGBoost算法</h5><p>XGBoost和GBDT都是属于Grandient Boosting，本质思想与GBDT一致，构建多个基学习器使用加法模型，学习前面基学习器的结果与真实值的偏差，通过多个学习器的学习，不断降低模型值和实际值的差。XGBoost 可以认为是 GBDT 的一个实现，但是XGBoost相比GBDT做了如下的改进：</p><ul><li>GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到二阶，XGBoost保留更多有关目标函数的信息。</li><li>GBDT是给新的基模型寻找新的拟合标签（前面加法模型的负梯度），而XGBoost是给新的基模型寻找新的目标函数（目标函数关于新的基模型的二阶泰勒展开）。</li><li><strong>XGBoost加入了和叶子权重的L2正则化</strong>，有利于模型获得更低的方差。</li><li><strong>XGBoost增加了自动处理缺失值特征的策略</strong>，通过把带缺失值的样本划分到左子树或右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失值特征进行填充预处理。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。</li><li>支持并发，xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。<strong>xgboost的并行是在特征粒度上的</strong>。节点分裂时，计算特征增益是多线程并行的。</li><li><strong>基分类器：</strong>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。</li><li><strong>子采样：</strong>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。</li></ul><p>最终分类器可以用这个公式表示：</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^M G_m(x)</script><p>优化的目标：</p><script type="math/tex; mode=display">Obj = \sum_{i=1}^N L(y_i,f(x_i))+\sum_{m = 1}^{M}\Omega(G_m(x))</script><p>其中 $N$ 为样本数量，$y_i$ 表示样本真实值，$f(x)$ 是模型输出，所以前半部分代表模型的损失函数。$M$ 表示树的个数，$G_m(x)$ 表示第 $m$ 棵树，<strong>$\Omega$ 是模型复杂度函数，是一个正则项</strong>。模型越复杂，越容易出现过拟合，所以采用这样的目标函数，为了使得最终的模型既有很好的准确度也有不错的泛化能力。</p><p><strong>追加法训练：</strong></p><p>核心的思想是，已经训练好的树 $T1…T_{t-1}$ 不再调整。根据目标函数最小原则，新增树 $T_t$ ，表示如下：</p><script type="math/tex; mode=display">f_0(x) = 0 \quad 算法初始化</script><script type="math/tex; mode=display">f_1(x) = G_1(x) = f_0(x)+G_1(x)\quad 训练第１棵树 G_1(x)</script><script type="math/tex; mode=display">f_2(x) = G_1(x)+G_2(x) = f_1(x)+G_2(x) \quad 训练第２棵树 G_2(x)</script><script type="math/tex; mode=display">f_t(x) = \sum_{m = 1}^{t} f_m(x) = f_{t-1}(x)+G_t(x) \quad 训练第t棵树，前面t-1棵不再调整</script><p>假设此时对第 t 棵树训练，则目标函数表示为：</p><script type="math/tex; mode=display">Obj^{(t)} = \sum_{i=1}^N L(y_i,f_t(x_i))+\sum_{m = 1}^{t}\Omega(G_m(x)) \\=\sum_{i=1}^{N}L(y_i, f_{t-1}(x_i)+ G_t(x_i)+\Omega(G_t(x))+constant</script><p>其中 constant 常数代表 $\sum_{m=1}^{t-1}\Omega(G_m(x))$ ，因为前面 t-1 棵树结构不再变化，所以他们的复杂度为常数。</p><p>回顾高等数学中的泰勒展开，它使用一个函数的高阶导数，用多项式的形式逼近原始函数。当展开到二阶导数的时候公式如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/6.jpg" alt=""></p><p>利用泰勒展开公式和上面推导的 $Obj^{(t)}$ ， $Obj^{(t)}$ 式中，$f_{t-1}(x)$ 对应泰勒公式中的 $x$，而 $G_t(x)$ 是一棵待增加的新树，对应泰勒公式中的 $\Delta x$ ，$L(y_i,f_{t-1}(x))$ 对应泰勒公式中的 $f(x)$ ，而 $L(y_i, f_{t-1}(x_i)+ G_t(x_i)$ 对应泰勒公式中的 $f(x+\Delta x)$ ，则对 $L(y_i, f_{t-1}(x_i)+ G_t(x_i)$ 做二阶泰勒展开，得：</p><script type="math/tex; mode=display">Obj^{(t)} =\sum_{i=1}^{N}[L(y_i, f_{t-1}(x_i))+ g_iG_t(x_i)+\frac{1}{2}h_iG_t^2(x_i)]+\Omega(G_t(x))+constant</script><p>其中：</p><script type="math/tex; mode=display">g_i = \frac{\partial L(y_i, f_{t-1}(x_i))}{\partial f_{t-1}(x_i)} \quad 一阶导数 \\h_i = \frac{\partial L(y_i, f_{t-1}(x_i))}{\partial^2 f_{t-1}(x_i)} \quad 二阶导数</script><p>因为我们求解的目标是使得 $Obj^{(t)}$ 最小的 $G_t(x)$ 。当前面 $t-1$ 棵树都已经确定时， $\sum_{i=1}^{N}[L(y_i, f_{t-1}(x_i))$ 是一个常量，可以省略，constant 常量也可以省略，因此简化得到新的目标函数：</p><script type="math/tex; mode=display">Obj^{(t)} =\sum_{i=1}^{N}[ g_iG_t(x_i)+\frac{1}{2}h_iG_t^2(x_i)]+\Omega(G_t(x))</script><p>从XGBoost原理部分可以看出，XGBoost的实现中，采用了<strong>二阶泰勒</strong>展开公式展开来近似损失函数，同时在求解树的过程中，<strong>使用了贪心算法</strong>，并使用枚举特征，样本按照特征排序后，采用线性扫描找到最优分裂点。这种实现方法，是对GDBT思想的逼近，但是在工程上确非常有效。</p><h5 id="对比总结-1"><a href="#对比总结-1" class="headerlink" title="对比总结"></a>对比总结</h5><h4 id="K-means聚类算法"><a href="#K-means聚类算法" class="headerlink" title="K-means聚类算法"></a>K-means聚类算法</h4><h5 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h5><ul><li><p>算法简介：</p><ul><li><p>k-means是一种聚类算法，聚类就是指在不知道任何样本的标签情况下，通过数据之间的内在关系将样本分成若干个类别，使得相同类别样本之间的相似度搞，不同类别之间的样本相似度低。因此，<strong>k-mean算法是属于非监督学习的范畴</strong>。</p></li><li><p>k是指k个簇（cluster），means是指每个簇内的样本均值，也就是聚类中心</p></li><li><p>基本思想：通过迭代的方式寻找 k 个簇的划分方案，使得聚类结果对应的代价函数最小。<strong>代价函数可以定义为各个样本距离它所属的簇的中心点的误差平方和</strong>：</p><script type="math/tex; mode=display">J(c,\mu)=\sum_{i=1}^{N} ||x_{i} - \mu _{c_{i}}||^{2} \\其中，x_{i}代表第i个样本，c_{i}是x_{i}所属的簇，\mu_{c_{i}}代表簇对应的中心点（即均值），N是样本总数.</script></li></ul></li><li><p>算法流程：</p><p>k-means算法采用了<strong>贪心策略</strong>，通过多次迭代来近似求解上面的代价函数，具体算法流程如下：</p><ul><li><p>（1）随机选取 k 个点作为初始聚类（簇）中心，记为 $\mu_{1}^{(0)},\mu_{2}^{(0)},…,\mu_{k}^{(0)}$。</p></li><li><p>（2）计算每个样本 $x_{i}$ 到各个簇中心的距离（<strong>欧式距离</strong>，余弦相似度），将其分配到与它距离最近的簇。</p><script type="math/tex; mode=display">c_{i}^{(t)} \leftarrow \underset{k'}{argmin} ||x_{i}-\mu _{k'}^{(t)}||^{2} \\ 其中,t为当前迭代步数，k'为第k'个簇（类别）(k'=1,2,..,k)</script></li><li><p>（3）对于每个簇，利用该簇中的样本重新计算该簇的中心（即均值向量）：</p><script type="math/tex; mode=display">\mu_{k}^{(t+1)} \leftarrow \underset{\mu}{argmin} \sum_{i:c_{i}^{(t)}=k'} ||x_{i}-\mu||^{2}</script></li><li><p>（4）重复迭代上面2-3步骤 T 次，若聚类结果保持不变，则结束。</p></li></ul></li><li><p>k-means算法的优点</p><ul><li>算法简单易实现。</li><li>对于大数据集，这种算法相对可伸缩且是高效的，计算复杂度为 $O(TNk)$ 接近于线性（其中T是迭代次数、N是样本总数、k为聚类簇数）。</li><li>虽然以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。</li></ul></li><li><p>k-means算法的缺点</p><ul><li>需要人工预先确定初始K值，该值与实际的类别数可能不吻合。</li><li><strong>K均值只能收敛到局部最优。</strong>因为求解这个代价函数是个NP问题，采用的是贪心策略，所以只能通过多次迭代收敛到局部最优，而不是全局最优。</li><li><strong>K均值的效果受初始值和离群点的影响大。</strong>因为 k 均值本质上是基于距离度量来划分的，均值和方差大的维度将对数据的聚类结果产生决定性的影响，因此需要进行<strong>归一化处理</strong>；此外，离群点或噪声对均值会产生影响，导致中心偏移，因此需要进行预处理。</li><li>对于数据簇分布差别较大的情况聚类效果很差。例如一个类别的样本数是另一类的100倍。</li><li>样本只能被划分到一个单一的类别中。</li><li>不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。</li><li>对异常值敏感。</li></ul></li></ul><h5 id="K-means-算法"><a href="#K-means-算法" class="headerlink" title="K-means++算法"></a>K-means++算法</h5><p>由于 k-means 算法中，初始K值是人为地凭借经验来设置的，聚类的结果可能不符合实际的情况。因此，K值的设置对算法的效果影响其实是很大的，那么，如何设置这个K值才能取得更好的效果呢？</p><p><strong>k-means++的主要思想：</strong></p><ul><li>首先随机选取1个初始聚类中心（n=1）。</li><li>假设已经选取了n个初始聚类中心（0&lt;n&lt;k），那么在选择第n+1个聚类中心时，距离当前已有的n个聚类中心越远的点越有可能被选取为第n+1个聚类中心。</li><li>按照 k-means 算法去优化。</li><li>可以这样理解上面的思想：<strong>聚类中心自然是越互相远离越好，即不同的类别尽可能地分开</strong>。</li><li>缺点：难以并行化。</li></ul><blockquote><p>k-means++算法虽然可以更好地初始k个聚类中心，但还是不能解决一个问题，k 值应该取多少才算合理？</p></blockquote><h5 id="如何选取K值？"><a href="#如何选取K值？" class="headerlink" title="如何选取K值？"></a>如何选取K值？</h5><p>如何才能合理地选取k值是k-means算法最大的问题之一，一般可以采取手肘法和<code>Gap Statistic</code>方法。</p><h6 id="手肘法"><a href="#手肘法" class="headerlink" title="手肘法"></a>手肘法</h6><p>k值的选择一般基于经验或者多次实验来确定，手肘法便是如此，其主要思想是：通过多次实验分别选取不同的k值，将不同k值的聚类结果对应的最小代价画成折线图，将曲线趋于平稳前的拐点作为最佳k值。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/8.png" style="zoom:50%;" /></p><blockquote><p>上图中，k取值在1~3时，曲线急速下降；当k&gt;3时，曲线趋于平稳。因此，在k=3处被视为拐点，所以手肘法认为最佳的k值就是3。</p></blockquote><h6 id="Gap-Statistic"><a href="#Gap-Statistic" class="headerlink" title="Gap Statistic"></a>Gap Statistic</h6><p>Gap Statistics 定义为：</p><script type="math/tex; mode=display">Gap(k)=E(logD_{k})-logD_{k} \\其中，D_{k}是第k簇聚类对应的损失值，E(logD_{k})是logD_{k}的期望。</script><p>对于上式的 $E(logD_{k})$，一般通过蒙特卡洛模拟产生。具体操作是：在样本所在的区域内，按照均匀分布随机产生和原样本数目一样的随机样本，计算这些随机样本的均值，得到一个 $D_{k}$，重复多次即可计算出 $E(logD_{k})$ 的近似值。</p><p>$Gap(k)$ 可以看做是随机样本的损失与实际样本的损失之差，假设实际样本最佳的簇类数目为 k，那么实际样本的损失应该相对较小，随机样本的损失与实际样本的损失的差值相应地达到最大，即<strong>最大的 $Gap(k)$ 值应该对应最佳的k值。</strong></p><p>因此，我们只需要用不同的k值进行多次实验，找出使得$Gap(k)$最大的k即可。</p><blockquote><p>到现在为止我们可以发现，上面的算法中，k值都是通过人为地凭借经验或者多次实验事先确定下来了的，但是当我们遇到高维度、海量的数据集时，可能就很难估计出准确的k值。那么，有没有办法可以帮助我们自动地确定k值呢？有的，下面来看看另一个算法。</p></blockquote><h5 id="ISODATA-算法"><a href="#ISODATA-算法" class="headerlink" title="ISODATA 算法"></a>ISODATA 算法</h5><p><strong>ISODATA 的全称是迭代自组织数据分析法。它解决了 K 的值需要预先人为的确定这一缺点。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出 K 的大小。I</strong>SODATA 就是针对这个问题进行了改进。</p><p><strong>主要思想：</strong></p><ul><li>当某个类别样本数目过多、分散程度较大时，将该类别分为两个子类别。（分裂操作，即增加聚类中心数）</li><li>当属于某个类别的样本数目过少时，把该类别去除掉。（合并操作，即减少聚类中心数）</li></ul><p><strong>算法优点：</strong>可以自动寻找合适的k值。</p><p><strong>算法缺点：</strong> 除了要设置一个参考聚类数量 $k_{0}$ 外，还需要指定额外的3个阈值，来约束上述的分裂和合并操作。具体如下：</p><ol><li>预期的聚类数目  $k_{0}$ 作为参考值，最终的结果在  $k_{0}$ 的一半到两倍之间。</li><li>每个类的最少样本数目 $N_{min}$，若分裂后样本数目会少于该值，则该簇不会分裂。</li><li>最大方差  $Sigma$，用于控制某个簇的样本分散程度，操作该值且满足条件2，则分裂成两个簇。</li><li>两个簇最小距离  $D_{min}$，若两个簇距离小于该值，则合并成一个簇。</li></ol><h5 id="K-means和-KNN-算法的区别"><a href="#K-means和-KNN-算法的区别" class="headerlink" title="K-means和 KNN 算法的区别"></a>K-means和 KNN 算法的区别</h5><p><strong>KNN：</strong>KNN的全称是K Nearest Neighbors，意思是K个最近的邻居。K个最近邻居，K的取值肯定是至关重要的。那么最近的邻居又是怎么回事呢？其实啊，KNN的原理就是当预测一个新的值x的时候，根据它距离最近的K个点是什么类别来判断x属于哪个类别。<strong>注意KNN算法是有监督学习中的分类算法。</strong></p><ul><li><p><strong>1、距离的计算</strong>：欧式距离；曼哈顿距离；闵可夫斯基距离。</p></li><li><p><strong>2、K值的选取</strong>：<strong>通过交叉验证</strong>（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。</p><p>当你增大k的时候，一般错误率会先降低，因为有周围更多的样本可以借鉴了，分类效果会变好。但注意，当K值继续增大的时候，错误率会更高。这也很好理解，比如说你一共就35个样本，当你K增大到30的时候，KNN基本上就没意义了。</p><p>选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合。<br>　　<br>选择较大的k值，就相当于用较大邻域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。</p><p><strong>选择K点的时候可以选择一个较大的临界K点，当它继续增大或减小的时候，错误率都会上升。</strong></p></li><li><p><strong>3、算法流程</strong>：</p><p>（1）计算测试数据与各个训练数据之间的距离；</p><p>（2）按照距离的递增关系进行排序；</p><p>（3）选取距离最小的K个点；</p><p>（4）确定前K个点所在类别的出现频率；</p><p>（5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。</p></li><li><p><strong>KNN的特点：非参，惰性</strong></p><ul><li>非参：<strong>非参</strong>的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的。</li><li>惰性：KNN算法不需要大量的训练过程得到一个算法模型，他没有明确的训练数据的过程，或者说这个过程很快。</li></ul></li><li><p>优点：简单易用；模型训练时间快；预测效果好；对异常值不敏感</p></li><li><p>缺点：对内存要求较高，因为该算法存储了所有训练数据；预测阶段可能很慢；对不相关的功能和数据规模敏感</p></li></ul><p><strong>k-means和KNN的不同点：</strong></p><ul><li><code>KNN</code>为分类，<code>K-means</code>为聚类；</li><li><code>KNN</code>为监督学习，<code>K-means</code>为无监督学习；</li><li><code>KNN</code>的输入样本为带<code>label</code>的，<code>K-means</code>的输入样本不带<code>label</code>；</li><li><code>KNN</code>没有训练过程，<code>K-means</code>有训练过程；</li><li><code>K</code>的含义不同：<ul><li>KNN：直接把待分类点周边最近的k个点计数，数量多的那类定为待分类点的类别。无训练的过程。</li><li>K-means：先定好k个类别，然后随机确定k个坐标（聚类中心），各点离哪个坐标近就算做哪类，然后不停算平均值求出中心，直到稳定，聚类完成。有训练的过程。</li></ul></li></ul><p><strong>相同点：</strong>都是计算最近邻，一般都用欧氏距离。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230823/7.png" alt=""></p><h5 id="K-means和GMM算法的区别"><a href="#K-means和GMM算法的区别" class="headerlink" title="K-means和GMM算法的区别"></a>K-means和GMM算法的区别</h5><p>高斯混合模型（GMM），GMM是指具有如下形式的概率分布模型</p><script type="math/tex; mode=display">P(x|\theta)=\sum_{k=1}^{K} \alpha_{k} \phi (x|\theta_{k})\\</script><script type="math/tex; mode=display">其中，\alpha_{k}是高斯混合系数，\alpha_{k} \geq 0 \ 且\sum_{k=1}^{K}\alpha_{k}=1;</script><script type="math/tex; mode=display">\theta_{k}=(\mu_{k},\sigma_{k}^{2});</script><script type="math/tex; mode=display">\phi (x|\theta_{k})是第k个高斯分布模型的概率密度函数，具体形式如下：</script><script type="math/tex; mode=display">\phi (x|\theta_{k})=\frac{1}{\sqrt {2\pi} \sigma_{k}} exp\left ( -\frac{(y-\mu_{k})^{2}}{2\sigma_{k}^{2}} \right )</script><script type="math/tex; mode=display">其中，u_k是均值，\sigma_{k}^2是方差</script><p>$\phi (x|\theta_{k})$的值代表：x由参数$\theta_{k}$下的高斯模型生成的概率。据此可以<strong>==判断x属于第k个高斯模型生成的概率。==</strong></p><p><strong>GMM聚类：</strong>高斯混合模型（GMM）聚类的思想和 k-means 其实有点相似，都是通过迭代的方式将样本分配到某个簇类中，然后更新簇类的信息，不同的是GMM是基于概率模型来实现的，而 k-means 是非概率模型，采用欧氏距离的度量方式来分配样本。</p><p><strong>GMM聚类主要思想和流程：</strong></p><p>​    每个GMM由K个混合成分组成，每个混合成分都是一个高斯分布，$\alpha_{k}$ 为相应的混合系数。GMM模型假设所有的样本都根据高斯混合分    布生成，那么<strong>每个高斯分布其实就代表了一个簇类</strong>。具体流程如下：</p><p>​    （1）先初始化高斯混合模型的参数 $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$ ，训练一个GMM模型需要估计这些参数，如何估计后面会介绍。</p><p>​    （2）对每个样本，固定各个高斯分布，计算样本在各个高斯分布上的概率（即该样本是由某个高斯分布生成而来的概率）。</p><p>​    （3）然后固定样本的生成概率，更新参数以获得更好的高斯混合分布。</p><p>​    （4）迭代至指定条件结束。</p><p><strong>EM算法估计GMM参数</strong></p><p>上面提到，要训练一个GMM模型，就需要估计每个高斯分布的参数 $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$，才能知道每个样本是由哪个高斯混合成分生成的，也就是说，数据集的所有样本是可观测数据， $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$ 这些是待观测数据(隐变量)，而估计待观测数据常用的算法就是EM算法。</p><p><strong>EM（Expectation-Maximization）算法</strong>是常用的估计参数隐变量的礼器，它是一种迭代的方法，其主要的思想就是：若<strong>参数</strong>$\theta$ 已知，则可以根据训练数据推断出最优隐变量<strong>Z</strong>的值（E步），若Z的值已知，则可方便的对<strong>参数$\theta$</strong> 做最大似然估计（M步）。</p><p>EM算法估计高斯混合模型的参数的步骤：</p><ul><li><p>（1）给定数据集$D=\{x_{1},x_{2},…,x_{m} \}$，初始化高斯混合分布的模型参数 $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$。</p></li><li><p><strong>（2）E步：</strong>遍历每个样本，对每个样本 $x_{i}$，计算其属于第k个高斯分布的概率：</p><script type="math/tex; mode=display">\gamma_{ik}=\frac{\alpha_{k}\phi(x_{i}|\theta_{k})}{\sum_{k=1}^{K}\alpha_{k}\phi(x_{i}|\theta_{k})} \ ,\quad 其中，\theta_{k}=(\mu_{k},\sigma_{k}^{2})</script></li><li><p><strong>（3）M步：</strong>更新各个高斯分布的参数为$\{(\hat{\alpha}_{k},\hat{\mu}_{k},\hat{\sigma}_{k}^{2})\ | \ 1\leq k \leq K \ \}$ :</p></li></ul><script type="math/tex; mode=display">\hat{\alpha}_{k}=\frac{\sum_{i=1}^{m} \gamma_{ik}}{m} --- m个样本在该类的平均概率</script><script type="math/tex; mode=display">\hat{\mu}_{k}=\frac{\sum_{i=1}^{m} \gamma_{ik} x_{i}}{\sum_{i=1}^{m} \gamma_{ik}}  --- m个样本特征值的加权平均</script><script type="math/tex; mode=display">\hat{\sigma}_{k}^{2}=\frac{\sum_{i=1}^{m} \gamma_{ik} (x_{i}-\mu_{k})^{2}}{\sum_{i=1}^{m} \gamma_{ik}}  --- m个样本的方差加权平均</script><ul><li><p>（4）重复2-3步，直至收敛。</p><blockquote><p>注意，EM算法通过迭代的方式估计GMM模型的参数，得到的是<strong>局部最优解</strong>而不是全局最优。</p></blockquote></li></ul><p><strong>在迭代收敛后，遍历所有的样本，对于每个样本 $x_{i}$，计算它在各个高斯分布中的概率，将样本划分到概率最大的高斯分布中</strong>（每个高斯分布都相当于是一个簇类，因此可以理解为是将每个样本划分到相应的类别中，<strong>不过实际上是给出属于每个类别的概率而非属于某个类别）。</strong></p><p><strong>k-means和GMM的区别：</strong></p><ul><li><p>k-means算法是非概率模型，而GMM是概率模型</p><blockquote><p>k-means算法基于欧氏距离的度量方式将样本划分到与它距离最小的类，而GMM则是计算各个高斯分布生成样本的概率，将样本划分到取得最大概率的高斯分布中</p></blockquote></li><li><p>两者计算的参数不一样</p><blockquote><p>k-means计算的是簇类的均值，GMM计算的是高斯分布的参数（均值，方差和高斯混合系数）</p></blockquote></li><li><p>k-means是硬聚类，要么属于这一类要么属于那一类；而GMM算法是软聚类，给出的是属于某些类别的概率。</p></li><li><p>GMM每一步迭代的计算量比k-means要大。</p></li></ul><p><strong>k-means和GMM的区别：</strong></p><ul><li>都是聚类算法</li><li>都需要指定K值，且都受初始值的影响。k-means初始化k个聚类中心，GMM初始化k个高斯分布。</li><li>都是通过迭代的方式求解，而且都是局部最优解。k-means的求解过程其实也可以用EM算法的E步和M步来理解。</li></ul><h4 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h4><h5 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h5><p><strong>支持向量机（support vector machine）SVM</strong>：就是有监督学习的二分类分类模型，他的基本模型是定义在特征空间上的间隔最大的线性分类器，SVM的学习策略就是间隔最大化。</p><blockquote><p>支持向量机（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。</p></blockquote><p><strong>直观理解</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/9.png" style="zoom:67%;" /></p><p>图中有分别属于两类的一些二维数据点和三条直线。如果三条直线分别代表三个分类器的话，请问哪一个分类器比较好？</p><p>我们凭直观感受应该觉得答案是H3。首先H1不能把类别分开，这个分类器肯定是不行的；H2可以，但分割线与最近的数据点<strong>只有很小的间隔</strong>，如果测试数据有一些噪声的话可能就会被H2错误分类(即对噪声敏感、泛化能力弱)。<strong>H3以较大间隔</strong>将它们分开，这样就能容忍测试数据的一些噪声而正确分类，是一个泛化能力不错的分类器。</p><p>对于支持向量机来说，数据点若是p维向量，我们用p−1维的<strong>超平面</strong>来分开这些点。但是可能有许多超平面可以把数据分类。最佳超平面的一个合理选择就是以最大间隔把两个类分开的超平面。因此，<strong>SVM选择能够使离超平面最近的数据点到超平面距离最大的超平面。</strong></p><p>以上介绍的SVM只能解决线性可分的问题，为了解决更加复杂的问题，支持向量机学习方法有一些由简至繁的模型:</p><ul><li><p>线性可分SVM</p><blockquote><p>当训练数据线性可分时，通过硬间隔(hard margin，什么是硬、软间隔下面会讲)最大化可以学习得到一个线性分类器，即硬间隔SVM，如上图的的H3。</p></blockquote></li><li><p>线性SVM</p><blockquote><p>当训练数据不能线性可分但是可以近似线性可分时，通过软间隔(soft margin)最大化也可以学习到一个线性分类器，即软间隔SVM。</p></blockquote></li><li><p>非线性SVM</p><blockquote><p>当训练数据线性不可分时，通过使用核技巧(kernel trick)和软间隔最大化，可以学习到一个非线性SVM。</p></blockquote></li></ul><h5 id="线性可分SVM-硬间隔"><a href="#线性可分SVM-硬间隔" class="headerlink" title="线性可分SVM-硬间隔"></a>线性可分SVM-硬间隔</h5><p>如下形式的线性可分的训练数据集</p><script type="math/tex; mode=display">(X_1,y_1), (X_2,y_2),....(X_i,y_i)</script><p>其中 $X_i \in R^d$ 是一个含有d个元素的列向量，$y_i$ 是标量，$y_i \in \{ -1, 1\}$，$y_i=1$ 时表示$X_i$属于正类别，反之亦然。</p><blockquote><p>注：$X$, $X_i$, $W$ 等都是（列）向量</p></blockquote><p><strong>感知机的目标</strong>: 找到一个超平面使其能正确地将每个样本正确分类。感知机使用误分类最小的方法求得超平面，不过此时解有无穷多个 (例如图1.1的H2和H3以及它俩的任意线性组合)。<strong>而线性可分支持向量机利用间隔最大化求最优分离超平面，这时解是唯一的。</strong></p><p>假设超平面$(W,b)$ 能够将训练样本正确分类，对于$(X_i,y_i)$ ,若 $y_i=+1$，则有 $X_{i}^{T} W+b \geq0$, ,若 $y_i=-1$，则有 $X_{i}^{T} W+b \leq 0$， 令：</p><script type="math/tex; mode=display">X_{i}^{T} W+b \geq+1, y_{i}=+1</script><script type="math/tex; mode=display">X_{i}^{T} W+b \leq-1, y_{i}=-1 \tag{2.2.1}</script><h6 id="超平面与间隔"><a href="#超平面与间隔" class="headerlink" title="超平面与间隔"></a><strong>超平面与间隔</strong></h6><p>一个超平面由法向量 $W$ 和截距 $b$ 决定，其方程如下，</p><script type="math/tex; mode=display">X^TW+b=0 \tag{2.2.2}</script><p>其中法向量决定了超平面的方向，截距决定了超平面与远点的距离。我们规定<strong>法向量指向的一侧为正类,另一侧为负类</strong>。下图画出了三个平行的超平面，法方向取左上方向。</p><blockquote><p>$X$ 和 $W$ 都是列向量，$X^TW$ 会得到二者的点积是一个标量</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/10.jpg" style="zoom:33%;" /></p><p>为了找到最大间隔超平面，我们可以先选择分离两类数据的两个平行超平面，使得它们之间的距离尽可能大。**在这两个超平面范围内的区域称为“间隔(margin)”，最大间隔超平面是位于它们正中间的超平面。这个过程如上图所示。</p><h6 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a><strong>间隔最大化</strong></h6><p>将高数里面求两条平行直接的距离公式推广到高维中可以求得上图中的<strong>间隔(margin)</strong>的  为</p><script type="math/tex; mode=display">margin = \rho = \frac{2}{||W||} \tag{2.2.3}</script><p>目标为 $\rho$ 最大，等价于 $\rho^2$ 最大</p><script type="math/tex; mode=display">\max _{W, b} \rho \Longleftrightarrow \max _{W, b} \rho^{2} \Longleftrightarrow \min _{W, b} \frac{1}{2}\|W\|^{2} \tag{2.2.4}</script><p>同时别忘记了约束条件：</p><script type="math/tex; mode=display">X_{i}^{T} W+b \geq+1, y_{i}=+1</script><script type="math/tex; mode=display">X_{i}^{T} W+b \leq-1, y_{i}=-1 \tag{2.2.5}</script><p>总结一下，间隔最大化问题的数学表达就是：</p><script type="math/tex; mode=display">\begin{array}{l}\min _{W, b} J(W)=\min _{W, b} \frac{1}{2}\|W\|^{2} \end{array}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(X_{i}^{T} W+b\right) \geq 1, i=1,2, \ldots n . \tag{2.2.6}</script><p>这就是支持向量机的基本型，下面就是求解过程。</p><h6 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h6><p>在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的数据点称为支持向量(support vector)，支持向量是使 式（2.5）中的约束条件取等的点，即满足：</p><script type="math/tex; mode=display">y_i((X_i^TW = b) =1</script><p>的点，也就是在直线 $X_{i}^{T} W+b= +1$ 或者  $X_{i}^{T} W+b= -1$ 的点，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/11.jpg" alt=""></p><p><strong>在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用 </strong>(具体推导见后面)。如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响。也即支持向量对模型起着决定性的作用，这也是“支持向量机”名称的由来。</p><h6 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h6><p>如何求解？</p><script type="math/tex; mode=display">\begin{array}{l}\min _{W, b} J(W)=\min _{W, b} \frac{1}{2}\|W\|^{2} \end{array}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(X_{i}^{T} W+b\right) \geq 1, i=1,2, \ldots n . \tag{2.4.1}</script><p>该公式本身是一个凸二次规划问题，虽然可以直接用现成的优化计算包求解，但是不够高效。定义该公式所述问题为<strong>原始问题</strong>，我们可以拉格朗日乘子法构造拉格朗日函数再通过求解其<strong>对偶问题</strong>得到原始问题的最优解，转换为对偶问题来求解的原因是：</p><ul><li>对偶问题更易求解，由下文知对偶问题只需优化一个变量 $\alpha$ 且约束条件更简单，更加高效；</li><li>能更加自然地引入<strong>核函数</strong>，进而推广到非线性问题。</li></ul><p>首先需要构建拉格朗日函数，为此引进拉格朗日乘子 $\alpha_i \geq 0, i=1,2,..,n $，则该问题的拉格朗日函数为：</p><script type="math/tex; mode=display">L(W, b, \alpha)=\frac{1}{2}\|W\|^{2} + \sum_{i=1}^{n} \alpha_{i}\left[ 1- y_{i}\left(X_{i}^{T} W+b\right)\right] \tag{2.4.2}</script><p>因此，给定$(W,b)$ ，若不满足式(2.4.1) 的约束条件，那么：</p><script type="math/tex; mode=display">\max _{\alpha} L(W, b, \alpha)=+\infty \tag{2.4.3}</script><p>否则，若满足式(2.4.1) 的约束条件，那么：</p><script type="math/tex; mode=display">\max _{\alpha} L(W, b, \alpha)=J(W)=\frac{1}{2}\|W\|^{2} \tag{2.4.4}</script><p>结合式(2.4.3) 和式(2.4.4)，优化问题为：</p><script type="math/tex; mode=display">\min _{W，b}\max _{\alpha} L(W, b, \alpha) \tag{2.4.5}</script><p>这与支持向量机的基本型式(2.4.1) 所述问题是完全等价的。</p><p>根据拉格朗日对偶性，式(2.4.1)所述的原始问题的对偶问题是：</p><script type="math/tex; mode=display">\max _{\alpha}\min _{W，b} L(W, b, \alpha) \tag{2.4.6}</script><p><strong>为了求得对偶问题的解，需要先求得 $L(W, b, \alpha)$ 对 $W$ 和 $b$ 的极小再求对α的极大。</strong></p><p>(1) 求 $\min _{W，b} L(W, b, \alpha)$ ：求拉格朗日函数对 $W$ 和 $b$ 的偏导。并且偏导等于0，有</p><script type="math/tex; mode=display">\nabla_{W} L(W, b, \alpha)=W-\sum_{i=1}^{n} \alpha_{i} y_{i} X_{i}=0 \Longrightarrow W=\sum_{i=1}^{n} \alpha_{i} y_{i} X_{i} \tag{2.4.7}</script><script type="math/tex; mode=display">\\\nabla_{b} L(W, b, \alpha)=-\sum_{i=1}^{n} \alpha_{i} y_{i}=0 \Longrightarrow \sum_{i=1}^{n} \alpha_{i} y_{i}=0 \tag{2.4.8}</script><p>将上面两个式代入 $L(W, b, \alpha)$, 可将其中的 $W$ 和 $b$ 消去，，</p><script type="math/tex; mode=display">\begin{array}{l}L(\mathbf{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\mathbf{w}\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left[y_{i}\left(\mathbf{x}_{i}^{T} \mathbf{w}+b\right)-1\right] \\=\frac{1}{2} \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i}^{T} \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j}-\sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i}^{T} \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j}-b \sum_{i=1}^{n} \alpha_{i} y_{i}+\sum_{i=1}^{n} \alpha_{i} \\=\sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i}^{T} \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j}=\sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{n} y_{i} y_{j} \alpha_{j} \alpha_{j} \mathbf{x}_{j}^{T} \mathbf{x}_{i}\end{array} \tag{2.4.9}</script><p>所以：</p><script type="math/tex; mode=display">\min _{W，b} L(W, b, \alpha) = \sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} y_{i} y_{j} \alpha_{i} \alpha_{j} \mathbf{x}_{j}^{T} \mathbf{x}_{i} \tag{2.4.10}</script><p>（2）对 $\min _{W，b} L(W, b, \alpha)$  对 $\alpha$ 的极大：就得到原始问题的对偶问题的最优化。</p><script type="math/tex; mode=display">\max _{\alpha}\min _{W，b} L(W, b, \alpha)  = \max _{\alpha} \sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{i=1}^{n} y_{i} y_{j} \alpha_{i} \alpha_{j} \mathbf{x}_{j}^{T} \mathbf{x}_{i}</script><script type="math/tex; mode=display">s.t. \quad \sum_{i=1}^{n} \alpha_{i} y_{i}=0,</script><script type="math/tex; mode=display">\quad \alpha_i \geq 0, i=1,2,..,n  \tag{2.4.11}</script><p>（3）如何求解优化问题式 (4.11) 的最优解 $\hat{\alpha}$ ，这是一个二次规划问题，，有现成的通用算法求解。</p><blockquote><p>事实上通用的求解二次规划问题的算法的复杂度正比于训练数据样本数，所以在实际应用中需要寻求更加高效的算法，例如序列最小优化(Sequential Minimal Optimiation, SMO)算法。</p></blockquote><p>（4）假设，我们求得了式  (2.4.11) 的最优解 $\hat{\alpha}$ ，则可以根据式(2.4.7) 求得最优 $\hat{W}$,</p><script type="math/tex; mode=display">\hat{W} = \sum_{i=1}^{n} \hat{\alpha_{i}} y_{i} X_{i} \tag{2.4.12}</script><p>因为至少存在一个 $\hat{\alpha_j} &gt; 0$，若不存在$(\alpha_i=0)$,则 $\hat{W} = 0$,得到 $margin = \frac{2}{||W||} = +\infty$,（显然不行）。再根据原始问题式(4.1)中是有不等式越苏的，因此上述过程需满足KKT(Karush-Kuhn-Tucker)条件，即要求：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}\text { 乘子非负: } \alpha_{i} \geq 0(i=1,2, \ldots n . \text { 下同 }) \\\text { 约束条件: } y_{i}\left(X_{i}^{T} W+b\right)-1 \geq 0 \\\text { 互补条件 }: \alpha_{i}\left(y_{i}\left(X_{i}^{T} W+b\right)-1\right)=0\end{array}\right.  \tag{2.4.13}</script><p>所以至少存在一个j，使得 $y_i(X_{i}^{T} \hat{W}+\hat{b}) - 1=0$，即可以求得最优的 $\hat{b}$:</p><script type="math/tex; mode=display">\hat{b}=\frac{1}{y_{j}}-X_{j}^{T} \hat{W} = y_{j}-X_{j}^{T} \hat{W} = y_{j}-\sum_{i=1}^{n} \hat{\alpha}_{i} y_{i} X_{j}^{T} X_{i}  \tag{2.4.14}</script><p>（5）至此，我们求得了整个线性可分SVM的解，求得的分离超平面为：</p><script type="math/tex; mode=display">\sum_{i=1}^{n} \hat{\alpha_{i}} y_{i} X^T X_{i} + \hat{b} = 0  \tag{2.4.15}</script><p>支持向量和非支持向量：<strong>最优超平面只与支持向量有关而与非支持向量无关。</strong></p><p>再来分析KKT条件里的互补条件，对于任意样本 $(X_i, y_i)$ ，总会有 $\alpha_i=0$ 或者 $y_if(X_i) = y_i(X_i^T\hat{W} + b) = 1$。则有若 $\alpha_i=0$ ，此样本点不是支持向量，对模型没有任何作用；若 $\alpha_i&gt;0$ ，此样本点位于最大间隔边界上，是一个支持向量，如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/15.jpg" style="zoom:33%;" /></p><p>则分类的决策函数为</p><script type="math/tex; mode=display">f(X) = sign(\sum_{i=1}^{n} \hat{\alpha_{i}} y_{i} X^T X_{i} + \hat{b}) \tag{2.4.16}</script><h5 id="线性SVM-软间隔"><a href="#线性SVM-软间隔" class="headerlink" title="线性SVM-软间隔"></a>线性SVM-软间隔</h5><p>在前面的讨论中，我们一直假定训练数据是严格线性可分的，即存在一个超平面能完全将两类数据分开。但是现实任务这个假设往往不成立，例如下图所示的数据。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/13.jpg" style="zoom:50%;" /></p><h6 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h6><p>解决该问题的一个办法是允许SVM在少量样本上出错，即将之前的硬间隔最大化条件放宽一点，为此引入“软间隔(soft margin)”的概念。即允许少量样本不满足约束：</p><script type="math/tex; mode=display">y_i((X_i^TW = b) \geq 1  \tag{3.1.1}</script><p>为了使不满足上述条件的样本点尽可能少，我们需要在优化的目标函数(2.2.4)里面新增一个对这些点的惩罚项。最常用的是hinge损失:</p><script type="math/tex; mode=display">l_{hinge}(z) = max(0, 1-z) \tag{3.1.2}</script><p>即若样本点满足约束条件损失就是0, 否则损失就是 $1-z$, 则优化目标(2.2.4)变成:</p><script type="math/tex; mode=display">\min _{W, b} \frac{1}{2}\|W\|^{2} + C \sum_{i=1}^{n}max(0, 1-y_i(X^TW+b))  \tag{3.1.3}</script><p><strong>其中 $C&gt;0$ 称为惩罚参数</strong>，C越小时对误分类惩罚越小，越大时对误分类惩罚越大，当C取正无穷时就变成了硬间隔优化。实际应用时我们要合理选取C，C越小越容易欠拟合，C越大越容易过拟合。</p><p>如果我们引入 <strong>松弛变量</strong> $\xi_{i} \geq 0$，那种优化目标可以重写为：</p><script type="math/tex; mode=display">\min _{W, b} \frac{1}{2}\|W\|^{2} + C \sum_{i=1}^{n} \xi_{i}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(X_{i}^{T} W+b\right) \ge 1-\xi_{i}</script><script type="math/tex; mode=display">\xi_{i} \ge 0 ,i=1,2,...n.  \tag{3.1.4}</script><p><strong>上式所述问题即软间隔支持向量机。</strong></p><h6 id="对偶问题-1"><a href="#对偶问题-1" class="headerlink" title="对偶问题"></a>对偶问题</h6><p>式(3.1.4)表示的软间隔支持向量机依然是一个凸二次规划问题，和硬间隔支持向量机类似，我们可以通过拉格朗日乘子法将其转换为对偶问题进行求解。 式(3.1.4)对应的拉格朗日函数为：</p><script type="math/tex; mode=display">L(W, b, \xi, \alpha, \beta)=\frac{1}{2}\|W\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i}\left[y_{i}\left(X_{i}^{T} W+b\right)-1+\xi_{i}\right]-\sum_{i=1}^{n} \beta_{i} \xi_{i} \tag{3.2.1}</script><p>类似上面，为了求得对偶问题的解，我们需要先求得 $L(W, b, \xi, \alpha, \beta)$ 对 $( W,b, \xi)$ 的极小再求对 $(\alpha, \beta )$ 的极大。</p><p>（1）求 $\min _{W，b， \xi} L(W, b, \xi, \alpha, \beta)$：将 $ L(W, b, \xi, \alpha, \beta) $ 分别对  $( W,b, \xi)$ 求偏导，并且偏导为0，得到：</p><script type="math/tex; mode=display">W=\sum_{i=1}^{n} \alpha_{i} y_{i} X_{i}</script><script type="math/tex; mode=display">\sum_{i=1}^{n} \alpha_{i} y_{i}=0</script><script type="math/tex; mode=display">C = \alpha_i + \beta_i  \tag{3.2.2}</script><p>将上面两个式代入 $L(W, b, \xi, \alpha, \beta)$, 可将其中的 $W$ 和 $b$ 和 $\beta$ 消去，</p><script type="math/tex; mode=display">\min _{W，b, \xi} L(W, b, \xi, \alpha, \beta) = \sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} y_{i} y_{j} \alpha_{i} \alpha_{j} \mathbf{x}_{j}^{T} \mathbf{x}_{i} \tag{3.2.3}</script><p>（2）对 $\min _{W，b, \xi} L(W, b, \alpha, \beta)$  对 $\alpha$ 的极大：就得到原始问题的对偶问题的最优化。</p><script type="math/tex; mode=display">\max _{\alpha}\min _{W，b, \xi} L(W, b, \alpha, \beta)  = \max _{\alpha} \sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{i=1}^{n} y_{i} y_{j} \alpha_{i} \alpha_{j} \mathbf{x}_{j}^{T} \mathbf{x}_{i}</script><script type="math/tex; mode=display">s.t. \quad \sum_{i=1}^{n} \alpha_{i} y_{i}=0,</script><script type="math/tex; mode=display">\quad 0 \leq \alpha_i \leq C, i=1,2,..,n  \tag{3.2.4}</script><p>（3）类似上面，可以利用现成的通用算法，例如SMO算法 ，求式(3.2.4)。</p><p>（4）KTT条件：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}\text { 乘子非负: } \alpha_{i} \geq 0, \beta_{i} \geq 0(i=1,2, \ldots n . \text { 下同 }) \\\text { 约束条件: } y_{i}\left(X_{i}^{T} W+b\right)-1 \geq \xi_i \\\text { 互补条件 }: \alpha_{i}\left[y_{i}\left(X_{i}^{T} W+b\right)-1 + \xi_i\right]=0, \beta_i\xi_i=0\end{array}\right.  \tag{3.2.5}</script><p>对于任意样本 $(X_i,y_i)$，若 $\alpha_i=0$，此样本点不是支持向量，该样本对模型没有任何的作用；若 $\alpha_i&gt;0$，此样本是一个支持向量。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/14.jpg" style="zoom:33%;" /></p><p>因此，<strong>最优超平面只与支持向量有关而与非支持向量无关。</strong></p><h6 id="惩罚参数C"><a href="#惩罚参数C" class="headerlink" title="惩罚参数C"></a>惩罚参数C</h6><p>对于不同惩罚参数C，SVM结果如下图所示</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/16.jpg" alt=""></p><p>再来看看我们的原始目标函数:</p><script type="math/tex; mode=display">\min _{W, b} \frac{1}{2}\|W\|^{2} + C \sum_{i=1}^{n} \xi_{i} \tag{3.2.6}</script><p>对于更加一般化的问题，可将上述式子抽象成：</p><script type="math/tex; mode=display">\min _{f} \Omega(f)+C \sum_{i=1}^{n} l\left(f\left(x_{i}\right), y_{i}\right)  \tag{3.2.7}</script><p>前一项可以理解为“结构风险(structural risk)”，用来描述所求模型的某些性质(SVM就是要求间隔最大)；第二项称为“经验风险(empirical risk)”，用来描述模型与训练数据的契合程度(即误差)。而参数C就是用于对二者的折中,即我们一方面要求模型要满足某种性质另一方面又想使模型与训练数据很契合。</p><p><strong>从正则化角度来讲，Ω(f)称为正则化项，C称为惩罚参数，C越大即对误分类的惩罚越大(要求模型对训练模型更契合)，这可能会存在过拟合；C越小即相对更加看重正则化项，此时可能存在欠拟合。</strong></p><h5 id="非线性SVM-核技巧"><a href="#非线性SVM-核技巧" class="headerlink" title="非线性SVM-核技巧"></a>非线性SVM-核技巧</h5><p>前面介绍的都是线性问题，但是我们经常会遇到非线性的问题(例如异或问题)，此时就需要用到<strong>核技巧(kernel trick)将线性支持向量机推广到非线性支持向量机</strong>。需要注意的是，不仅仅是SVM，很多线性模型都可以用核技巧推广到非线性模型，例如核线性判别分析(KLDA)。</p><p>为了使不满足上述条件的样本点尽可能少，我们需要在优化的目标函数(2.2.2)里面新增一个对这些点的惩罚项。最常用的是hinge损失:</p><h6 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h6><p>核技巧的基本思路分为两步:</p><ul><li>使用一个变换将原空间的数据映射到新空间(例如更高维甚至无穷维的空间)；</li><li>然后在新空间里用线性方法从训练数据中学习得到模型。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230917/12.jpg" style="zoom:50%;" /></p><p>和函数的定义：</p><blockquote><p>设X是输入空间(欧式空间$R^n$的子集或离散集合)，又设H是特征空间(希尔伯特空间)，如果存在一个X到H的映射 ϕ(x):X→H 使得对所有 x,z∈X，函数 K(x,z) 满足条 件K(x,z)=ϕ(x)⋅ϕ(z) 则称 K(x,z) 为核函数，ϕ(x)为映射函数，式中 ϕ(x)⋅ϕ(z) 为 ϕ(x)和ϕ(z) 的內积。</p></blockquote><p>通常，直接计算 K(x,z) 比较容易而通过 ϕ(x)和ϕ(z) 计算 K(x,z) 并不容易。而幸运的是，在线性支持向量机的对偶问题中，无论是目标函数还是决策函数都只涉及到输入样本与样本之间的內积，因此我们不需要显式地定义映射 ϕ(x) 是什么，而只需事先定义核函数 K(x,z) 即可。也就是说，在核函数 K(x,z) 给定的情况下，可以利用解线性问题的方法求解非线性问题的支持向量机，此过程是隐式地在特征空间中进行的。</p><h6 id="正定核"><a href="#正定核" class="headerlink" title="正定核"></a>正定核</h6><p>由上面的介绍可知，我们只需要定义核函数就可以了。但是如何不通过映射 ϕ(x) 判断给定的一个函数 K(x,z) 是不是核函数呢？或者说，K(x,z) 需要满足什么条件才是一个核函数？</p><p>通常所说的核函数就是正定核函数。常用的核函数：</p><ul><li><p>多项式核函数：</p><script type="math/tex; mode=display">K(x, z) = (x \cdot z + 1)^p</script></li><li><p>高斯核函数：</p><script type="math/tex; mode=display">K(x, z) = exp(-\frac{||x-z||^2}{2 \sigma})</script></li></ul><h6 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h6><p>利用核技巧可以很简单地把线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机中的內积换成核函数即可。下面简述非线性支持向量机学习算法。</p><ul><li><p>首先选取适当的核函数 $K(x,z)$ 和适当的参数 $C$，构造最优化问题：</p><script type="math/tex; mode=display">\max _{\alpha}\min _{W，b} L(W, b, \alpha)  = \max _{\alpha} \sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{n} y_{i} y_{j} \alpha_{j} \alpha_{j} K(X_i,K_j)</script><script type="math/tex; mode=display">s.t. \quad \sum_{i=1}^{n} \alpha_{i} y_{i}=0,</script><script type="math/tex; mode=display">\quad 0 \leq \alpha_i \leq C, i=1,2,..,n</script></li></ul><ul><li>构造决策函数：<script type="math/tex; mode=display">f(X) = sign(\sum_{i=1}^{n} \hat{\alpha_{i}} y_{i} K(X_i,K_j) + \hat{b})</script></li></ul><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li><p>简单介绍SVM？</p><p>支持向量机SVM是一种二类分类模型。它的基本模型是定义在特征空间中的间隔最大的线性分类器。学习的目标就是在特征空间内找到一个分离超平面，能够将实例分到不同的类。</p><blockquote><p>从分类平面，到求两类间的最大间隔，到转化为求间隔分之一，等优化问题，然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题，最后再利用SMO（序列最小优化）来解决这个对偶问题。</p></blockquote><ul><li>当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</li><li>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</li><li>当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</li></ul></li></ul><ul><li><p>什么是支持向量？</p><p>在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。</p></li></ul><ul><li><p>SVM为什么采用间隔最大化？</p><p>当训练数据线性可分时，存在<strong>无穷个分离超平面可以将两类数据正确分开</strong>。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间<strong>隔最大化求得最优分离超平面，这时，解是唯一的</strong>。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系</p></li></ul><ul><li><p>为什么要将求解问题SVM的原始问题转换为其对偶问题？</p><ul><li><p><strong>对偶问题往往更易求解</strong>，当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。</p></li><li><p><strong>可以自然引入核函数，进而推广到非线性分类问题。</strong></p></li><li><strong>求解更高效，改变了问题的复杂度</strong>，由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0。</li></ul></li></ul><ul><li><p>为什么SVM要引入核函数？</p><p><strong>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分</strong>。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数 K 计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。</p></li></ul><ul><li><p>为什么SVM对数据确实敏感？</p><p>缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。</p></li></ul><p><strong>支持向量机的优点:</strong></p><ol><li>由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。</li><li>不仅适用于线性线性问题还适用于非线性问题(用核技巧)。</li><li>拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。</li><li>理论基础比较完善(例如神经网络就更像一个黑盒子)。</li><li>SVM无需依赖所有样本， 只依赖支持向量</li></ol><p><strong>支持向量机的缺点是:</strong></p><ol><li>二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)</li><li>只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)</li><li>SVM对缺失值敏感</li><li>如果特征维度远大于样本个数，SVM变现一般</li><li>SNM在样本巨大且使用核函数时计算量很大。</li></ol><h4 id="降维算法"><a href="#降维算法" class="headerlink" title="降维算法"></a>降维算法</h4><p><strong>降维就是用低纬度的向量来表示原始高纬度的特征。</strong></p><p><strong>降维的作用：增大样本密度，可以缓解维数灾难；减少计算开销；去噪</strong></p><h5 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h5><p>PCA算法，就是主成分分析法（Components Analysis，PCA），<strong>旨在找到数据中的主成分，并利用这些主成分来表征原始数据</strong>。简单的来说就是将n维的特征映射到k维上（k&lt;n）,这k维的正交特征，这就是主成分。</p><p><strong>是一个非监督的机器学习算法</strong>，是一种用于探索高维数据结构的技术，主要用于对数据的降维，通过降维可以发现更便于人理解的特征，加快对样本有价值信息的处理速度，此外还可以应用于可视化（降到二维）和去噪。</p><p><strong>PCA降维准则：</strong></p><ul><li><strong>最大可分性：样本点在这个超平面（直线的高维推广）的投影尽可能地分开（最大化方差）</strong></li><li><strong>最近重构性：样本点到这个超平面地距离都足够近（最小化平方误差）</strong></li></ul><h6 id="PCA之最大可分性（最大方差）"><a href="#PCA之最大可分性（最大方差）" class="headerlink" title="PCA之最大可分性（最大方差）"></a>PCA之最大可分性（最大方差）</h6><p>用方差来定义样本的间距，方差越大表示样本分布越稀疏，方差越小表示样本分布越密集。</p><p><strong>PCA的优化目标，就是最大化投影方差。换种说法就是，让数据在某个超平面（主轴）上投影的方差最大</strong>。</p><p><strong>最大化方差公式推导：</strong></p><ul><li><p>（1）给定一组样本点$\{v_{1},v_{2},…,v_{n}\}$，首先将其<font color='red'>中心化</font>后表示为$\{x_{1},x_{2},…,x_{n}\}=\{v_{1}-\mu,v_{2}-\mu,…,v_{n}-\mu\}$，其中，$\mu=\frac{1}{n}\sum_{i=1}^{n}v_{i}$。<strong>去均值后，样本x每个特征维度上的均值都是0。</strong></p></li><li><p>（2）因为一个向量$x_{i}$在$\omega$（单位方向向量）上的投影可以表示为两者的内积$<x_{i},\omega>=x_{i}^{T}\omega$，而<strong>PCA的目标就是找到一个投影方向$\omega$，使得所有的数据$\{x_{1},x_{2},…,x_{n}\}$在$\omega$上的投影方差尽可能地大</strong>，因此：</p></li><li><p>（3）投影后的方差可以表示为：</p><script type="math/tex; mode=display">D(x)=\frac{1}{n}\sum_{i=1}^{n}(x_{i}^{T}\omega)^2\\=\frac{1}{n}\sum_{i=1}^{n}(x_{i}^{T}\omega)^{T}(x_{i}^{T}\omega)\\=\frac{1}{n}\sum_{i=1}^{n}\omega^{T}x_{i}x_{i}^{T}\omega=\omega^{T}\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}\right)\omega</script></li></ul><ul><li><p>（4）然后可以发现，上面大括号内的$\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$就是<strong>原始样本的<font color='red'>协方差矩阵</font></strong>，令其等于$\Sigma$。</p><blockquote><p>补充理解：协方差公式形式：</p><script type="math/tex; mode=display">Cov(x,y)=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\mu_x)(y_{i}-\mu_{y})</script><p>在均值 $\mu=0$ 时，（当n足够大时，n-1可以约等于n），于是有：</p><script type="math/tex; mode=display">Cov(x,y)=\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}</script><p>对于步骤③中 $x_{i}$，是原始样本 $v_{i}$ 中心化后的，因此可以说 $\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$ 就是<strong>原始样本的协方差矩阵</strong>。</p></blockquote></li><li><p>（5）因此，<font color='orange'>上面的最大化方差D(x)的优化问题可以转化为</font></p></li></ul><script type="math/tex; mode=display">\begin{cases}max\{\omega^{T}\Sigma\omega\},\\s.t. \ \ \omega^{T}\omega=1.\end{cases}\\其中，\omega是单位向量，因此有\ \omega^{T}\omega=1.</script><ul><li><p>（6）对于上面的优化目标，可以<font color='orange'>构造拉格朗日函数</font>来解决：</p><script type="math/tex; mode=display">L(\omega)=\omega^{T}\Sigma\omega+\lambda(1-\omega^{T}\omega)\\对\ \omega\ 求导并令其等于0，可得\ \Sigma\omega=\lambda\omega</script><blockquote><p>补充一点矩阵微分的知识，有助于理解上式的求导过程：（非常有用的公式！！！可以记住！）</p><script type="math/tex; mode=display">①\ \frac{\partial x^{T}a}{\partial x}=\frac{\partial a^{T}x}{\partial x}=a\ \ \ \ \\②\ \frac{\partial x^{T}Ax}{\partial x}=(A+A^{T})x</script><p>因此，对拉格朗日函数的求导便很容易理解了：</p><script type="math/tex; mode=display">\frac{\partial L(\omega)}{\partial\omega}=(\Sigma+\Sigma^{T})\omega-\lambda(I+I^{T})\omega\\其中 I为单位矩阵</script><p>还记得$\Sigma$是什么吗？由上面定义可知，$\Sigma=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$，很显然，$\Sigma$的转置$\Sigma^{T}$=$\Sigma$，因此上面可化简为：</p><script type="math/tex; mode=display">\frac{\partial L(\omega)}{\partial\omega}=2\Sigma\omega-2\lambda\omega\\令其等于0，便可得\ \Sigma\omega=\lambda\omega</script></blockquote></li></ul><ul><li>（7）由此，最终可以得到最大方差：<script type="math/tex; mode=display">D(x)=\omega^{T}\Sigma\omega=\lambda\omega^{T}\omega=\lambda</script>至此，公式推导已经完成，现在我们不难看出，<strong>x 投影后的方差就是协方差矩阵的特征值</strong>，理解了这一点一切就很清晰了。<strong>因此，我们要找到最大的方差，也就是相当于要求协方差矩阵的最大特征值，而最佳投影方向就是最大特征值所对应的特征向量。</strong></li></ul><h6 id="PCA的求解过程总结"><a href="#PCA的求解过程总结" class="headerlink" title="PCA的求解过程总结"></a>PCA的求解过程总结</h6><ul><li><p>（1）对原始样本进行中心化处理，即零均值化，即每一位特征减去各自的平均值。</p></li><li><p>（2）求出样本的协方差矩阵 $\Sigma=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$</p></li><li><p>（3）求解协方差矩阵的特征值和特征向量</p></li><li><p>（4）将特征值由大到小排列，取出前 k 个特征值对应的特征向量</p></li><li><p>（5）将 n 维样本映射到 k 维，实现降维处理。</p><script type="math/tex; mode=display">x_{i}^{'}=\begin{bmatrix}\omega_{1}^{T}x_{i}\\\omega_{2}^{T}x_{i}\\\vdots \\\omega_{k}^{T}x_{i} \end{bmatrix}\\新的x_{i}^{'}的第k维就是x_{i}在第k个主成分\omega_{k}方向上的投影.</script></li></ul><h6 id="PCA之最近重构性（最小平方误差）"><a href="#PCA之最近重构性（最小平方误差）" class="headerlink" title="PCA之最近重构性（最小平方误差）"></a>PCA之最近重构性（最小平方误差）</h6><p>超平面是直线在高维空间的推广，因此，最大化方差就是寻找一个超平面使得样本点在超平面上的投影方差最大，而<strong>最小平方误差就是寻找一个超平面使得样本点到这个超平面的距离平方和最小，也就是最近重构性</strong>。</p><h6 id="PCA的优缺点和应用场景"><a href="#PCA的优缺点和应用场景" class="headerlink" title="PCA的优缺点和应用场景"></a>PCA的优缺点和应用场景</h6><p><strong>优点：</strong></p><ul><li>它是无监督学习算法，完全无参数限制。</li><li><p>降维，减小计算开销</p></li><li><p>使得数据集更易使用；</p></li><li><p>去除噪声</p></li><li>使得结果容易理解</li></ul><p><strong>缺点：</strong></p><ul><li>如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高；</li><li>特征值分解有一些局限性，比如变换的矩阵必须是方阵</li></ul><p><strong>应用场景：</strong></p><ul><li>高维数据集的探索与可视化。</li><li>数据压缩。</li><li>数据预处理。</li><li>图象、语音、通信的分析处理。</li><li>降维(最主要)，去除数据冗余与噪声。</li></ul><h5 id="LDA算法"><a href="#LDA算法" class="headerlink" title="LDA算法"></a>LDA算法</h5><p>LDA算法，就是线性判别分析（Linear Discriminant Analysis，LDA）。</p><p><strong>LDA是一种线性的、有监督的降维方法，即每个样本都有对应的类别标签（这点和PCA不同）。</strong></p><p><strong>主要思想：</strong>给定训练样本集，设法将样本投影到一条直线上，使得同类的样本的投影尽可能的接近、异类样本的投影尽可能地远离（即<strong>最小化类内距离和最大化类间距离</strong>）。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20231009/17.png" style="zoom:67%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20231009/18.png" style="zoom:67%;" /></p><p>● 为什么要将最大化类间距离和最小化类内距离同时作为优化目标呢？</p><p>先看上面第二张图的左图（a），对于两个类别，只采用了最大化类间距离，其结果中两类样本会有少许重叠；而对于右图（b），同时最大化类间距离和最小化类内距离，可见分类效果更好，同类样本的投影分布更加集中了。当然，对于二维的数据，可以采用将样本投影到直线上的方式，对于高维的数据，则是投影到一个低维的超平面上，这应该很好理解。</p><p><strong>优化目标：LDA算法的思想就是最大化类间距离和最小化类内距离</strong>，其优化目标就很直观了，那怎么用数学方式来表示呢？要解决这个问题，就得先看看怎么描述类间距离和类内距离？推理略</p><h6 id="LDA算法求解流程"><a href="#LDA算法求解流程" class="headerlink" title="LDA算法求解流程"></a>LDA算法求解流程</h6><ul><li><p>（1）计算每类样本的均值向量 $\mu_{i}$。</p></li><li><p>（2）计算类间散度矩阵  $S_{\omega}$ 和类内散度矩阵 $S_{b}$ 。</p></li><li><p>（3）求矩阵 $S_{\omega}^{-1}S_{b}$ 的特征值即对应的特征向量，从大到小排序。</p></li><li><p>（4）将特征值由大到小排列，取出前 k 个特征值对应的特征向量。</p></li><li><p>（5）将 n 维样本映射到 k 维，实现降维处理。</p><script type="math/tex; mode=display">x_{i}^{'}=\begin{bmatrix}\omega_{1}^{T}x_{i}\\\omega_{2}^{T}x_{i}\\\vdots \\\omega_{k}^{T}x_{i} \end{bmatrix}\\</script></li></ul><h6 id="LDA算法总结"><a href="#LDA算法总结" class="headerlink" title="LDA算法总结"></a>LDA算法总结</h6><ul><li>LDA是线性的、有监督的降维方法，其优点是善于对有类别信息的数据进行降维处理（与PCA的不同）。</li><li>LDA因为是线性模型，对噪声的鲁棒性较好，但由于模型简单，对数据特征的表达能力不足。</li><li>LDA对数据的分布做了一些很强的假设，比如每个类别都是高斯分布、各个类别的协方差相等，实际中这些假设很难完全满足。</li></ul><h5 id="PCA和LDA的区别"><a href="#PCA和LDA的区别" class="headerlink" title="PCA和LDA的区别"></a>PCA和LDA的区别</h5><p><strong>异同点</strong></p><p>相同点：</p><ul><li>① 均是降维方法</li><li>② 降维时均使用了矩阵特征分解的思想</li><li>③ 两者都假设数据符合高斯分布</li></ul><p><strong>不同点：</strong></p><ul><li>① PCA是无监督的降维方法，而LDA是有监督的降维方法</li><li>② LDA除了可以降维，还可以用于分类</li><li>③ LDA降维最多降到类别数 <code>k-1</code>的维数（k是样本类别的个数），而PCA没有这个限制。</li><li>④ LDA选择的是分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向</li></ul><blockquote><p>● 关于第4点，可以这样理解：</p><p>LDA在降维过程中最小化类内距离，即同类样本的方差尽可能小，同时最大化类间距离，即异类样本尽可能分离，这本身是也为分类任务服务的；而PCA是无监督的降维方法，其假设方差越大，信息量越多，因此会选择样本点投影具有最大方差的方向。</p></blockquote><p><strong>优缺点</strong></p><p>LDA优点：</p><ul><li>① 降维过程中可以使用类别的先验知识（有监督的），而PCA是无监督的</li><li>② 在样本分类信息依赖均值而不是方差的时候，LDA算法优于PCA算法</li></ul><p>LDA缺点：</p><ul><li>① LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。</li><li>② LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。</li><li>③ LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。</li><li>④ LDA可能过度拟合数据。</li></ul><p>PCA优点：</p><ul><li>①它是无监督学习算法，完全无参数限制。</li><li>② 在样本分类信息依赖方差而不是均值的时候，PCA算法优于LDA算法</li></ul><p>PCA缺点：</p><ul><li>①特征值分解有一些局限性，比如变换的矩阵必须是方阵</li><li>②如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对比学习相关总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h4 id="Contrastive-Learning"><a href="#Contrastive-Learning" class="headerlink" title="Contrastive Learning"></a>Contrastive Learning</h4><h5 id="对比学习的思想"><a href="#对比学习的思想" class="headerlink" title="对比学习的思想"></a>对比学习的思想</h5><ul><li><p>对比学习的思想就是去<strong>拉近相似的样本，推开不相似的样本</strong>（在特征空间中拉近正样本之间的距离，推开负样本的距离）。而目标是要从<strong>样本中学习到一个较好的语义表示空间</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230810/1.jpg" style="zoom:80%;" /></p><p>对比学习应该具备两个属性：Alignment 和 Uniformity</p><p><strong>所谓“Alignment”</strong>，指的是相似的例子，也就是正例，映射到单位超球面后应该有接近的特征，也即是说，在超球面上距离比较近；</p><p><strong>所谓“Uniformity”，</strong>指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分</p><p><strong>对比学习在loss设计时</strong>，为单正例多负例的形式，因为是无监督，数据是充足的，也就可以找到无穷的负例，但如何构造有效正例才是重点</p></li></ul><h5 id="对比学习的范式"><a href="#对比学习的范式" class="headerlink" title="对比学习的范式"></a>对比学习的范式</h5><ul><li><p>对比学习的典型范式就是：<strong>代理任务+目标函数！</strong>代理任务和目标函数也是对比学习与有监督学习最大的区别。</p></li><li><p>对比学习之所以被认为是一种无监督的训练方式，是因为人们可以使用<strong>代理任务（pretext task）</strong>来定义谁与谁相似，谁与谁不相似，代理任务通常是人为设定的一些规则，这些规则定义了哪张图与哪张图相似，哪张图与哪张图不相似，从而提供了一个监督信号去训练模型，这就是所谓的自监督。对比学习可以叫自监督也可以叫无监督。<strong>数据增强是代理任务的实现常见手段</strong>。代理任务就是来解决无监督学习中没有ground-truth问题，我们用代理任务来定义对比学习的正负样本，无监督学习一旦有了输出y和真实的label，就需要有一个目标函数来计算两者的损失从而指导模型的学习方向。</p></li><li><p>以SimCLR提出的对比学习框架说明：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230810/2.jpg" style="zoom:50%;" /></p><ul><li><p><strong>代理任务阶段</strong>：对于同一个样本 $x$ ,经过两个代理任务分别生成 $\hat{x}_i$ 和 $\hat{x}_j$ 两个样本，SimCLR属于计算机视觉领域的Paper，文中使用数据增强手段作为代理任务，例如图片的随机裁剪，随机颜色失真，随机高斯模糊， $\hat{x}_i$ 和 $\hat{x}_j$ 就称为一个正样本对。</p></li><li><p><strong>特征提取器</strong>：$f\left( \cdot \right)$ 就是一个编码器，用什么编码器不做限制，SimCLR中使用的时ResNet， $\hat{x}_i$ 和 $\hat{x}_j$ 通过$f\left( \cdot \right)$ 得到 $h_i$ 和 $h_j$。</p></li><li><p><strong>MLP层(Projector层)</strong>：通过特征提取之后，再进入MLP层，SimCLR中强调了这个MLP层加上会比不加好，MLP层的输出就是对比学习的目标函数作用的地方，通过MLP层输出 $z_i$ 和 $z_j$。</p></li><li><p><strong>目标函数作用阶段</strong>：对比学习中的损失函数一般是 infoNCE loss， $z_i$ 和 $z_j$ 的损失函数定义如下：</p><script type="math/tex; mode=display">\mathcal{L} _{i, j}=-\log \frac{\exp \left(\operatorname{sim}\left(z_{i}, z_{j}\right) / \tau\right)}{\sum_{k=1}^{2 N} 1_{[k \neq i]} \exp \left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)}</script><p>其中，N 代表的是一个batch的样本数，即对于一个batch的N个样本，通过数据增强的得到 N 对正样本对，此时共有 2N 个样本，负样本是什么？SimCLR中的做法就是，对于一个给定的正样本对，剩下的2(N-1)个样本都是负样本，也就是负样本都基于这个batch的数据生成。$sim(z_i,z_j)$ 就是cosin相似度的计算公式。</p><p>从上式可以看出，分子中只计算正样本对的距离，负样本只会在对比损失的分母中出现，当正样本对距离越小，负样本对距离越大，损失越小。</p></li></ul></li></ul><h5 id="温度系数的作用"><a href="#温度系数的作用" class="headerlink" title="温度系数的作用"></a>温度系数的作用</h5><ul><li><p>温度系数虽然只是一个超参数，但它的设置是非常讲究的，直接影响了模型的效果。上式Info NCE loss中 $z_i,z_j$ 的相当于是logits，温度系数可以用来控制logits的分布形状。对于既定的logits分布的形状，当 $\tau$ 值变大，$1  / \tau$  则就变小，则会使得原来logits分布里的数值都变小，且经过指数运算之后，就变得更小了，导致原来的logits分布变得更平滑。相反，如果  $\tau$  取得值小，$1  / \tau$ 就变大，原来的logits分布里的数值就相应的变大，经过指数运算之后，就变得更大，使得这个分布变得更集中，更peak。</p></li><li><p><strong>如果温度系数设的越大，logits分布变得越平滑，那么对比损失会对所有的负样本一视同仁，导致模型学习没有轻重。如果温度系数设的过小，则模型会越关注特别困难的负样本，但其实那些负样本很可能是潜在的正样本，这样会导致模型很难收敛或者泛化能力差。</strong></p></li><li><p><strong>温度系数的作用是调节对困难样本的关注程度</strong>：<strong>越小的温度系数越关注于将本样本和最相似的困难样本分开</strong>，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小</p></li><li><p>考虑两个极端情况，温度系数趋向于0时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。</p></li></ul><h5 id="CV中的发展"><a href="#CV中的发展" class="headerlink" title="CV中的发展"></a>CV中的发展</h5><ul><li><p><strong>MoCo-v1</strong>：Momentum Contrast for Unsupervised Visual Representation Learning</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230810/3.jpg" style="zoom:80%;" /></p><p>MoCo v1的主要贡献就是把之前的对比学习的一些方法归纳总结成<strong>字典查询问题</strong>，它提出了两个东西：<strong>一个是队列，一个是动量编码器</strong>，从而形成了一个又大又一致的字典帮助对比学习。MoCo v1就是用这个队列去取代了原来的memory bank作为一个额外的数据结构去存这个负样本。然后用动量编码器去取代原来loss的约束项从而达到动量更新编码器的目的，而不是动量的去更新那个特征，从而得到更好的结果。</p><ul><li><p>1）它的改进真的是简单有效，而且有很大的影响力，如动量编码器，像SimCLR、BYOL，一直到最新的对比学习的工作都还在使用；</p></li><li><p>2）它的写作真的是高人一等，如把所有的对比学习方法统一为一个字典查询的框架中。</p></li></ul></li></ul><ul><li><p><strong>SimCLR</strong>：A Simple Framework for Contrastive Learning of Visual Representations</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230810/2.jpg" style="zoom:50%;" /></p><p>SimCLR的一个重大创新点就是在特征提取器提取特征后之后，又加了一个叫projector的东西，即g函数，就是一个全连接层跟了一个relu函数。就是这么一个简单的mlp，可以让最后学到的特征在ImageNet上提点将近10个点。我们可以想象有了一个特征h之后，再做一个非线性的变化，就得到另外一个特征z，一般这个z会维度小一点，最后就衡量一下正样本之间是不是能达到最大的一致性。它们采用的是一个叫normalized temperature-scaled的交叉熵函数，normalized指的是在特征后面进行了L2归一化，temperature-scaled就是说在这个loss上乘了一个τ，所以这个loss和之前说的InfoNCE也是非常接近的。</p><p>另外这里的projection head也就是g函数只有在训练的时候才用，而当你在做下游任务的时候我们把这个g函数扔掉了，还是只用h去做特征，这样和别的任务就能公平对比了。</p><ul><li><p>1）用了更多的数据增广技术，对对比学习十分有益；</p></li><li><p>2）加了g函数这个可以学习非线性变化的层；</p></li><li><p>3）用了更大的batch size来让字典更大，而且训练的更久；</p></li></ul></li></ul><ul><li><p><strong>MoCo-v2</strong>：Improved Baselines With Momentum Contrastive Learning</p><p>MoCo团队看到SimCLR比较好的结果后，发现SimCLR里面的技术都是即插即用型，所以MoCo-v2就都拿过来了。就在MoCo上做很简单的改动，把mlp projectioin head以及更多的数据增强的trick也加进来。</p><ul><li>MoCo相比于SimCLR的这个优越性：就是MoCo可以在较小的batch-size中训练(因为有memory-bank)，资源消耗和时间比SimCLR少。</li></ul></li></ul><ul><li><p><strong>SimCLR-v2</strong>：Big Self-Supervised Models are Strong Semi-Supervised Learners</p><p>第一个部分就是SimCLR v2，即怎么样通过自监督的对比学习去训练一个大的模型。第二部分就是一旦有了大模型，我们只需要一小部分的这个有标签的数据做一下有监督的微调。一旦微调结束，就等于有了一个teacher模型，就可以去生成很多伪标签，这样就可以在更多无标签的数据上去做这种自学习。</p><ul><li><p>1）无监督学习在模型越大的时候表现会越好，换了一个更大的模型，换了152层的ResNet和selective kernels，来让骨干网络变强</p></li><li><p>2）MLP层变深会不会更有用，于是经过实验发现2层的projector层最好。</p></li><li><p>使用动量编码器</p></li></ul></li></ul><ul><li><p><strong>SwAC</strong>：Unsupervised Learning of Visual Features by Contrasting Cluster Assignment</p><p>SwAC的思想就是给定一张图片，如果我去生成不同的视角，我希望可以用一个视角的特征去预测另外一个视角得到的特征，因为所有视角的特征按道理来都应该是非常接近的。这篇文章的做法就是把对比学习和之前聚类的方法合在了一起，聚类的方法本身也是一种无监督的特征表示学习，而且他也是希望相似的物体都聚集在某一个聚类中心附近，而不相似的物体尽量推开推到别的聚类中心，所以跟对比学习做法都比较接近。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230810/4.png" style="zoom:67%;" /></p><p>左边：之前对比学习的方法总结，不同的样本对通过编码器提取特征，然后用对比学习学习。</p><p>SwAC画到了右边：SwAC认为你直接拿所有图片的这个特征和特征做对比有点原始且有点费资源，因为所有的图片都是自己的类，所以像MoCo有6万个负样本，这还只是个近似（一共有128万个负样本），那我们能不能不去做近似呢？我们能不能借助一些先验信息，不去跟大量的负样本比，而去跟一些更简洁的东西比呢？<strong>因此SwAC作者就想出来了，可以和聚类的中心比</strong>。聚类中心就是右图画的c，<strong>即prototypes，它其实就是一个矩阵，维度是D×K，D是提取出来特征的维度，K是聚类中心的个数，这里取的是3000</strong>。</p><p><strong>SwAC的前向过程</strong>：原始数据，先做数据增强，然后分别过编码器提取出来特征 z1 z2，有了特征之后不是直接在特征上去做对比学习loss，而是说你先通过一个clustering聚类的方法，让你的特征z1 z2和prototypes C去生成一个目标，即q1 q2，q1 q2相当于是一个groundtruth一样的东西，而他真正要做的代理任务是什么呢？如果x1 x2是正样本，那么z1 z2应该很相似，那么按道理来说应该是可以互相去做预测的，即现在用z1和c做点乘，按道理来说是可以去预测q2的，反之亦然。那么点乘之后的结果就是我们的预测，而groundtruth就是之前按照聚类分类得到的这个q1和q2，所以通过这种swapped prediction也就是换位预测的方式，SwAC可以对模型进行训练。</p><ul><li><p>1）跟很多的负样本去做类别，你需要成千上万个负样本，而且即使如此你也只是一个近似。而如果只是跟聚类中心去做对比，可以用几百或最多3000个聚类中心就足以表示。</p></li><li><p>2）聚类的中心是有明确的语义含义的，而之前只是随机抽样负样本去做对比，那些负样本有的可能还是正样本，而且有时候抽出来的负样本这个类别也不均衡，所以不如聚类中心有效。</p></li></ul></li></ul><ul><li><p><strong>BYOL</strong>：Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</p><ul><li>为什么不用负样本在对比学习中这么稀奇呢？是因为在对比学习中，<strong>负样本是一个约束，如果你在算目标函数的时候你只有正样本，那其实你的目的就只有一个，那就是让所有相似的物体它们的特征也尽可能相似，那这个时候就有一个很明显的捷径解</strong>：即一个模型不论你给它什么输入，它都给你返回同样的输出，那这样他出来的所有特征都是一模一样的，那你拿这个去算对比学习的loss就都是0。而只有加上负样本这个约束，模型才有动力去继续学，因为他输出的都一样的话，那在负样本这边loss就无穷大，所以负样本在对比学习里面是一个必须的东西，防止模型学到一个捷径解，很多论文里把这个叫做model collapse，就是啥都没学到。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230810/5.png" style="zoom:67%;" /></p><p>BYOL的模型总览图如上所示,首先有一个mini-batch的输入x，经过两次数据增强得到了 v 和 v’，然后通过编码器得到特征，两个编码器使用一样的结构，但不共享参数，下面的编码器和MoCo一样是通过动量更新的方法更新的。接下来和SimCLR一样，用了一个projection head得到进一步的特征 z。之前的对比学习就是让这两个 z 尽可能接近，但是BYOL没有这么做，又加了一层叫predictor的东西，这个 qθ 和 gθ 网络结构一样，也是一个mlp，这样就得到一个新的特征，即 qθ(zθ)，他们想要他们这个预测和下面这个 zξ′尽可能一致，就是把原来一个匹配的问题换成现在一个预测的问题。</p><p>代理任务改成了一个视角去预测另一个视角的代理任务。训练网络用的目标函数也很有意思，直接用的mse loss，因为现在就是两个向量，现在想让他们尽可能接近，所以算一个mse loss就可以了。</p><p><strong>模型坍塌问题：</strong>带BN的BYOL，结果是正确的；没有用BN的话，结果就和随机的差不多。接下来作者发现只要在某一块放了BN，结果就不会太差，因此觉得肯定是BN惹的祸。而且思考出了一个结论：BN是把一个batch的所有数据拿过来算一下均值和方差，然后用这个均值和方差做归一化，那这也就意味着你在算某个正样本的loss的时候，你也看到了其他样本的这个特征，也就是说这里是有信息泄露的，<strong>因为有这个信息泄露的存在</strong>，所以可以把这个batch里面的其他样本，想象成一个隐式的这个负样本；换句话说，有了BN的时候，BYOL其实不光是正样本在自己跟自己学，他其实也在跟自己做对比，它的对比任务是当前的正样本图片，跟你这个平均图片有什么差别，而这个平均图片，就是batch norm产生的，还有很多图片的总结量。这个就跟SwAV很像了，因为SwAV就是没有跟负样本去比，而是找了一个聚类中心去比，而这里batch norm生成的这种平均图片相当于聚类中心。</p></li><li><p><strong>SimSiam</strong>：Exploring Simple Siamese Representation Learning</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230810/6.png" style="zoom:80%;" /></p><p>BYOL出来之后，就有很多工作在分析对比学习的成功，因为很多对比学习的工作，好像都是被很多改进堆起来的这个性能，比如projection head、动量编码器、更大的batch size等等。</p><p>网络结构如上所示，首先两个编码器的网络结构是一样的，而且是要共享参数的，然后有一个predictor，然后预测另一个特征，和BYOL不一样的就是不需要动量更新。目标函数用的是mse loss，然后预测可以是双向的，即可以用x1出去的特征过predictor预测x2出去的特征也可以反过来。</p><p>SimSiam之所以不会模型坍塌，主要就是因为有stop gradient这个操作的存在。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 对比学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vision_Transformer相关总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Vision-Transformer%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Vision-Transformer%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h4 id="Transformer的小总结"><a href="#Transformer的小总结" class="headerlink" title="Transformer的小总结"></a>Transformer的小总结</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230809/1.jpg" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230809/7.jpg" style="zoom:67%;" /></p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p><strong>计算过程：</strong>$Q,K,V \in R^{B \times H \times W \times C}$ ==&gt; 多头注意力$Q,K,V \in R^{B \times \times H \times W \times numHead \times C/numHead} ==&gt;Q,k,V \in R^{B \times numHead \times HW \times C/numHead }$ ==&gt; 注意力矩阵 $S = QK^T \in R^{B \times numHead \times HW \times HW}$==&gt;提取value $SV \in R^{B \times numHead \times HW \times C/numhead} ==&gt; R^{B \times HW \times C}$</p><h5 id="Transformer简要介绍"><a href="#Transformer简要介绍" class="headerlink" title="Transformer简要介绍"></a>Transformer简要介绍</h5><ul><li>Transfomer 是一种基于注意力机制的神经网络模型。Transformer模型由编码器和解码器两部分组成，其中编码器用于将输入序列编码成一个高维向量表示，解码器用于将这个向量表示解码成目标序列。Transformer模型最核心的部分是自注意力机制，它能够让模型在不同位置之间进行信息传递和交互，从而更好地学习输入序列中的信息。</li></ul><h5 id="Transformer的输入是什么"><a href="#Transformer的输入是什么" class="headerlink" title="Transformer的输入是什么"></a>Transformer的输入是什么</h5><ul><li><p>Trransformer的输入是词向量与位置向量之和，<code>词向量</code>可以通过预训练的词向量模型或在模型内部学习得到。<code>位置向量</code>可以通过固定位置编码公式获得或者在模型内容不学习得到。</p></li><li><p><strong>Vision Transformer：首先将图像进行patch化策略，把每一个patch视为向量，所有向量并在一起就形成一系列的patch序列，然后再加上位置编码作为Transformer的输入。</strong></p><p><code>self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) patch_size:也就是下采样的倍数，一般为4、16</code> </p></li></ul><h5 id="固定编码和可学习编码的优缺点"><a href="#固定编码和可学习编码的优缺点" class="headerlink" title="固定编码和可学习编码的优缺点"></a>固定编码和可学习编码的优缺点</h5><ul><li>固定位置编码优点是可以根据公式快速获得句子的位置信息，无需在训练中继续学习；其缺点是不能处理变化的序列（例如：我是大帅哥，大帅哥是我）。</li><li>可学习位置编码优点是可以通过训练时动态理解句子的位置信息；缺点是需要大量的数据才能获取比较全的位置信息。</li></ul><h5 id="Transformer的Encoder模块"><a href="#Transformer的Encoder模块" class="headerlink" title="Transformer的Encoder模块"></a>Transformer的Encoder模块</h5><ul><li>Transformer的Encoder模块是由多个相同的层堆叠而成的，每一层由两个子层组成，分别是多头注意力机制（Multi-Head Attention）和前馈神经网络（Feed-Forward Neural Network）。在多头注意力机制中，输入序列会经过三个线性变换，分别是Q、K、V，然后进行多头注意力计算，得到每个位置对其他位置的注意力权重，再将输入序列加权求和得到多头注意力的输出。在前馈神经网络中，多头注意力的输出经过两个全连接层和ReLU激活函数的变换，得到每个位置的特征表示。接下来，这两个子层会进行残差连接（Residual Connection）和层归一化（Layer Normalization）操作，使得模型更容易训练，也能更好地捕捉输入序列之间的相关性。</li><li><strong>残差结构以及意义</strong>：encoder和decoder的self-attention层和ffn层都有残差连接。反向传播的时候不会造成梯度消失。</li></ul><h5 id="为什么transformer块使用LayerNorm而不是BatchNorm？"><a href="#为什么transformer块使用LayerNorm而不是BatchNorm？" class="headerlink" title="为什么transformer块使用LayerNorm而不是BatchNorm？"></a>为什么transformer块使用LayerNorm而不是BatchNorm？</h5><ul><li>Layer Normalization是一种能够应对不同序列长度的归一化方法，它对每个样本的特征进行归一化。Batch Normalization是一种在深度神经网络中广泛使用的归一化方法，通过对每个小批量的输入进行归一化，从而使得网络的训练更加稳定，并加速收敛速度。但是，在自然语言处理任务中，输入的序列长度通常是不同的，因此很难将不同长度的序列组成一个小批量进行归一化。</li><li>Batchnorm为批归一化，对相同批次中所有样本的同一位置特征做归一化，而layernorm是对某个样本的所有位置特征做归一化。BN归一化会抹去同一样本所有位置特征的原有大小关系，LN会保留这种大小关系。</li></ul><h5 id="Transformer输入只能相加吗？"><a href="#Transformer输入只能相加吗？" class="headerlink" title="Transformer输入只能相加吗？"></a>Transformer输入只能相加吗？</h5><ul><li>在Transformer模型中，输入的两个部分是词向量和位置编码，它们是分别生成的，然后进行相加得到最终的输入表示。因此，在Transformer模型中，输入确实只能相加，即词向量和位置编码不能进行其他的运算，如乘法、除法等。这是因为词向量和位置编码的维度是相同的，都是模型的隐藏层维度，而它们的作用是不同的，词向量用于表示单词的语义信息，位置编码用于表示单词在句子中的位置信息。因此，将它们相加可以将这两种信息融合到一起，从而为模型提供更加丰富的输入信息。如果进行其他的运算，如乘法、除法等，可能会破坏词向量和位置编码的信息，影响模型的性能。因此，在Transformer模型中，输入只能相加，不能进行其他的运算。</li></ul><h5 id="Transformer为何使用多头注意力机制？"><a href="#Transformer为何使用多头注意力机制？" class="headerlink" title="Transformer为何使用多头注意力机制？"></a>Transformer为何使用多头注意力机制？</h5><ul><li><strong>提高模型的表达能力</strong>，多头注意力机制可以让模型在不同的注意力空间下学习到不同的特征，从而能够更好地表达输入序列的信息。如果只使用一个注意力头，那么模型可能会在学习特定的特征时出现瓶颈，导致模型的表达能力受限。</li><li>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</li><li>举例说明不一样的特征表达字空间。一张图中在颜色方面更加关注鲜艳亮丽的文字，而在字体方面会去注意大的、粗体的文字。</li></ul><h5 id="Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"><a href="#Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？" class="headerlink" title="Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"></a>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</h5><ul><li>使用 Q；K；V 不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</li><li>在Transformer中，Q和K使用不同的权重矩阵生成是为了让模型在学习不同的特征时更加灵活。Q和K的区别在于它们所代表的信息不同，Q代表查询信息，K代表键信息，它们的作用不同，因此使用不同的权重矩阵可以让模型在不同的注意力空间下学习到更加丰富的特征，并提高模型的表现能力。</li><li>如果使用同一个权重矩阵进行自身的点乘操作，可能会使模型在学习特定的特征时出现瓶颈，导致模型表达能力受限，从而影响模型的性能。</li><li>为了打破对称性，但是如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。</li></ul><h5 id="Transformer计算attention的时候为何选择点乘而不是加法？"><a href="#Transformer计算attention的时候为何选择点乘而不是加法？" class="headerlink" title="Transformer计算attention的时候为何选择点乘而不是加法？"></a>Transformer计算attention的时候为何选择点乘而不是加法？</h5><ul><li><p>K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</p></li><li><p>在Transformer中，采用点乘（dot-product）作为注意力机制的计算方式，是因为点乘计算的方式是一种更加有效的方法，可以更好地捕捉输入序列中的相关性。与加法相比，点乘可以使模型在计算注意力时更加精确，同时也具有更好的计算效率。</p></li></ul><h5 id="在计算attention时，为什么进行softmax之前需进行scaled（为什么除以dk的平方根）"><a href="#在计算attention时，为什么进行softmax之前需进行scaled（为什么除以dk的平方根）" class="headerlink" title="在计算attention时，为什么进行softmax之前需进行scaled（为什么除以dk的平方根）?"></a>在计算attention时，为什么进行softmax之前需进行scaled（为什么除以dk的平方根）?</h5><ul><li>在计算self-attention时，需要进行softmax操作，以计算每个输入序列位置对其他位置的注意力权重。<strong>为了避免softmax函数的指数计算导致数值溢出或下溢</strong>，Transformer模型中使用了scaled dot-product attention，即在softmax之前对向量点乘结果进行了缩放操作，用于控制点乘结果的大小。<br>具体来说，该缩放操作是将点乘结果除以一个值，这个值是输入向量的维度的平方根，即dk的平方根，其中dk表示每个向量的维度。这个缩放因子的作用是：当输入向量的维度增加时，点乘结果的大小也会增加，导致softmax函数的指数计算变得困难，缩放因子能够使点乘结果的大小保持在一个合适的范围内，从而提高计算的稳定性。</li></ul><h5 id="大概讲一下Transformer的Decoder模块？"><a href="#大概讲一下Transformer的Decoder模块？" class="headerlink" title="大概讲一下Transformer的Decoder模块？"></a>大概讲一下Transformer的Decoder模块？</h5><ul><li>Transformer模型的Decoder模块是用于将Encoder模块的输出映射到目标序列的一组连续表示的核心部分。该模块由多个Decoder层组成，每个Decoder层包括了以下几个部分：<ol><li><strong><code>自注意力层</code></strong>：与Encoder中的自注意力层类似，Decoder中的自注意力层也是将输入序列中每个位置的表示向量作为查询向量(Q)、键向量(K)和值向量(V)，通过多头注意力计算得到每个位置的上下文向量。但是多了一个Mask的过程，让输入序列只看到过去的信息，不能让他看到未来的信息。具体来说，在attention score矩阵中添加mask矩阵来屏蔽不可见的信息，对padding做mask操作就是在padding位置设为负无穷(一般来说-1000就可以)，再对attention score进行相加即可。</li><li><strong><code>编码器-解码器注意力层(cross-attention)</code></strong>：该层用于将Encoder模块的输出与Decoder中上一层的输出结合起来，以便更好地理解输入和输出之间的关系。具体来说，该层将Encoder模块的输出作为键向量(K) 和值向量(V) ，将Decoder中上一层的输出作为查询向量(Q)，通过多头注意力机制计算得到每个位置的上下文向量。</li><li><strong><code>前馈神经网络层</code></strong>：该层对经过自注意力层和编码器-解码器注意力层编码的信息进行非线性变换，以提高模型的表达能力。</li><li><strong><code>残差连接层和层归一化层</code></strong>：这两个层与Encoder模块中的残差连接层和层归一化层类似，用于保证模型的稳定性和加速训练。在每个Decoder层之间，都进行了层归一化处理。<br>Decoder模块的最后一层输出的表示向量经过一个线性变换和softmax函数，得到每个位置上每个单词的概率分布。然后可以根据分布进行单词的选择和预测。</li></ol></li></ul><h5 id="Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？"><a href="#Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？" class="headerlink" title="Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？"></a>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</h5><ul><li><p>在Transformer模型中，Encoder和Decoder都包含多头自注意力机制。虽然它们的原理类似，但是在具体实现中，它们之间存在一些区别。下面分别介绍Decoder阶段的多头自注意力和Encoder的多头自注意力的区别：</p><p><strong><code>1.查询向量不同</code></strong>：在Encoder的多头自注意力中，每个词向量都被用作查询、键和值，即Q=K=V，而在Decoder的多头自注意力中，查询向量(Q) 是上一个Decoder层的输出，而键(K) 和值向量(V) 是Encoder模型的输出。</p><p><strong><code>2.掩码</code></strong>：在Decoder的多头自注意力中，需要使用掩码来防止当前时间步的解码器看到未来时间步的信息。具体来说，将未来时间步的位置的注意力权重设置为0，这样在计算当前时间步的注意力分数时，就不会考虑未来时间步的信息。</p><p><strong><code>3.添加编码</code></strong>：在Decoder的多头自注意力中，需要将编码器的输出添加到查询向量和键向量中，以便解码器能够了解输入序列的信息。</p><p><strong><code>4.位置编码</code></strong>：在Decoder的多头自注意力中，位置编码的计算方式与Encoder中的位置编码不同。Encoder中的位置编码是为了表示输入序列中单词的位置关系，而Decoder中的位置编码是为了表示输出序列中单词的位置关系。</p></li></ul><h5 id="Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？"><a href="#Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？" class="headerlink" title="Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？"></a>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</h5><ul><li><p><strong><code>1. 多头注意力机制</code></strong>：多头注意力机制将输入序列分成多个子序列，并同时计算每个子序列的注意力表示，从而实现了多头并行计算。这种并行计算方式可以有效地加速模型的训练和推理过程。</p><p><strong><code>2. Encoder端的并行化</code></strong>：在Encoder端，Transformer模型将输入序列分成多个子序列，并分别在不同的计算设备上进行计算。这种并行计算方式可以显著提高模型训练的速度。而在Decoder端，由于每个时间步的计算需要依赖上一个时间步的输出，因此无法进行完全的并行化。但是，可以通过一定的技巧来提高Decoder的并行化效率，例如：</p><p><strong><code>3. 延迟解码</code></strong>：在训练时，可以将目标序列分成多个子序列，并在不同的计算设备上同时进行解码。但是，在推理时，由于无法知道整个目标序列，因此需要使用延迟解码的方式，即在每个时间步上进行解码，并将上一个时间步的输出作为当前时间步的输入。</p><p><strong><code>4. Beam Search并行化</code></strong>：在推理时，可以使用Beam Search算法来生成目标序列，并通过将不同的Beam分配到不同的计算设备上来实现推理的并行化。<br>因此，虽然Decoder端无法像Encoder端那样进行完全的并行化，但是可以通过一定的技巧来提高其并行化效率。</p></li></ul><h5 id="简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？"><a href="#简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？" class="headerlink" title="简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？"></a>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</h5><ul><li>Transformer模型中的前馈神经网络（Feed-Forward Neural Network，简称FFN）是在每个Encoder和Decoder层的自注意力层和编码器-解码器注意力层之间添加的一层全连接的前馈神经网络。它的输入是自注意力层或编码器-解码器注意力层的输出，输出是一个新的表示向量，其中包含了更高层次的语义信息。<br>在Transformer模型中，FFN使用了两层全连接的结构，两层之间使用了ReLU激活函数。具体来说，在每个FFN层中，输入的表示向量首先通过一个全连接层进行线性变换，然后再通过一个ReLU激活函数进行非线性变换，最后再通过另一个全连接层进行线性变换得到输出。<br>FFN的优点是可以通过多层的非线性变换，提取输入的更高层次的语义特征，从而提高模型的表达能力。另外，由于FFN的计算是独立进行的，因此可以通过并行化来加速模型的训练和推理过程。<br>但是，FFN也存在一些缺点。首先，由于FFN只考虑了每个位置的局部信息，因此无法处理序列中的长距离依赖关系。其次，由于FFN的计算复杂度较高，因此容易成为模型的瓶颈。最后，由于FFN没有考虑序列中的位置信息，因此可能会存在位置信息的混淆问题。<br>为了解决这些问题，一些变种的Transformer模型，如XLNet和Relative Positional Encoding等，引入了新的机制，以提高模型的性能和稳定性。</li></ul><h5 id="分析计算复杂度"><a href="#分析计算复杂度" class="headerlink" title="分析计算复杂度"></a>分析计算复杂度</h5><ul><li><p>Vision Transformer 中计算复杂度是平方次幂级别的，具有高计算量和内存消耗；这也在最近大量论文针对解决的问题，降低2次幂的复杂度问题。</p></li><li><p>分析Vision Transformer的计算复杂度：</p><ul><li><p>Multi-head Attention：输入query, key, value; 其中query, key首先通过矩阵相乘得到一个attention矩阵，然后 attention 矩阵再通过和value进行矩阵相乘得到了加了attention信息的feature。我们知道query，key，value来自同一个feature，但是需要通过三个不一样的全连接层映射到不一样的投影空间。假设输入的 feature 的特征维度是：$N  \times D \times H \times W$, 分别通过三个不一样的全连接层 $D \times D$， 此处他们三个的权值是不共享的，所以：</p><ul><li>参数量：$3 \times D \times D$ ；计算量： $3 \times HW \times D \times D$ ；其中3就是三个不一样的全连接层分别得到q，k，v。</li></ul><p>接下来是multi-head的过程，multi-head的原理是把通道维度D分为多个head，每个head学习不同方面的attention信息，同时参数量和计算量也不会额外增加。只包含计算两次矩阵相乘的结果：</p><ul><li>query@key and attention_map@value：$2 \times (HW)^2 \times D$;</li></ul><p>最后再接一个全连接层：</p><ul><li>参数量：$D \times D$ ；计算量： $ HW \times D \times D$ ；</li></ul><p><strong>总结：</strong></p><p><strong>参数量：$4 \times D \times D$ ；计算量： $4 \times HW \times D^2 + 2(HW)^2 \times D$ ；2次幂指的是$(HW)^2$</strong></p></li><li><p>FFN过程：经过两个全连接层：计算量为 $2 \times HW \times D \times D_{hiddn}$ </p></li></ul></li><li><p>减少计算量的主要方面：</p><ul><li>1、Token 通道修剪：就是修剪一开始线性生成q，k，v的 feature 的通道D，意味着减少了全连接层的计算量</li><li>2、Token 数量的修剪：就是减少 HW（N）的数量，这会直接影响到2次幂的关键，也是最常见的减少计算量的方式</li><li>3、Attention 通道的修剪：就是减少计算attention时的通道数目。</li></ul></li></ul><h5 id="VIT和CNN的对比分析"><a href="#VIT和CNN的对比分析" class="headerlink" title="VIT和CNN的对比分析"></a>VIT和CNN的对比分析</h5><ul><li>CNN的<strong>归纳偏置</strong>(Inductive Bias)主要有两种：局部感知性(locality )；平移不变性( translation equivariance) ==&gt;需要较少的数据去学好一个模型</li><li>VIT中的自注意力机制层是全局的，比CNN结构少了一定的平移不变性和局部感知性，<strong>在数据量较少的情况下</strong>，效果可能不如CNN模型，但是在大规模数据集上预训练过后，再进行迁移学习，可以在特定任务上达到SOTA性能。</li></ul><h5 id="Transformer与LSTM的对比"><a href="#Transformer与LSTM的对比" class="headerlink" title="Transformer与LSTM的对比"></a>Transformer与LSTM的对比</h5><ul><li><p>是对RNN存在的梯度消失、梯度爆炸问题的一种优化模型。</p></li><li><p>LSTM引入三个控制门<strong>（输入门，输出门，遗忘门）</strong>，拥有了长期记忆，更好的解决了<strong>RNN的梯度消失和梯度爆炸问题</strong>。</p></li><li><p>LSTM的三个门的作用：<strong>输入门</strong>决定何时让输入进入细胞单元；<strong>遗忘门</strong>决定何时应该记住前一时刻的信息；<strong>输出门</strong>决定何时让记忆流入下一时刻。</p></li><li><p>LSTM包含了两种激活函数，sigmoid 用在了各种门限上，产生0~1之间的值。tanh 用在了状态和输出上，是对数据的处理。</p></li><li><p><strong>LSTM是怎么解决梯度消失的问题的？</strong></p><p>传统的神经网络层数一多，就会有梯度消逝和爆炸的现象，因为导数的链式法则导致了连乘的形式。造成梯度指数级的消失，lstm使用CEC(constant error carousel)机制，使得远处的梯度传到近处没有改变、但这样又会造成输入输出权重矛盾，所以又使用了门限单元来解决。</p><p>因为lstm有两个通道可以保持记忆：短期记忆h，保持非线性操作；长期记忆C，保持线性操作。因为线性操作是比较稳定的，所以C的变化相对稳定，保持了长期记忆。而对有用信息的长期记忆是通过训练获得的，也就是说在内部的几个权重矩阵中。</p><p>LSTM总可以通过选择合适的参数，再不发生梯度爆炸的情况下，找到合理的梯度方向来更新参数，而且这个方向可以充分考虑远距离的隐藏层信息的传播影响。</p></li><li><p><strong>Transformer相对于LSTM的优势</strong></p><ul><li><strong>并行计算</strong>：RNN 和LSTM 需要顺序处理序列数据，有时许的概念，因此很难进行并行计算。而 Transformer 的 自注意力机制允许同时处理整序列，从而可以充分利用 GPU 的并行计算能力，大大提高模型训练和推理的速度。</li><li><strong>可解释性</strong>：Transformer 中的自注章力机制为每个位置的输出都分配了一个权重，这些权重表明了输入序列中不同位置对于输出的贡献。这使得 Transformer 更具可解释性，可以直观地观察模型在处理序列数据时关注的区域。</li><li><strong>模型容量</strong>：Transformer 可以很容易地堆叠多层，从而增加模型容量，多层 Transformer 结构可以让模型学习更复杂和抽象的表示。</li></ul><p>尽管 Transformer 在许多方面具有优势，但它也有一些局限，如需要<strong>大量的计算资源和内存，以及可能产生较高的计算复杂度</strong>，</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见问题总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="深度学习基础常见问题总结"><a href="#深度学习基础常见问题总结" class="headerlink" title="深度学习基础常见问题总结"></a>深度学习基础常见问题总结</h3><h4 id="神经网络中1-1卷积有什么作用？"><a href="#神经网络中1-1卷积有什么作用？" class="headerlink" title="神经网络中1*1卷积有什么作用？"></a>神经网络中1*1卷积有什么作用？</h4><ul><li><p><strong>降维</strong>，减少计算量；在ResNet模块中，先通过11卷积对通道数进行降通道，再送入33的卷积中，能够有效的减少神经网络的参数量和计算量；</p></li><li><p><strong>升维</strong>；用最少的参数拓宽网络通道，通常在轻量级的网络中会用到，经过深度可分离卷积后，使用1*1卷积核增加通道的数量。</p></li><li><p><strong>实现跨通道的交互和信息整合</strong>；增强通道层面上特征融合的信息，在feature map尺度不变的情况下，实现通道升维、降维操作其实就是通道间信息的线性组合变化，也就是通道的信息交互整合的过程；</p></li><li><p><strong>增加非线性</strong>；1*1卷积核可以在保持feature map尺度（不损失分辨率）不变的情况下，大幅增加非线性特性（利用后接的非线性激活函数）。</p></li></ul><p>卷积神经网络中用1*1 卷积有什么作用或者好处呢？</p><ul><li>卷积后再次应用非线性</li></ul><h4 id="梯度消失和梯度爆炸的原因是什么？怎么解决？"><a href="#梯度消失和梯度爆炸的原因是什么？怎么解决？" class="headerlink" title="梯度消失和梯度爆炸的原因是什么？怎么解决？"></a>梯度消失和梯度爆炸的原因是什么？怎么解决？</h4><p>梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。</p><ul><li><p><strong>梯度消失和梯度爆炸的概念</strong></p><ul><li><p>根据损失函数计算的误差通过梯度<strong>反向传播</strong>的方式对深度网络权值进行更新时，得到的<strong>梯度值接近0</strong>或<strong>特别大</strong>，也就是<strong>梯度消失</strong>或<strong>爆炸</strong>。梯度消失或梯度爆炸在本质原理上其实是一样的。</p></li><li><p>梯度消失：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较小时，则权重参数呈指数级减小。</p></li><li>梯度爆炸：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较大时，则权重参数呈指数级增长。</li></ul></li><li><p><strong>分析产生梯度消失和梯度爆炸的原因</strong></p><ul><li><p><strong>【梯度消失】</strong>：(1) 深层网络；(2) 不合适的激活函数（sigmoid）</p></li><li><p><strong>【梯度爆炸】</strong>：(1) 深层网络；(2) 权重初始值太大</p></li><li><p>深层网络：</p><p>由于深度网络是多层非线性函数的堆砌，整个深度网络可以视为是一个<strong>复合的非线性多元函数</strong>（这些非线性多元函数其实就是每层的激活函数），那么对loss function求不同层的权值偏导，相当于应用梯度下降的链式法则，链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数传播。</p><p>如果接近输出层的激活函数求导后梯度值大于1，那么层数增多的时候，最终求出的梯度很容易指数级增长，就会产生<strong>梯度爆炸</strong>；相反，如果小于1，那么经过链式法则的连乘形式，也会很容易衰减至0，就会产生<strong>梯度消失</strong>。</p><p>从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。</p></li><li><p>激活函数：</p><p>类似 <strong>sigmoid 激活函数</strong>，在链式求导梯度回传过程中，返回的值很小，层数越多，求导结果越小，最终导致梯度消失的情况出现。</p></li><li><p>初始化权重过大</p><p>权重比较大的情况。根据链式相乘(反向传播)可得，则前面的网络层比后面的网络层梯度变化更快，很容易发生梯度爆炸的问题。所以在一般的神经网络中，权重的初始化一般都利用高斯分布(正态分布)随机产生权重值。</p></li></ul></li><li><p><strong>梯度消失和梯度爆炸的解决方案</strong></p><ul><li><p><strong>pre-training + fine-tunning</strong></p><p>Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p></li><li><p><strong>梯度剪切：对梯度设定阈值</strong></p><p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p></li><li><p><strong>权重正则化</strong></p><p>解决梯度爆炸的手段是采用权重正则化（weithts regularization），正则化主要是通过对网络权重做正则来限制过拟合。如果发生梯度爆炸，那么权值就会变的非常大，反过来，通过正则化项来限制权重的大小，也可以在一定程度上防止梯度爆炸的发生。比较常见的是 L1 正则和 L2 正则，在各个深度框架中都有相应的API可以使用正则化。</p></li><li><p><strong>选择 relu，leakrelu，elu等梯度大部分在常数上的激活函数</strong></p><p>relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</p></li><li><p><strong>Batch Normalizaition (BN)</strong></p><p>BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。</p></li><li><p><strong>残差结构（shortcut）</strong></p><p>比较于以前直来直去的网络结构，残差中有很多的跨层连接结构，这样的结构在反向传播中具有很大的好处，可以避免梯度消失。kaiming大佬的杰作！</p></li></ul></li></ul><h4 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h4><ul><li><p>欠拟合是指模型不能在训练集上获得足够低的误差。</p></li><li><p>过拟合是指训练误差和测试误差之间的差距太大，神经网络对训练数据进行很好的建模但在看到来自同一问题域的新数据时失败的现象。</p><p><strong>模型容量低，而训练数据复杂度高</strong>时，此时模型由于学习能力不足，无法学习到数据集中的“一般规律”，因而导致训练误差高，表现为欠拟合。</p><p><strong>模型容量高，而训练数据简单</strong>时，此时模型由于学习能力太强，记住所有的训练数据，却没有理解数据背后的规律，对于训练集以外的数据泛化能力差，表现为过拟合。</p></li></ul><h5 id="解决欠拟合"><a href="#解决欠拟合" class="headerlink" title="解决欠拟合"></a>解决欠拟合</h5><ul><li>在模型容量和训练数据复杂度不匹配时，发生了欠拟合现象，常见解决方法有：</li><li><strong>增加新特征</strong>：可以考虑加入特征组合、高次特征等，来增大假设空间。</li><li><strong>增大模型容量</strong>：容量低的模型可能很难拟合训练集。</li><li><strong>减少正则化参数</strong>：正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数。</li></ul><h5 id="解决过拟合"><a href="#解决过拟合" class="headerlink" title="解决过拟合"></a>解决过拟合</h5><p><strong>引起过拟合</strong>的主演原因如下：</p><ul><li><strong>训练数据集问题</strong>。如训练样本不均衡，训练集中正样本偏多，那么去预测负样本肯定不准；训练样本数据少，尤其是当比模型参数数量还少时，更容易发生过拟合；训练数据噪声干扰过大，模型会学习很多的噪声特征等。</li><li><strong>模型过于复杂</strong>。模型参数数量太多，参数取值范围太大，模型已经能够“死记硬背”记下了所有训练数据的信息（记住了不适合于测试集的训练集特性）；模型假设的合理性不存在，也就是假设成立的条件实际并不成立。</li><li><strong>模型训练迭代次数太多</strong>。对数据反复地训练也可能会让模型拟合了训练样本中没有代表性的特征</li></ul><p><strong>解决过拟合问题</strong>，最重要的减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。通常解决过拟合的方法有：</p><ul><li>数据集增强</li><li>控制模型容量大小</li><li>正则化（Regularization）L1和L2</li><li>丢弃法（Dropout）</li><li>提前终止训练（Early Stopping）</li></ul><h4 id="Loss为Nan，为何？怎么解决？"><a href="#Loss为Nan，为何？怎么解决？" class="headerlink" title="Loss为Nan，为何？怎么解决？"></a>Loss为Nan，为何？怎么解决？</h4><ul><li><p>排查顺序</p><ul><li>是否有脏数据，训练数据（包括标签）是否有异常值（nan，inf等）</li><li>网络中是否有除法，分母可能出现0的情况，这种情况一般添加一个平滑项，+ eps=1e-8</li><li>开跟号，也是要保证根号下 &gt;= 0的，这种情况也是添加平滑项</li><li>在某些涉及指数计算，可能最后算得值为INF（无穷）（比如不做其他处理的softmax中分子分母需要计算exp(x)，值过大，最后可能为INF/INF，得到NaN，此时你要确认你使用的softmax中在计算exp(x)做了相关处理（比如减去最大值等等））</li></ul></li><li><p>典型案例</p><ul><li><p>梯度爆炸，</p><p>梯度变得非常大，使得学习过程难以继续。Loss随着每轮迭代越来越大，最终超过了浮点型表示范围，就变成NaN了，</p><p>改善措施：数据归一化(BN,L2 norm等)；参数初始方法；学习率；batch-size；梯度截断；</p></li><li><p>不当的损失函数，</p><p>有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p><p>改善措施：重现错误，逐步排查。</p></li><li><p>不当的输入，</p><p>就是 training sample中出现了脏数据，脏数据导致网络输出的logits计算出了0，0计算损失得到NaN，</p><p>改善措施：找到脏数据剔除即可，设置batch-size=1，一个一个来判断。</p></li></ul></li></ul><h4 id="深度学习中常用的优化器"><a href="#深度学习中常用的优化器" class="headerlink" title="深度学习中常用的优化器"></a>深度学习中常用的优化器</h4><p>Pytorch中优化器的目的：将损失函数计算出来的插值Loss减少。</p><p>优化过程：优化器计算网络参数的梯度，然后使用一定的算法策略来对参数进行计算，用新的参数来重进行训练，最终降低Loss。</p><ul><li><p><strong>SGD算法</strong></p><p><strong>SGD算法可以实现随机梯度下降，融合了动量梯度，权重衰减，没有自适应学习率机制，但仔细调参最终模型效果可能最好</strong></p><p>基本思想：<strong>通过当前梯度和历史梯度共同调节梯度的方向和大小</strong>，<strong>所有参数共享一个学习率，对学习率敏感</strong></p><p><code>torch.optim.SGD(params, lr= , momentum=0, damening=0, weight_decay=0, nesterov=False)</code></p><ul><li>params：可迭代参数以优化或定义参数组</li><li>lr：初始学习率，可随着训练过程不断调整学习率</li><li>momentum：动量，通常设置为0.9，0.8，又称动量梯度下降</li><li>damening：动量阻尼，默认为0</li><li>weight_decay：权值衰减系数，即L2正则化</li></ul></li><li><p>Adagrad算法</p><p><strong>Adagrad(Adaptive Gradient)的核心思想是</strong>，深度模型带来的稀疏性，导致模型中一些参数可能频繁获得较大梯度，另一些参数偶尔获得较大梯度，若采用统一学习率导致后者的更新会非常缓慢。<strong>基于此，可以调节模型中不同参数的学习率，而不是用统一的学习率</strong>。如果一个参数的历史累计梯度更新量大，则降低该参数的学习率；如果一个参数的历史累计梯度更新量小，则增大该参数的学习率。</p><ul><li><p>Adagrad算法可以自适应的给所有的参数分配学习率，学习率的大小与梯度的大小成反比。</p></li><li><p>因为梯度有正有负所以对梯度的平方进行累计。</p></li><li>对每个参数分别调节学习率，也可以看成调节每个参数的梯度。</li><li>缺点是随着迭代次数增多，累计值越来越大，导致模型参数的实际更新越来微弱</li></ul></li><li><p>RMSprop算法</p><p>RMSprop算法是Adagrad的改进形式，特点是使用<strong>指数衰减滑动平均来更新梯度平方</strong>，以避免Adagrad累计梯度平方导致学习率过小的缺点。区别于adagrad之处是它采用了EMA方式来统计每个参数的最近的累计梯度量，所以多次迭代后不会导致模型参数更新缓慢。</p></li><li><p><strong>Adam算法</strong></p><p><strong>Adam(Adaptive Moment Estimation)优化算法，整合了RMSprop中的自适应梯度机制和动量梯度机制。Adam优化器实现中还加入了权重衰减机制。</strong></p><p><strong>可以自适应调节模型中不同参数的学习率，而不是用统一的学习率，所以对学习率相对不敏感。</strong> </p><p><code>torch.optim.Adam(network.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, amsgrad=False)</code></p></li><li><p>AdamW算法</p><p>AdamW优化器修正了Adam中权重衰减的bug。</p></li><li><p><strong>总结</strong>：</p><ul><li><strong>SGD：没有自适应学习率机制，但仔细调参最终模型效果可能最好，需要慢慢调节学习率可以最佳效果。</strong></li><li><strong>Adam：学习率相对不敏感，减少调参试验代价，没有条件调参建议采用也可以达到较好的水平。但weight decay实现不好，能用Adam的地方可以都用AdamW来代替。</strong></li><li><strong>AdamW：相对于Adam，权重衰减（weight decay）实现解耦，效果更好，可代替Adam。</strong></li></ul></li></ul><h4 id="为什么输入网络前要对图像做归一化？"><a href="#为什么输入网络前要对图像做归一化？" class="headerlink" title="为什么输入网络前要对图像做归一化？"></a>为什么输入网络前要对图像做归一化？</h4><h4 id="感受野的计算问题？"><a href="#感受野的计算问题？" class="headerlink" title="感受野的计算问题？"></a>感受野的计算问题？</h4><ul><li><p><strong>感受野（Receptive Field）：</strong>卷积神经网络中每一层输出的特征图（feature map）中的每一个像素点在输入图片上映射的大小。也就是特征图上的一个像素点对应输入图上的区域。</p></li><li><p><strong>感受野的计算：</strong></p><ul><li><p>最后一层（卷积层或者池化层）输出特征图感受野的大小等于卷积核的大小。</p></li><li><p>第i层卷积层的感受野大小和第i层的卷积核大小和步长有关系，同时也与第（i+1）层感受野大小相关。</p></li><li><p>计算感受野的大小时忽略图像边缘的影响，即不考虑padding大小。</p></li><li><p><strong>关于感受野大小的计算方式是从最后一层往下计算的方法</strong>，即先计算最深层在前一层上的感受野，然后计算最深层在前二层上的感受野，然后依次递归到第一层，计算的流程和公式如下：</p><p>1、设要计算感受野的这层为第N层</p><p>2、第<code>N</code>层到第<code>N-1</code>层的感受野就是对第<code>N-1</code>层进行卷积时使用的卷积核大小，这里我们设为$RF_{N-1}$。</p><p>3、接着计算第<code>N</code>层到第<code>N-2</code>层的感受野大小，公式是：</p><script type="math/tex; mode=display">RF_{N-2} = (RF_{N-1} -1)*stride + kernel\_size</script><p><strong>注意：这里的<code>stride</code>和<code>kernel_size</code>是第<code>N-2</code>层的</strong></p></li></ul></li><li><p><strong>记住：两层3x3的卷积核的感受野为5，三层3X3的卷积核的感受野为7.</strong> 其中stride=1，padding=0</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230918/1.jpg" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230918/2.jpg" style="zoom:80%;" /></p></li><li><p>感受野计算的例子：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230918/3.jpg" alt=""></p><p>从最后一层的池化层开始计算感受野：</p><script type="math/tex; mode=display">\begin{aligned}pool3：&RF=2（最后一层池化层输出特征图的感受野大小等于卷积核的大小）\\conv4：&RF=(2-1)*1+3=4。\\conv3：&RF=(4-1)*1+3=6。\\pool2：&RF=(6-1)*2+2=12。\\conv2：&RF=(12-1)*1+3=14。\\pool1：&RF=(14-1)*2+2=28。\\conv1：&RF=(28-1)*1+3=30。\\\end{aligned}</script><p>因此，pool3输出的特征图在输入图片上的感受野为30*30。</p><p><strong>要从最后一层向前计算感受野，而不是从前往后计算感受野。</strong></p></li><li><p><strong>为什么用多个3X3的小卷积核代替大卷积和？</strong>（2个3X3代替5X5；3个3X3代替7X7）</p><ul><li><p>（1）保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果；（2）减少计算参数量。</p></li><li><p>3x3是最小的能够捕获像素八邻域信息的尺寸。</p></li><li>两个3x3的堆叠卷基层的有限感受野是5x5；三个3x3的堆叠卷基层的感受野是7x7，故可以通过小尺寸卷积层的堆叠替代大尺寸卷积层，并且感受野大小不变。</li><li>多个3x3的卷基层比一个大尺寸filter卷基层有更多的非线性（更多层的非线性函数，使用了3个非线性激活函数）。</li><li>多个3x3的卷积层比一个大尺寸的filter有更少的参数。</li><li>唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存；</li></ul></li></ul><h4 id="MLP和卷积的区别"><a href="#MLP和卷积的区别" class="headerlink" title="MLP和卷积的区别"></a>MLP和卷积的区别</h4><p><strong>MLP</strong>：全连接神经网络，在视觉这边往往会<strong>存在如下两个问题</strong></p><ul><li><strong>1、输入数据的空间信息被丢失。</strong>空间上相邻的像素点往往具有相似的RGB值，RGB的各个通道之间的数据通常密切相关，但是转化成1维向量时，这些信息被丢失。如 <strong>下图</strong> 所示，空间位置相邻的两个点A和B，转化成1维向量后并没有体现出他们之间的空间关联性。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230919/1.jpg" alt=""></p><ul><li><strong>2、模型参数过多，容易发生过拟合。</strong>由于每个像素点都要跟所有输出的神经元相连接。当图片尺寸变大时，输入神经元的个数会按图片尺寸的平方增大，导致模型参数过多，容易发生过拟合。</li></ul><p><strong>卷积（Convolution）来对输入的图像进行特征提取。</strong>卷积的计算范围是在像素点的空间邻域内进行的，因此可以利用输入图像的空间信息；此外，由于卷积具<strong>有局部连接、权重共享</strong>等特性，卷积核参数的数目也远小于全连接层。</p><ol><li><strong>局部连接：</strong>全连接层是一种稠密连接方式，而卷积层却只使用卷积核对局部进行处理，这种处理方式其实也刚好对应了图像的特点。在视觉识别中，关键性的图像特征、边缘、角点等只占据了整张图像的一小部分，相隔很远的像素之间存在联系和影响的可能性是很低的，而局部像素具有很强的相关性。</li><li><strong>共享参数：</strong>如果借鉴全连接层的话，对于1000×1000大小的彩色图像，一层全连接层便对应于三百万数量级维的特征，即会导致庞大的参数量，不仅计算繁重，还会导致过拟合。而卷积层中，卷积核会与局部图像相互作用，是一种稀疏连接，大大减少了网络的参数量。另外从直观上理解，依靠卷积核的滑动去提取图像中不同位置的相同模式也刚好符合图像的特点，不同的卷积核提取不同的特征，组合起来后便可以提取到高级特征用于最后的识别检测了。</li></ol><h4 id="医学图像分割与自然图像分割的区别与难点"><a href="#医学图像分割与自然图像分割的区别与难点" class="headerlink" title="医学图像分割与自然图像分割的区别与难点"></a>医学图像分割与自然图像分割的区别与难点</h4><p>医学图像相比于自然图像（通过可见光成像）有以下<strong>区别</strong>：</p><ul><li><strong>医学图像的模态（格式）更加多样化</strong>，而且不同型号的成像设备得到的成像结果有一定差异。</li><li><strong>医学图像的像素值范围与自然图像（0~255）有很大差别，如CT一般会上千。</strong></li><li><strong>噪声</strong>。由于成像设备、成像原理以及个体自身差异的影响，医学图像一般会含有很多噪声。由于噪声对于位置和空间约束是独立的，从而可以利用噪声的分布来实现降噪，但是在抑制噪声的同时也需要考虑图像细节的保留问题。</li><li><strong>伪影</strong>。伪影一般是在图像配准或三维重建时产生（如CT），从原理上来，只能减少，无法消除。</li><li><strong>成像原理带来的干扰，噪声，以及伪影。</strong></li></ul><p>深入理解医学图像的格式和特点；设计合适的图像预处理操作增强目标特征；将原始格式的数据处理为适合深度学习模型输入的格式。</p><p>医学图像相比于自然图像（通过可见光成像）有以下<strong>难点</strong>：</p><ol><li><strong>数据可用性</strong>：获取医学图像数据通常受到法规和伦理问题的限制。因此，医学图像的数据可用性通常有限。这使得在医学图像分割中使用深度学习方法变得更加具有挑战性，因为这些方法通常需要大量的标注数据。</li><li><strong>标注困难</strong>：正确标注医学图像需要医学专业知识，而且通常需要专业人员的参与。这增加了标注的成本和时间，因为需要确保标注的准确性，以支持医学诊断。</li><li><strong>类别不平衡</strong>：在医学图像中，不同的结构和组织可能出现的频率差异很大。例如，在肿瘤分割任务中，正例（肿瘤区域）通常比负例（非肿瘤区域）稀缺。这种类别不平衡可能导致模型对常见类别更为敏感，而忽略了罕见的类别。</li><li><strong>模型泛化</strong>：由于医学图像数据有限，模型需要在不同的机器和数据集上进行有效的泛化，以在新的患者和医疗场景中进行准确的分割。这需要特殊的技术来处理数据集之间的差异。</li><li>数据量少，标注要求高。</li><li>在某些医学分割任务中，<strong>目标区域和背景区域的对比度极低</strong>，导致边界很难辨别。即使不是专业的医学人士都是很难通过肉眼辨别的。</li></ol><h4 id="改进小目标识别的几个有效策略"><a href="#改进小目标识别的几个有效策略" class="headerlink" title="改进小目标识别的几个有效策略"></a>改进小目标识别的几个有效策略</h4><ul><li><strong>提升图像采集的分辨率</strong>：非常小的物体的边界框中可能只包含几个像素，这意味着增加图像的分辨率可以增加探测器可以从那个小盒子中形成的丰富特征，这是非常重要的。</li><li><strong>提高模型的输入分辨率</strong>：有了更高分辨率的图像，你就可以放大模型的输入分辨率。警告：这将导致大型模型需要更长的时间来训练，并且当你开始部署时，也会更慢地进行推断。你可能需要实验来找出速度与性能之间的正确权衡。</li><li><strong>对图像进行Tiling技术</strong>：检测小物体的另一个重要策略是将图像切割后形成batch，这个操作叫做tile，作为预处理步骤。tile可以有效地将检测器聚焦在小物体上，但允许你保持所需的小输入分辨率，以便能够运行快速推断。tile图像作为预处理步骤，如果你在训练中使用tile，重要的是要记住，你也需要在推理时tile你的图像。</li><li><strong>通过增强产生更多数据</strong>：数据增强是一种提升小目标检测性能的最简单和有效的方法，通过不同的数据增强策略可以扩充训练数据集的规模，丰富数据集的多样性，从而增强检测模型的鲁棒性和泛化能力</li><li><strong>多尺度学习</strong>：浅层特征图感受野小，更适合检测小目标，深层特征图较大，更适合检测大目标。因此，有人提出将不同阶段的特征映射整合在一起来提高目标检测性能，称之为特征金字塔网络FPN。。<strong>小目标与常规目标相比可利用的像素较少，难以提取到较好的特征，而且随着网络层数的增加，小目标的特征信息与位置信息也逐渐丢失，难以被网络检测</strong>。这些特性导致小目标同时需要深层语义信息与浅层表征信息，而多尺度学习将这两种相结合，是一种提升小目标检测性能的有效策略。</li><li>损失函数：可以在小目标的loss上施加更高的权重，使得网络更关注小目标的优化。</li></ul><h4 id="边界分割不准时需做出哪些改进"><a href="#边界分割不准时需做出哪些改进" class="headerlink" title="边界分割不准时需做出哪些改进"></a>边界分割不准时需做出哪些改进</h4>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 基础问答 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用Attention总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8Attention%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8Attention%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h4 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h4><p><strong>什么是注意力机制？</strong>注意力机制是<strong>深度学习常用的一个小技巧</strong>，它有多种多样的实现形式，尽管实现方式多样，但是<strong>每一种注意力机制的实现的核心</strong>都是类似的，就是<strong>注意力</strong>。</p><p><strong>注意力机制的核心重点就是让网络关注到它更需要关注的地方。</strong></p><p>当我们使用卷积神经网络去处理图片的时候，<strong>我们会更希望卷积神经网络去注意应该注意的地方</strong>，而不是什么都关注，我们不可能手动去调节需要注意的地方，这个时候，<strong>如何让卷积神经网络去自适应的注意重要的物体变得极为重要</strong>。</p><p><strong>注意力机制就是实现网络自适应注意的一个方式。</strong></p><p>一般而言，注意力机制可以分为<strong>通道注意力机制（Channel-Attention），空间注意力机制（Spatial-Attention），以及二者的结合，自注意力（Self-Attention）</strong>。</p><h4 id="SENet的实现"><a href="#SENet的实现" class="headerlink" title="SENet的实现"></a>SENet的实现</h4><p><strong>（1）SENet是通道注意力的典型实现</strong></p><p>（2）示意图如下所示，对于输入进来的特征层，我们关注每个通道的权重，对于SENet而言，<strong>其重点是获得输入进来的特征层，每个通道的权值</strong>。利用SENet，可以让网络关注它最需要关注的通道。</p><p>（3）具体实现：</p><ul><li>1）对输入进来的特征层进行全局平均池化，通过pooling操作从而将通道压缩（Squeeze）到一个标量值。</li><li>2）然后进行两次全连接，第一次全连接神经元个数较少，第二次全连接神经元个数和输入特征层相同。</li><li>3）在完成两次全连接后，我们再取一次<strong>Sigmoid</strong>将值固定到0-1之间，此时我们获得了输入特征层每一个通道的权值（0-1之间）。</li><li>4）在获得这个权值后，我们将这个权值乘上原输入特征层即可。给通道加权的过程对应了SE中的Excitation。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230808/1_1.png" alt=""></p><p>（4）代码实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import math</span><br><span class="line">class se_block(nn.Module):</span><br><span class="line">    def __init__(self, channel, ratio=16):</span><br><span class="line">        super(se_block, self).__init__()</span><br><span class="line">        # 自适应全局池化</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(1) </span><br><span class="line">        # 全连接层</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">                nn.Linear(channel, channel // ratio, bias=False),</span><br><span class="line">                nn.ReLU(inplace=True),</span><br><span class="line">                nn.Linear(channel // ratio, channel, bias=False),</span><br><span class="line">                nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c) # 池化</span><br><span class="line">        y = self.fc(y).view(b, c, 1, 1) # 全连接</span><br><span class="line">        return x * y # 作用回去原来特征</span><br></pre></td></tr></table></figure><p>（5）以SE Attention为基准，后续又有很多工作对其进行了扩展。这些扩展主要体现在两方面，<strong>一个是通道特征的提取</strong>，另一个是如何<strong>由通道特征得到权重</strong>，对于前者，SE Attention中采用全局均值pooling来提取通道特征，这可以看作是一种一阶的特征提取方法。改进的方案通常是采用二阶或高阶的复杂特征，对于后者，SE Attention中采用全连接层来生成权重，因此每一个通道的权重计算都包含了所有通道特征的信息，是一种全局的计算方式。<strong>改进的方案通常采用1D卷积操作来进行局部的权重计算</strong>，也就是说每一个通道的权重计算只与其相邻的部分通道相关。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230808/2.jpg" style="zoom:80%;" /></p><h4 id="ECA的实现"><a href="#ECA的实现" class="headerlink" title="ECA的实现"></a>ECA的实现</h4><p>ECANet是也是通道注意力机制的一种实现形式。ECANet可以看作是SENet的改进版。</p><p>ECANet的作者认为<strong>SENet对通道注意力机制的预测带来了副作用，捕获所有通道的依赖关系是低效并且是不必要的。</strong></p><p>在ECANet的论文中，<strong>作者认为卷积具有良好的跨通道信息获取能力。</strong></p><p>ECA模块的思想是非常简单的，它去除了原来SE模块中的全连接层，直接在全局平均池化之后的特征上通过一个1D卷积进行学习。</p><p>既然使用到了1D卷积，<strong>那么1D卷积的卷积核大小的选择就变得非常重要了</strong>，了解过卷积原理的同学很快就可以明白，<strong>1D卷积的卷积核大小会影响注意力机制每个权重的计算要考虑的通道数量。用更专业的名词就是跨通道交互的覆盖率</strong>。</p><p>如下图所示，左图是常规的SE模块，右图是ECA模块。ECA模块用1D卷积替换两次全连接。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230808/5.png" style="zoom:80%;" /></p><p>代码实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class eca_block(nn.Module):</span><br><span class="line">    def __init__(self, channel, b=1, gamma=2):</span><br><span class="line">        super(eca_block, self).__init__()</span><br><span class="line">        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))</span><br><span class="line">        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) </span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        y = self.avg_pool(x)</span><br><span class="line">        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)</span><br><span class="line">        y = self.sigmoid(y)</span><br><span class="line">        return x * y.expand_as(x)</span><br></pre></td></tr></table></figure><h4 id="CBAM的实现"><a href="#CBAM的实现" class="headerlink" title="CBAM的实现"></a>CBAM的实现</h4><p>（1）CBAM将<strong>通道注意力机制和空间注意力机制</strong>进行一个结合，相比于<strong>SENet只关注通道的注意力机制</strong>可以取得更好的效果。其实现示意图如下所示，CBAM会对输入进来的特征层，分别进行<strong>通道注意力机制的处理和空间注意力机制的处理</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230808/3.png" style="zoom:80%;" /></p><p>（2）下图是通道注意力和空间注意力机制的具体实现：</p><ul><li>1）图像的上半部分为<strong>通道注意力机制</strong>，通道注意力机制的实现可以分为两个部分，我们会对输入进来的单个特征层，分别进行<strong>全局平均池化和全局最大池化</strong>。之后对平均池化和最大池化的结果，利用<strong>共享的全连接层进行处理</strong>，我们会对处理后的两个结果进行相加，然后取一个<strong>sigmoid</strong>，此时我们<strong>获得了输入特征层每一个通道的权值（0-1之间）</strong>。在获得这个权值后，我们将这个权值乘上原输入特征层即可。</li><li>2）图像的下半部分为<strong>空间注意力机制</strong>，我们会对输入进来的特征层，<strong>在每一个特征点的通道上取最大值和平均值</strong>。之后将这两个结果进行一个堆叠，利用一次<strong>通道数为1的卷积</strong>调整通道数，然后取一个<strong>sigmoid</strong>，此时我们获得了<strong>输入特征层每一个特征点的权值（0-1之间）</strong>。在获得这个权值后，我们将这个权值乘上原输入特征层即可。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230808/4.png" style="zoom:80%;" /></p><p>（3）代码实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># 通道注意力</span><br><span class="line">class ChannelAttention(nn.Module):</span><br><span class="line">    def __init__(self, in_planes, ratio=8):</span><br><span class="line">        super(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(1)</span><br><span class="line">        # 利用1x1卷积代替全连接</span><br><span class="line">        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        out = avg_out + max_out</span><br><span class="line">        return self.sigmoid(out)</span><br><span class="line"># 空间注意力</span><br><span class="line">class SpatialAttention(nn.Module):</span><br><span class="line">    def __init__(self, kernel_size=7):</span><br><span class="line">        super(SpatialAttention, self).__init__()</span><br><span class="line">        assert kernel_size in (3, 7), &#x27;kernel size must be 3 or 7&#x27;</span><br><span class="line">        padding = 3 if kernel_size == 7 else 1</span><br><span class="line">        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avg_out = torch.mean(x, dim=1, keepdim=True)</span><br><span class="line">        max_out, _ = torch.max(x, dim=1, keepdim=True)</span><br><span class="line">        x = torch.cat([avg_out, max_out], dim=1)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        return self.sigmoid(x)</span><br><span class="line"># 通道注意力和空间注意力相结合得到CBAM模块</span><br><span class="line">class cbam_block(nn.Module):</span><br><span class="line">    def __init__(self, channel, ratio=8, kernel_size=7):</span><br><span class="line">        super(cbam_block, self).__init__()</span><br><span class="line">        self.channelattention = ChannelAttention(channel, ratio=ratio)</span><br><span class="line">        self.spatialattention = SpatialAttention(kernel_size=kernel_size)</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x * self.channelattention(x)</span><br><span class="line">        x = x * self.spatialattention(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h4 id="自注意力的实现（Self-Attention）"><a href="#自注意力的实现（Self-Attention）" class="headerlink" title="自注意力的实现（Self-Attention）"></a>自注意力的实现（Self-Attention）</h4><ul><li>Transformer的重要组成模块：self-attention，mult-head attention。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Loss_in_Deep_Learning</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Loss-in-Deep-Learning/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Loss-in-Deep-Learning/</url>
      
        <content type="html"><![CDATA[<h3 id="Loss-in-Deep-Learning"><a href="#Loss-in-Deep-Learning" class="headerlink" title="Loss in Deep Learning"></a>Loss in Deep Learning</h3><p>监督学习主要分为两类</p><ul><li><p>回归问题：目标输出变量是连续结果，如预测西瓜的含糖率（0.00~1.00）</p></li><li><p>分类问题：目标输出变量是离散结果，如判断一个西瓜是好瓜还是坏瓜，那么目标变量只能是1（好瓜）,0（坏瓜）</p><ul><li>二分类问题：只有两个类别</li><li>多分类问题：多个类别。</li></ul></li></ul><p>损失函数严格上可分为两类：<strong>分类损失</strong>和<strong>回归损失</strong>，其中<strong>分类损失</strong>根据类别数量又可分为<strong>二分类损失</strong>和<strong>多分类损失</strong>。在使用的时候需要注意的是：<strong>回归函数预测数量，分类函数预测标签</strong>。</p><h4 id="回归损失函数"><a href="#回归损失函数" class="headerlink" title="回归损失函数"></a>回归损失函数</h4><h5 id="平均绝对误差损失-MAE"><a href="#平均绝对误差损失-MAE" class="headerlink" title="平均绝对误差损失(MAE)"></a>平均绝对误差损失(MAE)</h5><p>平均绝对误差 Mean Absolute Error(MAE) 是常用的损失函数，也称为 L1 Loss，其基本公式为：</p><script type="math/tex; mode=display">L_{MAE} = \frac{1}{N} \sum_{i=1}^{N}\left | y_{i} - \hat{y}_i \right |</script><p>简单说就是预测值和标签值差值的绝对值平均值，其可视化图如下图，MAE损失的最小值为0（当预测值等于真实值），最大值为无穷大，可以看出随着预测值与真实值绝对误差 $\left | y_{i} - \hat{y}_i \right |$ 的增加，<strong>MAE损失呈线性增长</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/1.jpg" style="zoom:80%;" /></p><h5 id="均方差损失-MSE"><a href="#均方差损失-MSE" class="headerlink" title="均方差损失(MSE)"></a>均方差损失(MSE)</h5><p>均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下：</p><script type="math/tex; mode=display">L_{MSE} = \frac{1}{N} \sum_{i=1}^{N}\left ( y_{i} - \hat{y}_i \right )^2</script><p>损失函数的最小值为 0（当预测等于真实值时），最大值为无穷大。下图是对于真实值 ，不同的预测值 [-1.5, 1.5] 的均方差损失的变化图。横轴是不同的预测值，纵轴是均方差损失，可以看到随着预测与真实值绝对误差  $\left | y_{i} - \hat{y}_i \right |$ 的增加，<strong>均方差损失呈二次方地增长</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/2.jpg" style="zoom:80%;" />/&gt;</p><h5 id="MAE和MSE的区别"><a href="#MAE和MSE的区别" class="headerlink" title="MAE和MSE的区别"></a>MAE和MSE的区别</h5><p><strong>MAE 和 MSE 作为损失函数的主要区别是：MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier 更加健壮，即更加不易受到 outlier 影响。</strong></p><p>MSE 通常比 MAE 可以更快地收敛。当使用梯度下降算法时，MSE 损失的梯度为 $-\hat{y}_i$ ，而 MAE 损失的梯度为  $\mp 1$ ，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的。当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了 MSE 在大部分时候比 MAE 收敛地更快。这个也是 MSE 更为流行的原因.</p><p>MAE 对于 outlier 更加 robust，MAE 和 MSE 损失函数可视化中，由于MAE 损失与绝对误差之间是<strong>线性关系</strong>，MSE 损失与误差是<strong>平方关系</strong>，<strong>当误差非常大的时候，MSE 损失会远远大于 MAE 损失</strong>。因此当数据中出现一个误差非常大的 outlier 时，MSE 会产生一个非常大的损失，对模型的训练会产生较大的影响。</p><h5 id="Huber-Loss"><a href="#Huber-Loss" class="headerlink" title="Huber Loss"></a>Huber Loss</h5><p>MSE损失收敛快但是容易收到 outlier 的影响，MAE对 outlier 更加健壮但是收敛满，Huber Loss则是一种将 MSE 和 MAE结合起来，取二者优点的损失函数，也称为 Smooth Absolute Error Loss，其原理就是在误差接近0的时候使用 MSE，误差较大时使用 MAE，公式为：</p><script type="math/tex; mode=display">L_{h u b e r}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right| \leq \delta} \frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}+\mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right|>\delta}\left(\delta\left|y_{i}-\hat{y}_{i}\right|-\frac{1}{2} \delta^{2}\right)</script><p>其中 $\delta$  是 Huber Loss 的一个超参数， $\delta$ 的值是在 MSE 和 MAE 两个损失连接的位置，上述等式第一项是 MSE 部分。第二项是 MAE 部分，在MAE部分公式为 $\delta\left|y_{i}-\hat{y}_{i}\right|-\frac{1}{2} \delta^{2}$ 是为了保证误差   $\left | y_{i} - \hat{y}_i \right | = \mp \delta$ 时，MAE和MSE的取值是一致的，进而保证 Huber Loss损失连续可导。下图是 $\delta = 1.0$ 时的 Huber Loss，可以看到在区间 $[-\delta,\delta]$ 就是MSE损失，在区间 $(-\infty,\delta)$ 和 $(\delta, +\infty)$ 就是MAE损失：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/3.jpg" style="zoom:80%;" /></p><p>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使<strong>损失函数可导并且梯度更加稳定</strong>；在误差较大时使用 MAE 可以降低 outlier 的影响，使<strong>训练对 outlier 更加健壮</strong>。缺点是需要额外地设置一个 $\delta$ 超参数。</p><h4 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h4><h5 id="交叉熵损失-CrossEntropy-Loss"><a href="#交叉熵损失-CrossEntropy-Loss" class="headerlink" title="交叉熵损失(CrossEntropy Loss)"></a>交叉熵损失(CrossEntropy Loss)</h5><p>交叉熵损失函数是分类中最常用的损失函数，交叉熵是用来度量<strong>两个概率分布的差异性</strong>的，用来衡量模型<strong>学习到的分布和真实分布的差异</strong>。</p><p><strong>在二分类中</strong>，通常使用 Sigmoid 函数将模型的输出压缩到 【0，1】区间内，$\hat{y}_i \in (0,1)$, 用来代表给定输入 $x_i$ ，模型判断为为正类的概率。因此也得到负类的概率，</p><script type="math/tex; mode=display">p(y_i=1 | x) = \hat{y}_i \quad p(y_i=0 | x) = 1-\hat{y}_i</script><script type="math/tex; mode=display">p(y_i | x_i) = (\hat{y})^{y_i}(1-\hat{y}_i)^(1-y_i)</script><p><strong>二分类交叉熵损失（binary entropy loss）</strong>的一般形式：P为正类概率，y为标签。</p><script type="math/tex; mode=display">Loss = -(y\,log\,P + (1-y)log\,(1-P))</script><p><strong>交叉熵公式的原理</strong>：</p><ul><li><p>信息量，信息量表示一条信息消除不确定性的程度，<strong>信息量的大小和事件发生的概率成反比</strong></p><script type="math/tex; mode=display">-log\,p</script></li><li><p>信息熵，信息熵则是在结果出来之前对可能产生的信息量的期望，期望可以理解为所有可能结果的概率乘以该对应的结果。</p><script type="math/tex; mode=display">H(X) = - \sum_{i=1}^n P(X=x_i)\,log\,P(X=x_i)</script><p><strong>信息熵是用来衡量事物不确定性的。信息熵越大（信息量越大，P越小），事物越具不确定性，事物越复杂。</strong></p></li></ul><ul><li><p>相对熵（KL散度），相对熵又称互熵 设 P(x) 和 Q(x) 是取值的两个概率分布，相对熵用来表示两个<strong>概率分布的差异</strong>，当两个随机分布相同时，他们的相对熵为0，当两个随机分布的差别增大时，他们的相对熵也会增大：</p><script type="math/tex; mode=display">D_{K L}(P \| Q)=-\sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)-\left(-\sum_{i=1}^{n} P\left(x_{i}\right) \log P\left(x_{i}\right)\right)=\sum_{i=1}^{n} P\left(x_{i}\right) \log \frac{P\left(x_{i}\right)}{Q\left(x_{i}\right)}</script><ul><li>KL散度不是一个对称量，$D_{K L}(P | Q) \neq D_{K L}(Q | P)$。</li><li>KL散度的值始终大于&gt;=0,当且仅当 $P(X)=Q(x)$ 等号成立。</li></ul></li></ul><p><strong>交叉熵</strong>， KL散度换种写法：</p><script type="math/tex; mode=display">D_{K L}(P \| Q)=-\sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)-\left(-\sum_{i=1}^{n} P\left(x_{i}\right) \log P\left(x_{i}\right)\right) = -H(P(X)) - \sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)</script><p><strong>交叉熵 $H(P,Q)$ 即等于 信息熵+KL散度</strong>：</p><script type="math/tex; mode=display">H(P,Q) = H(P) + D_KL((P \| Q)) = - \sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)</script><p>把 P 看作随机变量的真实分布的话，KL散度左半部分 $ -H(P(X))$ 的信息熵其实是一个固定值，KL散度的大小变化其实是由右半部分交叉熵来决定的，因为右半部分含有近似分布 Q，我们可以把它看作网络或模型的实时输出，把KL散度或者交叉熵看做<strong>真实标签与网络预测结果的差异</strong>，所以<strong>神经网络的目的就是通过训练使近似分布逼近真实分布</strong>。从理论上讲，<strong>优化KL散度与优化交叉熵的效果应该是一样的</strong>。所以我认为，在深度学习中选择优化交叉熵而非KL散度的原因可能是为了减少一些计算量，交叉熵毕竟比KL散度少一项。</p><p><strong>多分类交叉熵损失</strong>，多分类和二分类类似，二分类的标签为1和0，而多分类可以用one-hot编码来表示，公式为：</p><script type="math/tex; mode=display">Loss = -\frac{1}{N} \sum_{i=0}^{N-1}\sum_{k=0}^{K-1}\,y_{i,k}\,log\, p_{i,k}</script><p>其中，$y_{i,k}$ 表示第 i 个样本的真实标签为 k，共有 K 个标签值 N 个样本，$p_{i,k}$ 表示第 i 个样本预测为第 k 个标签值得概率，通过对该损失函数得拟合，也在一定程度上增大了类间距离，在 torch中，<code>torch.nn.functional.cross_entropy</code> 实现了将输出整合到【0，1】概率区间，不需要 softmax 操作。</p><h5 id="Weight-Loss"><a href="#Weight-Loss" class="headerlink" title="Weight Loss"></a>Weight Loss</h5><p>交叉熵Loss可以用在大多数语义分割场景中，但它有一个明显的缺点，那就是对于只用分割前景和背景的时候，当前景像素的数量远远小于背景像素的数量时，即 y=0 的数量远大于 y=1 的数量，损失函数中 y=0 的成分就会占据主导，<strong>使得模型严重偏向背景，导致效果不好。</strong></p><p>由于交叉熵损失会分别评估每个像素的类别预测，然后对所有像素的损失进行平均，因此我们实质上是在对图像中的每个像素进行平等地学习。<strong>如果多个类在图像中的分布不均衡，那么这可能导致训练过程由像素数量多的类所主导，即模型会主要学习数量多的类别样本的特征，并且学习出来的模型会更偏向将像素预测为该类别。</strong></p><p>FCN论文和U-Net论文中针对这个问题，对输出概率分布向量中的每个值进行加权，即希望模型更加关注数量较少的样本，以缓解图像中存在的类别不均衡问题。</p><p>比如对于二分类，正负样本比例为1: 99，此时模型将所有样本都预测为负样本，那么准确率仍有99%这么高，但其实该模型没有任何使用价值。</p><p>为了平衡这个差距，就对正样本和负样本的损失赋予不同的权重，<strong>带权重的二分类损失函数公式</strong>如下：</p><script type="math/tex; mode=display">Loss = -(pos\_weight \times y\,log\,P + (1-y)log\,(1-P))</script><script type="math/tex; mode=display">pos\_weight = \frac{num\_neg}{num\_pos}</script><p>要减少假阴性样本的数量，可以增大 pos_weight；要减少假阳性样本的数量，可以减小 pos_weight。</p><p><strong>带权重的交叉熵Loss</strong>，公式为：</p><script type="math/tex; mode=display">Loss = -\sum_{c=1}^M w_c\,y_c\,log(p_c)</script><p>可以看到只是在交叉熵Loss的基础上为每一个类别添加了一个权重参数，其中 $w_c$ 的计算公式为： $w_c = \frac{N-N_c}{N}$ 其中 N 表示总的像素个数，而 $N_c$  表示 GT 类别为 c 的像素个数。这样相比于原始的交叉熵Loss，在样本数量不均衡的情况下可以获得更好的效果。</p><h5 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h5><p><strong>重点：</strong>需要解决不同类别的<strong>像素数量不均衡</strong>问题，但有时还需要将<strong>像素分为难学习和容易学习这两种样本</strong>。</p><p>容易学习的样本模型可以很轻松地将其预测正确，模型只要将大量容易学习的样本分类正确，loss就可以减小很多，从而导致模型不怎么顾及难学习的样本，所以我们要想办法让模型更加关注难学习的样本。</p><p>何凯明团队在RetinaNet论文中引入了Focal Loss来解决<strong>难易样本数量不平衡</strong>，我们来回顾一下。 我们在计算分类的时候常用的损失——二分类交叉熵的公式如下：</p><script type="math/tex; mode=display">Loss_\mathrm{CE}(p, y)=\left\{\begin{array}{ll}-\log (p) & \text { if } y=1 \\-\log (1-p) & \text { otherwise }\end{array}\right.</script><p>为了解决<strong>正负样本数量不平衡</strong>的问题，我们经常在二元交叉熵损失前面加一个参数 $\alpha$，即：</p><script type="math/tex; mode=display">Loss_\mathrm{CE}(p, y)=\left\{\begin{array}{ll}- \alpha \log (p) & \text { if } y=1 \\- (1-\alpha) \log (1-p) & \text { otherwise }\end{array}\right.</script><p><strong>虽然参数 $\alpha$ 平衡了正负样本的数量，上式针对不同类别的像素数量不均衡提出了改进方法</strong>。但实际上，有时候候选目标都是<strong>易分样本</strong>。这些样本的损失很低，但是由于数量极不平衡，易<strong>分样本的数量相对来讲太多，最终主导了总的损失。</strong></p><p>因此，这篇论文认为<strong>易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本</strong> 。所以Focal Loss横空出世了。一个简单的想法就是只要我们将高置信度样本的损失降低一些就好了吧？ 也即是下面的公式：</p><script type="math/tex; mode=display">Focal\_Loss(p, y)=\left\{\begin{array}{ll}- (1-p)^{\gamma } \log (p) & \text { if } y=1 \\- p^{\gamma } \log (1-p) & \text { otherwise }\end{array}\right.</script><p>其中的 $\gamma$ 通常设置为2。</p><p>举个例子，预测一个正样本，如果预测结果为0.95，这是一个容易学习的样本，有 $(1−0.95)^2=0.0025$ ，损失直接减少为原来的1/400。</p><p>而如果预测结果为0.5，这是一个难学习的样本，有 $(1−0.5)^2=0.25$ ，损失减小为原来的1/4，虽然也在减小，但是相对来说，减小的程度小得多。</p><p><strong>所以通过这种修改，就可以使模型更加专注于学习难学习的样本</strong>。</p><p><strong>而将这个修改和对正负样本不均衡的修改合并在一起，就是大名鼎鼎的 focal loss：</strong>带两个超参数：$\alpha$ 和 $\gamma$</p><script type="math/tex; mode=display">Focal\_Loss(p, y)=\left\{\begin{array}{ll}- \alpha(1-p)^{\gamma } \log (p) & \text { if } y=1 \\- (1-\alpha)p^{\gamma } \log (1-p) & \text { otherwise }\end{array}\right.</script><p><strong>Focal Loss代码：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class FocalLoss(nn.Module):</span><br><span class="line">    def __init__(self, gamma=0, alpha=None, size_average=True):</span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])</span><br><span class="line">        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)</span><br><span class="line">        self.size_average = size_average</span><br><span class="line"></span><br><span class="line">    def forward(self, input, target):</span><br><span class="line">        if input.dim()&gt;2:</span><br><span class="line">            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W =&gt; N,C,H*W</span><br><span class="line">            input = input.transpose(1,2)    # N,C,H*W =&gt; N,H*W,C</span><br><span class="line">            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C =&gt; N*H*W,C</span><br><span class="line">        target = target.view(-1,1)</span><br><span class="line"></span><br><span class="line">        logpt = F.log_softmax(input)</span><br><span class="line">        logpt = logpt.gather(1,target)</span><br><span class="line">        logpt = logpt.view(-1)</span><br><span class="line">        pt = Variable(logpt.data.exp())</span><br><span class="line"></span><br><span class="line">        if self.alpha is not None:</span><br><span class="line">            if self.alpha.type()!=input.data.type():</span><br><span class="line">                self.alpha = self.alpha.type_as(input.data)</span><br><span class="line">            at = self.alpha.gather(0,target.data.view(-1))</span><br><span class="line">            logpt = logpt * Variable(at)</span><br><span class="line"></span><br><span class="line">        loss = -1 * (1-pt)**self.gamma * logpt</span><br><span class="line">        if self.size_average: return loss.mean()</span><br><span class="line">        else: return loss.sum()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Regularization_Summary</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Regularization-Summary/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Regularization-Summary/</url>
      
        <content type="html"><![CDATA[<h3 id="正则化总结"><a href="#正则化总结" class="headerlink" title="正则化总结"></a>正则化总结</h3><h4 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h4><ul><li><p>欠拟合是指模型不能在训练集上获得足够低的误差。</p></li><li><p>过拟合是指训练误差和测试误差之间的差距太大，神经网络对训练数据进行很好的建模但在看到来自同一问题域的新数据时失败的现象。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230329/1.png" style="zoom:50%;" /></p><p><strong>模型容量低，而训练数据复杂度高</strong>时，此时模型由于学习能力不足，无法学习到数据集中的“一般规律”，因而导致训练误差高，表现为欠拟合。</p><p><strong>模型容量高，而训练数据简单</strong>时，此时模型由于学习能力太强，记住所有的训练数据，却没有理解数据背后的规律，对于训练集以外的数据泛化能力差，表现为过拟合。</p></li></ul><h5 id="解决欠拟合"><a href="#解决欠拟合" class="headerlink" title="解决欠拟合"></a>解决欠拟合</h5><ul><li>在模型容量和训练数据复杂度不匹配时，发生了欠拟合现象，常见解决方法有：</li><li><strong>增加新特征</strong>：可以考虑加入特征组合、高次特征等，来增大假设空间。</li><li><strong>增大模型容量</strong>：容量低的模型可能很难拟合训练集。</li><li><strong>减少正则化参数</strong>：正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数。</li></ul><h5 id="解决过拟合"><a href="#解决过拟合" class="headerlink" title="解决过拟合"></a>解决过拟合</h5><p><strong>引起过拟合</strong>的主演原因如下：</p><ul><li><strong>训练数据集问题</strong>。如训练样本不均衡，训练集中正样本偏多，那么去预测负样本肯定不准；训练样本数据少，尤其是当比模型参数数量还少时，更容易发生过拟合；训练数据噪声干扰过大，模型会学习很多的噪声特征等。</li><li><strong>模型过于复杂</strong>。模型参数数量太多，参数取值范围太大，模型已经能够“死记硬背”记下了所有训练数据的信息（记住了不适合于测试集的训练集特性）；模型假设的合理性不存在，也就是假设成立的条件实际并不成立。</li><li><strong>模型训练迭代次数太多</strong>。对数据反复地训练也可能会让模型拟合了训练样本中没有代表性的特征</li></ul><p><strong>解决过拟合问题</strong>，最重要的减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。通常解决过拟合的方法有：</p><ul><li>数据集增强</li><li>控制模型容量大小</li><li>正则化（Regularization）</li><li>丢弃法（Dropout）</li><li>提前终止训练（Early Stopping）</li></ul><h4 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化(Regularization)"></a>正则化(Regularization)</h4><p><strong>正则化其实就是在原来的目标函数的基础上又加了一项非负项</strong>， 并且这个非负项是 w 的函数。 这样的话target不变的基础上得让这个loss变得小一点， 相当于对其产生了一种约束。也可以认为<strong>正则化是减小权值方差的一种策略，减小权值的取值范围，从而使得模型求解空间变小，不会拟合出复杂的曲线（函数）</strong>.</p><p><strong>正则化主要用于避免过拟合的产生和减少网络误差，通过将L1范数或者L2范数加入到损失函数中，在损失函数中增加一个惩罚项，在进行反向传播]时就相当于在原来的权重上乘上了一个小于1的系数，使得权重减小，从而使得模型的权重的取值范围减小，即模型的求解空间变小，不会拟合出复杂的曲线（函数），从而达到抑制过拟合的目的，从而限制模型参数的数量以及参数的大小。</strong></p><ul><li>通常使用 L2 正则，称为权重衰减（weight decay），一般在优化器中使用，取值一般为0.01,0.001,0.0001;</li><li>L2 正则：容易计算， 可导， 适合基于梯度的方法，对异常值非常敏感；<strong>执行 L2 正则化鼓励权重值趋向于零</strong>（但不完全为零）。</li><li>L1 正则：<strong>L1模型可以将 一些权值缩小到零</strong>（稀疏，由于它可以提供稀疏的解决方案， 因此通常是建模特征数量巨大时的首选模型）;<strong>执行 L1 正则化鼓励权重值为0.</strong></li><li>直观地说，较小的权重会减少隐藏神经元的影响。在那种情况下，那些隐藏的神经元变得可以忽略不计，神经网络的整体复杂性得到降低。不太复杂的模型通常会避免数据中的建模噪声，因此不会出现过度拟合。</li></ul><h5 id="正则化之L1和L2"><a href="#正则化之L1和L2" class="headerlink" title="正则化之L1和L2"></a>正则化之L1和L2</h5><p>误差 = 偏差 + 方差 + 噪声</p><ul><li><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度， 即刻画了学习算法本身的拟合能力</li><li><strong>方差</strong>度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li><li><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界。</li></ul><p><strong>正则化就是在目标损失函数上面添加一个正则项：</strong></p><script type="math/tex; mode=display">L1 \;  Regularization：\quad \mathcal{L}_{L1} = \mathcal{L} + \alpha ||w||_1 = \mathcal{L} + \frac{\alpha}{n}\sum_{i=1}^{n}|w_i|</script><script type="math/tex; mode=display">L2 \;  Regularization：\quad \mathcal{L}_{L2} = \mathcal{L} + \alpha ||w||_2^2 = \mathcal{L} + \frac{\alpha}{n}\sum_{i=1}^{n}w_i^2</script><p>其中 $\alpha$ 是正则化参数，决定了我们对模型进行正则化的程度（惩罚力度），$w$ 是权重。加上正则项，就是希望我们的<strong>代价函数小</strong>，同时也希望我们这里的 <strong>$w_i$ 小，这就是说明每个样本的权重都很小</strong>，这样模型就不会太多的关注某种类型的样本， 模型参数也不会太复杂，有利于缓解过拟合现象。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230329/2.png" style="zoom:67%;" /></p><p>等高线图中：</p><ul><li>彩色的圆圈就是损失函数值 $\mathcal{L}$ 等高线，也就是说同一条线上的损失函数值是相等的；</li><li>左边菱形代表L1的等高线；右边圆形代表L2的等高线;</li><li>假设二维权重下的示意图 $(w_1,w_2)$;</li></ul><p>分析：</p><ul><li><p>损失函数值 $\mathcal{L}$ 等高线和正则项等高线，分别代表了两个优化问题。对原始的损失函数中添加了正则项之后，优化问题就变成了两个子优化问题的博弈。</p></li><li><p>左边是涉及 L1 正则项的，其中 A,B,C点产生的损失值是相等的，黑色的菱形(坐标中心) 表示L1正则项的一个等高线，假设$w_1 + w_2 = r,假设r=1$ ，那么菱形框任意一个点产生的正则项值都是1。我们现在考虑A, B,C三点的目标函数，他们的损失值 $\mathcal{L}$ 是相等的，那么这三个点哪个点的Regularization最小呢？ C点的正则项是1， 而我们会发现A点和B点的正则项都比C大（其实A,B,C这一条损失值 $\mathcal{L}$ 等高线上就是C的正则项最小）， 所以C点的目标函数最小。所以我们如果在L1的情况下找个最优解，既能损失 $\mathcal{L}$ 最小，又能权重最小，那么往往这个最优解就发生在坐标轴上，也就是上面的C点。 这样一个神奇的现象发生了，w1竟然等于0， 就说明参数解有0 的出现了， $w_1$ 就消失了。所以L1正则项一般会产生稀疏的解，也就是有项权重解为 0 。这是因为加上L1之后，我们参数的解往往会发生在坐标轴上导致某些参数的值为0;</p></li><li>右边是涉及 L2 正则项的，其中A’, B’, C’点产生的损失值也是相等的，黑色的圆形(坐标中心) 表示L2正则项的一个等高线，假设$w_1^2 + w_2^2 = r,假设r=1$ 。和上面的分析一样，，如果我们在A’, B’, C’点确定最优解的话，依然是C’点， 因为它在损失值  $\mathcal{L}$ 相等的情况下正则最小。但是我们发现L2正则下不过出现某个参数为0的情况，而是w1和w2都比较小。所以L2正则项的最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化 $|w|$ 时，就会使每一项趋近于0。</li><li>对于 L1 正则化中，在正则项等高线和损失值等高线相切/相交始终都是在坐标轴上的；最优点会保持在 L1 等高线的断点处，依然在坐标轴上，故某个权重值会变为0。</li><li>而 L2 的相切点则只能无限接近坐标轴，惩罚力度再大，都到不了0。</li><li>惩罚力度 $\alpha$ 相当于 L1中的菱形在坐标中心处大小 $w_1 + w_2 = \alpha$ ；相当于 L2 中圆形在坐标中心处的大小  $w_1^2 + w_2^2 = \alpha$ 。当损失值  $\mathcal{L}$ 和正则项之和最小时，上述的博弈取得平衡。而此时平衡点一定是相切点/端点。相切点的具体位置，取决于正则项的惩罚力度，也就是公式里的α。每一个平衡点，对应着一个α的设置。<strong>当惩罚力度大时，相切/相交点往靠近坐标轴的方向移动</strong>，而<strong>惩罚力度小时，相切/相交往远离坐标轴的方向移动</strong>。</li></ul><p><strong>L1正则化的特点：</strong></p><ul><li>不容易计算， 在零点连续但不可导， 需要分段求导</li><li>L1模型可以将 一些权值缩小到零（稀疏）</li><li>执行隐式变量选择。 这意味着一些变量值对结果的影响降为0， 就像删除它们一样</li><li>其中一些预测因子对应较大的权值， 而其余的（几乎归零）</li><li>由于它可以提供稀疏的解决方案， 因此通常是建模特征数量巨大时的首选模型</li><li>它任意选择高度相关特征中的任何一个， 并将其余特征对应的系数减少到0</li><li>L1范数对于异常值更具提抗力</li></ul><p><strong>L2正则化的特点：</strong></p><ul><li>容易计算， 可导， 适合基于梯度的方法</li><li>将一些权值缩小到接近0</li><li>相关的预测特征对应的系数值相似</li><li>当特征数量巨大时， 计算量会比较大</li><li>对于有相关特征存在的情况， <strong>它会包含所有这些相关的特征</strong>， 但是相关特征的权值分布取决于相关性。</li><li><strong>对异常值非常敏感</strong></li><li>相对于L1正则会更加准确</li></ul><p>为什么增加<strong>正则项后会使得权重变小，缩小了模型的权重的取值范围</strong>？在 pytorch 中 L2正则项 称为 <strong>权值衰减(weight decay)</strong>，为啥叫衰减？怎么实现衰减的？</p><ul><li><p>模型参数的更新公式：</p><script type="math/tex; mode=display">w_{i+1} = w_i - \varepsilon  \frac{ \partial Loss}{ \partial w_i}</script></li><li><p>在损失函数上添加了一个 L2 正则项后 $Loss_{new} = Loss + \frac{\alpha}{2}\sum_{i=1}^{n}w_i^2$ ,那么参数的更新变为：</p><script type="math/tex; mode=display">w_{i+1} = w_i - \varepsilon  \frac{ \partial Loss_{new}}{ \partial w_i} = w_i - \varepsilon (\frac{ \partial Loss}{ \partial w_i} + \alpha * w_1 ) = w_i - \varepsilon \frac{ \partial Loss}{ \partial w_i} - \varepsilon * \alpha * w_i</script><p>唯一的区别是，通过添加正则化项，我们从当前权重（等式中的第一项）中引入了额外的减法。</p><p>换句话说，与损失函数的梯度无关，每次执行更新时，我们都会使权重变小一点。也可以看成参数 $w_i$ 本身发生一个衰减即使移项 $w_i(1-\varepsilon <em> \alpha)，其中 \varepsilon </em> \alpha \in(0,1)$。</p></li><li><p>pytorch 中使用 L2 就是在优化器中（例：<code>torch.optim.SGD</code>）指定 weight_decay 这个参数即可。 </p></li></ul><h5 id="正则化之Dropout"><a href="#正则化之Dropout" class="headerlink" title="正则化之Dropout"></a>正则化之Dropout</h5><p><strong>Dropout叫做随机失活。就是给出一个概率(随机)，让某个神经元的权重为0(失活)，dropout 意味着在以某种概率P进行训练期间，神经网络的神经元在训练期间被关闭</strong>。一般是应用到<strong>全连接层（nn.Linear）</strong>，而且只在训练过程中使用，在推理预测是不使用的。所以一般在pytorch中设置 <code>model.train,model.eval</code> 来限制Dropout和BN。</p><p><strong>为什么dropout可以抑制过拟合呢</strong>？dropout通过随机失活神经元，使得模型参数减少，网络变得简单，不会拟合出较为复杂的曲线，事实上也就相当于正则。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230329/3.jpg" alt=""></p><p>就是每一层，让某些神经元不起作用，这样就就相当于把网络进行简化了(左边和右边可以对比），我们有时候之所以会出现过拟合现象，就是因为我们的网络太复杂了，参数太多了，并且我们后面层的网络也可能太过于依赖前层的某个神经元，加入Dropout之后， 首先网络会变得简单，减少一些参数，并且由于不知道浅层的哪些神经元会失活，导致后面的网络不敢放太多的权重在前层的某个神经元，这样就减轻了一个过渡依赖的现象， 对特征少了依赖， 从而有利于缓解过拟合。</p><p>关于Dropout还有一个注意的问题，就是<strong>数据的尺度变化</strong>。 这个是什么意思呢？ 我们用Dropout的时候是这样用的： <strong>只在训练的时候开启Dropout，而测试的时候是不用Dropout的，也就是说模型训练的时候会随机失活一部分神经元， 而测试的时候我们用所有的神经元，</strong>那么这时候就会出现这个数据尺度的问题， <strong>所以测试的时候，所有权重都乘以1-drop_prob</strong>， 以保证训练和测试时尺度变化一致，drop_prob是我们的随机失活概率。</p><p>pytorch 中的 <code>torch.nn.Dropout(p=0.5, inplace=False)</code>：p就是随机失活概率。第一点就是Dropout加的时候注意放置的位置，第二点就是由于Dropout操作；模型训练和测试是不一样的，上面我们说了，训练的时候采用Dropout而测试的时候不用Dropout， 那么我们在迭代的时候，就得告诉网络目前是什么状态，如果要测试，就得先用<code>.eval()</code>函数告诉网络一下子，训练的时候就用<code>.train()</code>函数告诉网络一下子。<strong>Pytorch在实现Dropout的时候， 是权重乘以 $\frac{1}{1-p}$  的，也就是除以1-p, 这样就不用再测试的时候权重乘以1-p了， 也没有改变原来数据的尺度.</strong></p><h5 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h5><ul><li><p>L1和L2 哪个抑制过拟合的效果好?</p><p>L1正则化和L2正则化在抑制过拟合方面有不同的效果，具体哪个效果更好取决于<strong>问题的性质、数据集的大小、网络架构以及其他因素</strong>。<strong>一般来说，L2会比L1好，L2平滑权重，L1是稀疏权重，有的权重直接为0</strong>。实际中还是得交叉验证来评估！</p><p><strong>L1正则化：</strong></p><ul><li>L1正则化倾向于使权重稀疏，即将某些权重归零。这有助于进行特征选择，从而排除对模型不重要的特征。</li><li>当数据集中存在大量不相关的特征时，L1正则化可能会更有效，因为它可以显著减少不相关特征的影响。</li><li>L1正则化对异常值更敏感，可能会将异常值的权重置零，从而降低模型对异常值的拟合。</li></ul><p><strong>L2正则化：</strong></p><ul><li>L2正则化通过限制权重的大小来避免过度拟合，但它不会强制使权重为零，而是使权重接近零。</li><li>L2正则化可以平滑权重，降低权重之间的差异，有助于提高模型的稳定性。</li><li>当数据集的特征之间存在共线性时，L2正则化可能会更有效，因为它可以减少共线性的影响。</li></ul></li></ul><ul><li><p>L1和L2 哪个收敛更快？</p><p>A：在正则化对收敛速度的影响方面，通常情况下，L2正则化（也称为权重衰减）可能会比L1正则化更有利于快速收敛。这是因为L2正则化将权重的平方项添加到损失函数中，这可以使权重的更新变得更加平稳，有助于避免参数变化过大，从而导致更快的收敛。</p><p>L1正则化在优化中可能导致稀疏权重，其中某些权重变为零。这可能会导致一些梯度在某些情况下突然变为零，从而减缓收敛速度。然而，这也取决于初始权重、学习率和其他优化算法的细节。</p></li></ul><ul><li><p>Dropout 应用在深层好还是浅层好？</p><p>Dropout是一种用于训练神经网络的正则化技术，旨在减少过拟合问题。它通过在每次训练迭代中随机地将一部分神经元的输出置为零，从而强制网络不依赖于特定神经元的存在，从而提高泛化能力。</p><p>关于将Dropout应用于深层还是浅层网络，有一些一般性的观点：</p><ol><li><strong>深层网络：</strong> 在深层网络中使用Dropout通常是有益的，因为深层网络更容易出现过拟合问题。由于深层网络具有更多的参数和表示能力，它们可以更好地拟合训练数据，但同时也更容易记住噪声或训练数据的细节，导致泛化性能下降。在这种情况下，Dropout可以帮助减少神经元之间的过度依赖，从而提高网络的泛化能力。</li><li><strong>浅层网络：</strong> 浅层网络相对于深层网络具有较少的参数和复杂性，因此它们通常具有更好的泛化能力，而且过拟合的风险较低。在一些情况下，添加Dropout可能不如在深层网络中明显有效，因为浅层网络已经具有一定的简化能力，能够更好地应对泛化问题。</li></ol><p>总之，Dropout的效果通常在网络的复杂性和数据集的大小之间变化。在训练数据较少、网络较深或容易过拟合的情况下，Dropout通常会带来显著的改善。然而，如果网络相对浅并且数据集足够大，可能不需要过多地使用Dropout。</p></li></ul><p>Reference:  <a href="https://blog.csdn.net/wuzhongqiang/article/details/105612578">正则化</a> ; <a href="https://zhuanlan.zhihu.com/p/376000306">L1和L2的正确姿势</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Regularization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metric_in_Semantic_Segmentation</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Metric-in-Semantic-Segmentation/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Metric-in-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<h3 id="Metric-in-Semantic-Segmentation"><a href="#Metric-in-Semantic-Segmentation" class="headerlink" title="Metric in Semantic Segmentation"></a>Metric in Semantic Segmentation</h3><h4 id="Dice-Coefficient"><a href="#Dice-Coefficient" class="headerlink" title="Dice Coefficient"></a>Dice Coefficient</h4><ul><li><p>定义：Dice系数，是一种集合相似度度量函数，通常用于计算两个样本点的相似度（值范围为[0, 1]）。用于分割问题，分割最好时为1，最差为0。（可解决样本不均衡问题）</p></li><li><p>计算公式：</p><script type="math/tex; mode=display">Dice = \frac{2 \times \left |  X \cap Y \right |}{\left | X \right | + \left | Y \right | } =            \frac{2 \times 预测正确的结果 }{ 真实结果 + 预测结果 } \qquad\qquad X是标签；Y是预测值</script><p>其中 $\left |  X \cap Y \right |$ 是表示 X 和 Y 的交集<strong>（逐像素相乘后相加）</strong>，$\left | X \right | $ 和 $ \left | Y \right |$ 表示其元素的个数<strong>（逐像素（Or平方）相加）</strong>。在计算的时候一般会加一个smooth，防止分母出现0</p></li><li><p>Dice loss = 1 - Dice</p></li><li><p>代码实现1（简单）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># H*W，只针对二维，多类多batch分开计算</span><br><span class="line">def dice_coeff(pred, target):</span><br><span class="line">    smooth = 1.</span><br><span class="line">    num = pred.size(0)</span><br><span class="line">    m1 = pred.view(num, -1)  # Flatten </span><br><span class="line">    m2 = target.view(num, -1)  # Flatten</span><br><span class="line">    intersection = (m1 * m2).sum() # 计算交集</span><br><span class="line">    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)</span><br></pre></td></tr></table></figure></li><li><p>代码实现2（标准）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># H*W</span><br><span class="line">def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):</span><br><span class="line">    # Average of Dice coefficient for all batches, or for a single mask</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    if input.dim() == 2 and reduce_batch_first:</span><br><span class="line">        raise ValueError(f&#x27;Dice: asked to reduce batch </span><br><span class="line">        but got tensor without batch dimension (shape &#123;input.shape&#125;)&#x27;)</span><br><span class="line"></span><br><span class="line">    if input.dim() == 2 or reduce_batch_first:</span><br><span class="line">    # torch.dot 点乘，对应元素相乘后相加，一个值，分子交集</span><br><span class="line">        inter = torch.dot(input.reshape(-1), target.reshape(-1))</span><br><span class="line">        # 分母，并集</span><br><span class="line">        sets_sum = torch.sum(input) + torch.sum(target)</span><br><span class="line">        if sets_sum.item() == 0:</span><br><span class="line">            sets_sum = 2 * inter</span><br><span class="line">        return (2 * inter + epsilon) / (sets_sum + epsilon)</span><br><span class="line">    else:</span><br><span class="line">        # compute and average metric for each batch element</span><br><span class="line">        dice = 0</span><br><span class="line">        for i in range(input.shape[0]):</span><br><span class="line">            dice += dice_coeff(input[i, ...], target[i, ...])</span><br><span class="line">        return dice / input.shape[0]</span><br><span class="line"></span><br><span class="line">def multiclass_dice_coeff(input: Tensor, target: Tensor,</span><br><span class="line">  reduce_batch_first: bool = False, epsilon=1e-6):</span><br><span class="line">    # Average of Dice coefficient for all classes</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    dice = 0</span><br><span class="line">    for channel in range(input.shape[1]):</span><br><span class="line">        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], </span><br><span class="line">           reduce_batch_first, epsilon)</span><br><span class="line"></span><br><span class="line">    return dice / input.shape[1]</span><br><span class="line"></span><br><span class="line">def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):</span><br><span class="line"># 在调用的时候，groud-truth若是多类别，需要进行one-hot编码</span><br><span class="line"># 【B,C,H,W】target and input</span><br><span class="line">    # Dice loss (objective to minimize) between 0 and 1</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    fn = multiclass_dice_coeff if multiclass else dice_coeff</span><br><span class="line">  return 1 - fn(input, target, reduce_batch_first=True)</span><br></pre></td></tr></table></figure></li></ul><h4 id="Mean-Intersection-over-Union"><a href="#Mean-Intersection-over-Union" class="headerlink" title="Mean Intersection over Union"></a>Mean Intersection over Union</h4><ul><li><p>mIoU：Mean Intersection over Union，均交并比，为语义分割的标准度量。其计算所有<strong>类别交集和并集之比</strong>的平均值.</p></li><li><p>先验提示：</p><ul><li>TP(真正): 预测正确, 预测结果是正类, 真实是正类</li><li>FP(假正): 预测错误, 预测结果是正类, 真实是负类</li><li>FN(假负): 预测错误, 预测结果是负类, 真实是正类</li><li>TN(真负): 预测正确, 预测结果是负类, 真实是负类  # 跟该类别无关,所以不包含在并集中</li></ul></li><li><p>mIoU的计算：直观理解，计算两圆交集（橙色部分）与两圆并集（红色+橙色+黄色）之间的比例，理想情况下两圆重合，比例为1</p><script type="math/tex; mode=display">mIoU = \frac{1}{k+1} \sum_{i=0}^{k} \frac{TP}{FN+FP+TP}</script><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230326/1.png" style="zoom: 50%;" /></p></li><li><p>计算：</p><ul><li><p>先求混淆矩阵：K 分类问题就会生成 K * K 的混淆矩阵。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230707/1.jpg" style="zoom: 50%;" /></p><p>假设有150个样本数据，预测类别1，2，3各有50 个，分类结束的混淆矩阵为上：</p><p>每一行之和表示该类别的真实样本数量，每一列之和表示被预测为该类别的样本数量</p></li></ul><p>第一行说明有43个属于第一类别的样本被正确预测为了第一类，有两个属于第一类别的样本被错误预测成为了第二类。</p><ul><li><p>再求 mIoU：</p><p><strong>mIoU = 混淆矩阵对角线的值  / (混淆矩阵的每一行再加上每一列，最后减去对角线上的值)</strong></p><p>混淆矩阵:  对角线上的值的和代表分类正确的像素点个数(preb与target一致),对角线之外的其他值的和代表所有分类错误的像素的个数。</p><p>混淆矩阵矩阵中 (x, y) 位置的元素代表该张图片中真实类别为 x ,被预测为 y 的像素个数。</p></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 1先求混淆矩阵</span><br><span class="line">def _fast_hist(self, label_pred, label_true):</span><br><span class="line">        # 找出标签中需要计算的类别,去掉了背景</span><br><span class="line">        mask = (label_true &gt;= 0) &amp; (label_true &lt; self.num_classes)</span><br><span class="line">        # np.bincount计算了从0到n**2-1这n**2个数中每个数出现的次数，返回值形状(n, n)</span><br><span class="line">        hist = np.bincount(self.num_classes * label_true[mask].astype(int) +</span><br><span class="line">                            label_pred[mask], minlength=self.num_classes ** 2)</span><br><span class="line">                            .reshape(self.num_classes,self.num_classes)</span><br><span class="line">        return hist</span><br><span class="line"># 2根据混淆矩阵求mIoU</span><br><span class="line"># 输入：预测值和真实值 [batch_size, H, W] </span><br><span class="line"># 语义分割的任务是为每个像素点分配一个label</span><br><span class="line">def ev aluate(self, predictions, gts):</span><br><span class="line">    for lp, lt in zip(predictions, gts):</span><br><span class="line">    assert len(lp.flatten()) == len(lt.flatten())</span><br><span class="line">    self.hist += self._fast_hist(lp.flatten(), lt.flatten())</span><br><span class="line">    # miou</span><br><span class="line">    # 每个类别 iou</span><br><span class="line">    iou = np.diag(self.hist) / (self.hist.sum(axis=1) + self.hist.sum(axis=0) np.diag(self.hist))</span><br><span class="line">    # 取平均值</span><br><span class="line">    miou = np.nanmean(iou) </span><br></pre></td></tr></table></figure></li></ul></li><li><p>常用求mIoU代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># 输入 pred，target 【B,H,W】</span><br><span class="line"># 第一种方式 比较合适我理解</span><br><span class="line">def iou_mean(pred, target, n_classes = 1):</span><br><span class="line">    # n_classes ：the number of classes in your dataset,not including background</span><br><span class="line">    # for mask and ground-truth label, not probability map</span><br><span class="line">    ious = [] #每个类别的 IoU</span><br><span class="line">    iousSum = 0</span><br><span class="line">    pred = pred.view(-1)</span><br><span class="line">    target = target.view(-1)</span><br><span class="line">    # Ignore IoU for background class (&quot;0&quot;)</span><br><span class="line">    for cls in range(1, n_classes+1):  </span><br><span class="line">      pred_inds = pred == cls</span><br><span class="line">        target_inds = target == cls</span><br><span class="line">        # Cast to long to prevent overflows</span><br><span class="line">        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()  </span><br><span class="line">        union = pred_inds.long().sum().data.cpu().item() </span><br><span class="line">        + target_inds.long().sum().data.cpu().item() - intersection</span><br><span class="line">        if union == 0:</span><br><span class="line">          ious.append(float(&#x27;nan&#x27;))  # If there is no ground truth, do not include in evaluation</span><br><span class="line">        else:</span><br><span class="line">          ious.append (float(intersection) / float(max(union, 1)))</span><br><span class="line">          iousSum += float(intersection) / float(max(union, 1))</span><br><span class="line">       </span><br><span class="line">      return iousSum/n_classes  # mIoU</span><br><span class="line">      </span><br><span class="line"># 第二种方式</span><br><span class="line"># &#x27;K&#x27; classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.</span><br><span class="line">def intersectionAndUnion(output, target, K, ignore_index=255):</span><br><span class="line">    assert output.ndim in [1, 2, 3]</span><br><span class="line">    assert output.shape == target.shape</span><br><span class="line">    output = output.reshape(output.size).copy()</span><br><span class="line">    target = target.reshape(target.size)</span><br><span class="line">    output[np.where(target == ignore_index)[0]] = ignore_index</span><br><span class="line">    intersection = output[np.where(output == target)[0]]</span><br><span class="line">    area_intersection, _ = np.histogram(intersection, bins=np.arange(K + 1))</span><br><span class="line">    area_output, _ = np.histogram(output, bins=np.arange(K + 1))</span><br><span class="line">    area_target, _ = np.histogram(target, bins=np.arange(K + 1))</span><br><span class="line">    area_union = area_output + area_target - area_intersection</span><br><span class="line">    </span><br><span class="line">    ious = area_intersection / area_union+epsilon  # 是一个array，代表每个类别的IoU</span><br><span class="line">    mIoU = np.nanmean(ious)  # mIoU</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Dice和IoU的联系：</p><script type="math/tex; mode=display">IoU = \frac{TP}{FN+FP+TP}</script><script type="math/tex; mode=display">Dice = \frac{2 \times \left |  X \cap Y \right |}{\left | X \right | + \left | Y \right | }</script><p>其中在 Dice 中 $\left |  X \cap Y \right |$ 就是 TP，$\left | X \right |$ 假设是ground-truth的话就是 FN+TP，$\left | Y \right |$ 假设是预测的 mask的话就是 TP+FP：</p><script type="math/tex; mode=display">Dice = \frac{2 \times TP}{TP+FN+TP+FP}</script><p>得到：</p><script type="math/tex; mode=display">IoU = \frac{Dice}{2-Dice}</script><p>根据在【0，1】值域中的函数图像，可以发现：</p><ul><li>IoU和Dice同时为0，同时为1；这很好理解，就是全预测正确和全部预测错误</li><li>在相同的预测情况下，可以发现Dice给出的评价会比IoU高一些</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Metric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Normalized_Summary</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Normalized-Summary/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Normalized-Summary/</url>
      
        <content type="html"><![CDATA[<h3 id="归一化总结-Normalization"><a href="#归一化总结-Normalization" class="headerlink" title="归一化总结(Normalization)"></a>归一化总结(Normalization)</h3><p>神经网络的学习的本质就是学习数据的分布，使模型收敛，得到学习数据的特性。如果没有对数据进行<strong>归一化处理</strong>，那么<strong>每一批次训练的数据的分布就有可能不一样</strong>。从大的方面来讲，神经网络需要在多个分布中找到一个合适的平衡点；从小的方面来说，由于每层网络的输入数据在不断的变化，这会导致不容易找到合适的平衡点，最终使得构建的神经网络模型不容易收敛。当然，如果只是对输入数据做归一化，这样只能保证数据在输入层是一致的，并不能保证每层网络的输入数据分布是一致的，所以在神经网络模型的<strong>中间层也需要加入归一化处理</strong>。利用随机梯度下降更新参数时，每次参数更新都会导致网络中间每一层的输入的分布发生改变。<strong>越深的层，其输入分布会改变的越明显</strong>。</p><p>内部协变量偏移(<strong>Internal Covariate Shift</strong>):也就是<strong>在训练过程中，隐层的输入分布老是变来变去</strong>， 每一层的参数在更新过程中，会改变下一层输入的分布，神经网络层数越多，变现的越明显。为了解决内部协变量偏移问题，这就要使得每一个神经网络层的输入的分布在训练过程保持一致。</p><h4 id="批量归一化-Batch-Normalization-BN"><a href="#批量归一化-Batch-Normalization-BN" class="headerlink" title="批量归一化(Batch Normalization, BN)"></a>批量归一化(Batch Normalization, BN)</h4><h5 id="为什么需要BN"><a href="#为什么需要BN" class="headerlink" title="为什么需要BN?"></a>为什么需要BN?</h5><p>在深层网络中的训练中，由于反向传播算法，<strong>模型的参数</strong>在发生指数型变化(因为是<strong>链式传播</strong>)，从而导致每一层的输入分布会发生剧烈变化，这就会引起两个问题：</p><ul><li><p>网络需要不断调整来适应输入数据分布的变化，导致网络学习速度的降低</p></li><li><p>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度，训练不稳定。</p></li><li><p>什么是梯度饱和区？</p><p>当我们在神经网络中采用饱和激活函数（saturated activation function）时，例如sigmoid，tanh激活函数，很容易使得模型训练陷入梯度饱和区，此时梯度会变得很小接近于0，从而导致网络收敛很慢。有两种解决方法：</p><ol><li><p>线性整流函数ReLU可以在一定程度上解决训练进入梯度饱和区的问题。</p></li><li><p>我们可以让激活函数的输入分布保持在一个稳定的状态来尽可能避免他们陷入梯度饱和区，这就是BN的想法。</p></li></ol></li></ul><h5 id="解决上述问题"><a href="#解决上述问题" class="headerlink" title="解决上述问题"></a>解决上述问题</h5><ul><li>固定网络每一层<strong>输入值的分布</strong>来缓解这两个问题。比如归一化到标准正态分布。</li></ul><h5 id="BN基本原理和公式"><a href="#BN基本原理和公式" class="headerlink" title="BN基本原理和公式"></a>BN基本原理和公式</h5><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/1.png" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/2.png" style="zoom:80%;" /></p><ul><li>$\gamma $ 和 $\beta $ : 是两个可训练参数，主要是在一定程度上恢复数据本身的表达能力，对规范化后的数据进行线性处理。</li></ul><h5 id="BN的计算"><a href="#BN的计算" class="headerlink" title="BN的计算"></a>BN的计算</h5><p>选择 torch.nn.BatchNorm2d为例子，给定特征图 【N, H, W, C】，其中N是batch_size, HW特征图的宽高，C是通道数，那么上面公式的B就是下图【N,C,HW】中的蓝色部分：保留通道 C 计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/3.jpg" style="zoom:80%;" /></p><ul><li><p>也就是说 <strong>BN是对不同样本里面的同一个特征通道进行归一化处理，逐特征维度归一化</strong>，可训练参数 $\gamma $ 和 $\beta $  的维度是 C。保留通道C计算均值和方差，故有2C个可训练参数（C个 $\gamma $ 和C个 $\beta $  ），2C个不可训练参数（均值mean和方差var）。</p></li><li><p><strong>BN训练时的均值和方差</strong>：<strong>该批次</strong>内数据相应维度的均值与方差</p></li><li><p><strong>BN测试时的均值和方差</strong>：<strong>基于所有批次</strong>的期望(无偏估计)计算所得，训练阶段根据 mini-batch 的数据计算均值和方差，并使用滑动平均法计算<strong>全局均值和方差</strong>；推理阶段使用训练阶段计算的全局均值和方差参与计算。</p></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">def BatchNorm(x, gamma, beta, eps=1e-5): </span><br><span class="line">    # x: input shape [N, C, H, W] </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    mean = torch.mean(input=x, dim=[0,2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[0,2,3], keepdim=True) </span><br><span class="line">    # mean, var shape : [1, C, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"></span><br><span class="line"># numpy版本 更对(两个可学习的参数)</span><br><span class="line">def batch_normalize2d(x, alpha=1.0, beta=0.0, eps=1e-5):</span><br><span class="line"># x: input shape [N, C, H, W] </span><br><span class="line">    mean = np.mean(x, axis=(0, 2, 3), keepdims=True)  # [1, C, 1, 1]</span><br><span class="line">    variance = np.var(x, axis=(0, 2, 3), keepdims=True)  # [1, C, 1, 1]</span><br><span class="line">    alpha = np.zeros_like(mean) + alpha #[1,C,1,1]</span><br><span class="line">    beta = np.zeros_like(mean) + beta #[1,C,1,1]</span><br><span class="line">    x = (data - mean) / np.sqrt(variance + eps) </span><br><span class="line">    return x * alpha + beta</span><br></pre></td></tr></table></figure></li></ul><h5 id="BN的优点缺点"><a href="#BN的优点缺点" class="headerlink" title="BN的优点缺点"></a>BN的优点缺点</h5><ul><li><p>优点：加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失，同时起到一定的正则化作用。</p><ul><li>BN将每层的数据进行标准化，并通过可学习参数 $\gamma $ 和 $\beta $  两个学习参数来调整这种分布。使得网络中<strong>每层输入数据的分布相对稳定，加速模型的训练速度</strong>，解决“internal covariate shift”的问题。</li><li>允许网络使用饱和性激活函数(sigmoid,tanh)，<strong>缓解了梯度消失的问题</strong>，<strong>避免了梯度弥散和爆炸</strong>。BN可以控制数据的分布范围，在遇到sigmoid或者tanh等激活函数时，不会使数据落在饱和区域导致梯度弥散。并且BN可以避免ReLU激活函数数据死亡的问题。</li><li>降低权重初始化的困难，在深度网络中，网络的最终训练效果也受到初始化的影响，初始化决定了神经网络最终会收敛到哪个局部最小值中，具有随机性。通过BN来标准化分布，<strong>可以降低初始化权重的影响</strong>。</li><li>因为不同的mini_batch均值和方差都有所不同，这就<strong>为网络的学习过程增加了随机噪音</strong>，与dropout随机关闭神经元给网络带来的噪音类似，一定程度上起到了正则化的作用。<strong>在正则化方面，一般全连接层用dropout，卷积层用BN。</strong></li><li>BN中的  $\gamma $ 和 $\beta $ 的作用：<ul><li>保证了模型的capacity，意思就是，γ和β作为调整参数可以调整被BN刻意改变过后的输入，即能够保证能够还原成原始的输入分布。BN对每一层输入分布的调整有可能改变某层原来的输入，当然有了这两个参数，经过调整也可以不发生改变，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。</li><li>适应激活函数，如果是sigmoid函数，那么BN后的分布在0-1之间，由于sigmoid在接近0的地方趋于线性，非线性表达能力可能会降低，因此通过γ和β可以自动调整输入分布，使得非线性表达能力增强。</li><li>如果激活函数为ReLU，那么意味着将有一半的激活函数无法使用，那么通过β可以进行调整参与激活的数据的比例，防止dead-Relu问题。</li></ul></li></ul></li><li><p>缺点</p><ul><li><p>BN特别依赖于大的batch_size，而由于显卡等硬件限制，我们大多数batch_size都设置的较小，性能会急剧下降。</p></li><li><p>对于序列化数据的网络不太适用，尤其是序列样本长度不同时。如RNN，LSTM。</p></li></ul></li></ul><h4 id="层归一化-Layer-Normalization"><a href="#层归一化-Layer-Normalization" class="headerlink" title="层归一化(Layer Normalization)"></a>层归一化(Layer Normalization)</h4><p>如果一个神经元的净输入分布在神经网络中是动态变化的，比如循环神经网络，那么无法应用批归一化操作。</p><p>层归一化和批归一化不同的是，<strong>层归一化是对一个中间层的所有神经元进行归一化</strong>。</p><p><strong>注意</strong>：LayerNorm的均值和方差是根据单个数据计算的，所以<strong>不需要计算全局均值和全局方差</strong>。类似BN的计算公式，只不过是在计算均值和方差是在不同的维度上进行的。</p><h5 id="LN基本原理和公式"><a href="#LN基本原理和公式" class="headerlink" title="LN基本原理和公式"></a>LN基本原理和公式</h5><p>基本原理和公式与 BN的差不多，只不过是计算均值和方差在不一样的维度罢了。</p><h5 id="LN的计算"><a href="#LN的计算" class="headerlink" title="LN的计算"></a>LN的计算</h5><p>选择 torch.nn.LayerNorm为例（<em>normalized_shape=(C,H,W)</em>），对于【N, C, H, W】的特征图（下图可以理解为【N ,C, H*W】，保留通道 N 计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/4.jpg" style="zoom:80%;" /></p><ul><li><p>注意与BatchNorm2d的不同：</p><ul><li>当torch.nn.LayerNorm的elementwise_affine为True时， $\gamma $ 和 $\beta $ 的参数数量分别时 C*H*W 。</li><li>当torch.nn.LayerNorm的elementwise_affine为False时，没有 $\gamma $ 和 $\beta $ 参数</li></ul></li><li><p><strong>训练和推理阶段</strong>：均值和方差根据输入数据计算，不需要在训练集上用滑动平均方法计算，所以不保存均值和方差。</p></li><li><p>代码：实现的时候γ和β参数的维度和一开始想的不一样（一开始以为和batch_norm2d一样，那应该是2N个可学习参数，实际上是根据元素数量来的），看了源码才发现没有均值和方差这2个参数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">def LayerNorm(x, gamma, beta, eps=1e-5): </span><br><span class="line">    # x: input shape [N, C, H, W] </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    mean = torch.mean(input=x, dim=[1,2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[1,2,3], keepdim=True) </span><br><span class="line">    # mean, var shape: [N, 1, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"></span><br><span class="line"># numpy版本 更对(两个可学习的参数)</span><br><span class="line">def layer_normalize2d(x, alpha=1.0, beta=0.0, eps=1e-5):</span><br><span class="line"># x: input shape [N, C, H, W] </span><br><span class="line">    mean = np.mean(x, axis=(1, 2, 3), keepdims=True)  # [N, 1, 1, 1]</span><br><span class="line">    variance = np.var(x, axis=(1, 2, 3), keepdims=True)  # [N, 1, 1, 1]</span><br><span class="line">    alpha = np.zeros(shape=(1,) + x.shape[1:]) + alpha #[1,C,H,W]</span><br><span class="line">    beta = np.zeros(shape=(1,) + x.shape[1:]) + beta #[1,C,H,W]</span><br><span class="line">    x = (x- mean) / np.sqrt(variance + eps)</span><br><span class="line">    return x * alpha + beta</span><br></pre></td></tr></table></figure><p>在这种方法中，batch(N) 中的每个示例都在 [C, H, W] 维度上进行了归一化。 与 BN 一样，它可以加速和稳定训练，并且不受批次的限制。 此方法可用于批量为 1 的在线学习任务。</p></li></ul><h4 id="实例归一化-Instance-Normalization"><a href="#实例归一化-Instance-Normalization" class="headerlink" title="实例归一化(Instance Normalization)"></a>实例归一化(Instance Normalization)</h4><p>IN是针对图像像素做归一化处理，适用于生成模型中，例如图像的风格化迁移等。</p><h5 id="IN基本原理和公式"><a href="#IN基本原理和公式" class="headerlink" title="IN基本原理和公式"></a>IN基本原理和公式</h5><p>基本原理和公式与 BN的差不多，只不过是计算均值和方差在不一样的维度罢了。</p><h5 id="IN的计算"><a href="#IN的计算" class="headerlink" title="IN的计算"></a>IN的计算</h5><p>以 torch.nn.InstanceNorm2d（track_running_stats=True, affine=True）为例，对于【N, C, H, W】的特征图（下图可以理解为[N ,C, H*W]），保留通道N和C计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/5.jpg" style="zoom:80%;" /></p><ul><li><p>相信有很多朋友都是根据这张图来理解Instance Norm，一眼看上去均值mean和方差variance的维度应该是[N, C]，但是打印出runing_mean和runing_var维度一看，竟然是[C]！What?</p><p>好家伙，经过多次测试，终于搞明白了，原来在归一化计算（一、归一化  中的公式）的时候，均值和方差维度是[N, C]，但是因为batch_size可能会变化，所以running_mean和running_var保存的时候把[N,C]的均值和方差取了个均值，所以runing_mean和runing_var维度是[C]。</p></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#torch版本</span><br><span class="line">def InstanceNorm(x, gamma, beta, eps=1e-5): </span><br><span class="line">    # x: input shape [N, C, H, W] </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    mean = torch.mean(input=x, dim=[2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[2,3], keepdim=True) </span><br><span class="line">    # mean, var  shape: [N, C, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"></span><br><span class="line">#numpy版本</span><br><span class="line">def instance_normalize2d(x, alpha=1.0, beta=0.0, eps=1e-5):</span><br><span class="line">    mean = np.mean(x, axis=(2, 3), keepdims=True)  # [N, C, 1, 1]</span><br><span class="line">    variance = np.var(x, axis=(2, 3), keepdims=True)  # [N, C, 1, 1]</span><br><span class="line">    alpha = np.zeros_like(mean) + alpha</span><br><span class="line">    beta = np.zeros_like(mean) + beta</span><br><span class="line">    # print(mean[:, :, 0, 0].mean(axis=0), variance[:, :, 0, 0].mean(axis=0))</span><br><span class="line">    x = (x - u) / np.sqrt(variance + eps) </span><br><span class="line">return x * alpha + beta</span><br></pre></td></tr></table></figure></li></ul><h4 id="组归一化-Group-Normalization"><a href="#组归一化-Group-Normalization" class="headerlink" title="组归一化(Group Normalization)"></a>组归一化(Group Normalization)</h4><p>InstanceNorm就是GroupNorm的特例，当Group Norm分组和channels相同时，就是instance norm，当分组为1时，就是LayerNorm。 GN 将通道分成组并在它们之间进行标准化。 该方案使计算独立于批量大小。</p><h5 id="GN基本原理和公式"><a href="#GN基本原理和公式" class="headerlink" title="GN基本原理和公式"></a>GN基本原理和公式</h5><p>基本原理和公式与 BN的差不多，只不过是计算均值和方差在不一样的维度罢了。</p><h5 id="GN计算"><a href="#GN计算" class="headerlink" title="GN计算"></a>GN计算</h5><p>以 torch.nn.GroupNorm为例，对于【N, C, H, W】的特征图（下图可以理解为[N ,C, H*W]），需要先将通道C维度划分为G个组得到新的特征图【N, G,C/G H, W】，然后保留通道N和G计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/6.jpg" style="zoom:80%;" /></p><ul><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def GroupNorm(x, gamma, beta, G, eps=1e-5): </span><br><span class="line">    # x: input features with shape [N, C, H, W] </span><br><span class="line">    # G : number of groups </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, G, C // G, H, W]) </span><br><span class="line">    mean = torch.mean(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    # mean, var shape : [N, G, 1, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, C, H, W]) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"># 可学习参数的不同(更好)    </span><br><span class="line">def GroupNorm(x, G, gamma=1.0, beta=0.0, eps=1e-5): </span><br><span class="line">    # x: input features with shape [N, C, H, W] </span><br><span class="line">    # G : number of groups </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, G, C // G, H, W]) </span><br><span class="line">    mean = torch.mean(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    # mean, var shape : [N, G, 1, 1, 1] </span><br><span class="line">    gamma = np.zeros_like(mean) + gamma</span><br><span class="line">    beta = np.zeros_like(mean) + beta</span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, C, H, W]) </span><br><span class="line">    return x * gamma + beta</span><br></pre></td></tr></table></figure></li></ul><h4 id="可切换归一化-Switchable-Normalization"><a href="#可切换归一化-Switchable-Normalization" class="headerlink" title="可切换归一化(Switchable Normalization)"></a>可切换归一化(Switchable Normalization)</h4><p>BN、LN、IN这些归一化方法往往能提升模型性能，但当你接收一个任务时，具体选择哪个归一化方法仍然需要人工选择，这往往需要大量的对照实验或者开发者本身优秀的经验才能选出最合适的归一化方法，因此SN出场了。</p><p>它的算法核心在于提出了一个可微的归一化层，可以让模型根据数据来学习到每一层该选择的归一化方法，或是三个归一化方法的加权和。所以SN是一个任务无关的归一化方法，在分类，检测，分割，IST，LSTM等各个方向的任务中，均取得了非常好的效果。</p><p>SN算法是为三组不同的γ和β分别学习共六个标量值$(W_k,W_k^{‘})$，得到他们的加权和：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/9.png" alt=""></p><p>其中=$W={IN,LN,BN}$。这样仅仅通过增加六个参数，由三种不同统计方法的共同优化，得到更好的归一化结果，但训练过程比较复杂。因此其比其他的归一化方法存在更高的鲁棒性、通用性和多样性。</p><h4 id="权重归一化-Weight-Normalization"><a href="#权重归一化-Weight-Normalization" class="headerlink" title="权重归一化(Weight Normalization)"></a>权重归一化(Weight Normalization)</h4><p>权重归一化(Weight Normalization)是对神经网络的连接权重进行归一化，通过再参数化(Reparameterization)方法，将连接权重分解为长度和方向两种参数。</p><p>已经对输入和层输出进行了标准化，唯一剩下的就是权重。因为它们可以在没有任何控制的情况下变大，尤其是当我们无论如何都要标准化输出时。 通过标准化权重，我们实现了更平滑的损失和更稳定的训练。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/7.jpg" style="zoom:80%;" /></p><ul><li><p>代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def WeightStand(w, eps=1e-5): </span><br><span class="line">    # w: input features shape [Cin, Cout, kernel_size, kernel_size] </span><br><span class="line">    mean = torch.mean(input=w, dim=[0,2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=w, dim=[0,2,3], keepdim=True) </span><br><span class="line">    # mean, var shape : [1, Cout, 1, 1] </span><br><span class="line">    w = (w - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return w</span><br></pre></td></tr></table></figure></li></ul><h4 id="归一化对比"><a href="#归一化对比" class="headerlink" title="归一化对比"></a>归一化对比</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/8.jpg" alt=""></p><ul><li><p>BN：<strong>取整个Batch-size,多个样本做归一化</strong>。</p></li><li><p>LN：<strong>取同一个样本的不同通道做归一化，逐样本归一化</strong>。是由Hinton及其学生提出，可以很好的用在序列型网络如RNN中，同时LN在训练和预测时均值方差都由当前样本确定，这与BN不同。可以不进行批训练。</p></li><li><p>IN：<strong>仅仅对每一个样本的每一个通道做归一化</strong>。主要用于生成模型中， feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格，如图片风格迁移，图片生成结果主要依赖于某个图像实例，所以BN不行</p></li><li><p>GN：<strong>介于LN和IN之间的一种方法，对每个样本的多个通道进行归一化</strong>。用由何凯明团队提出，优化了BN在batch_size较小时的劣势，适用于占用显存较大的任务，如图像分割，一般为16个通道为一组(经验)</p></li></ul><p><strong>目前BN使用最为广泛，能加快模型收敛速度，提高网络泛化性</strong>，但是存在小batch时错误率的问题。LN多用于RNN中，且不需要批训练，但在输入的特征区别较大时不建议使用。</p><p>IN用于图像的风格化迁移方面，不会受到通道数和batch size的影响，但特征通道之间的存在相关性时，则不建议使用。GN避免了BN的问题，训练时与batch size大小无关，但验证效果比BN差一些。<strong>SN是让模型根据数据来学习到每一层该选择的归一化方法或是BN、IN、GN归一化方法的加权和，但计算复杂</strong>。WN在噪声较大时能取得更好的效果，不受限于batch，但是对初始参数较为敏感，目前使用极少。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data_Enhancemenct</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Data-Enhancemence/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Data-Enhancemence/</url>
      
        <content type="html"><![CDATA[<h2 id="Data-Enhancement"><a href="#Data-Enhancement" class="headerlink" title="Data Enhancement"></a>Data Enhancement</h2><ul><li>数据增强可以提高泛化能力，但这一过程依赖于数据集，而且需要专门知识。其次，数据增强假定领域内样本都是同一类，且没有对不同类不同样本之间领域关系进行建模。</li><li>避免过拟合。当数据集具有某种明显的特征,例如数据集中图片基本在同一个场景中拍摄,使用Cutout方法和风格迁移变化等相关方法可避免模型学到跟目标无关的信息。</li><li>提升模型鲁棒性,降低模型对图像的敏感度。当训练数据都属于比较理想的状态,碰到一些特殊情况,如遮挡,亮度,模糊等情况容易识别错误,对训练数据加上噪声,掩码等方法可提升模型鲁棒性。</li><li>增加训练数据,提高模型泛化能力。</li><li>避免样本不均衡。在工业缺陷检测方面,医疗疾病识别方面,容易出现正负样本极度不平衡的情况,通过对少样本进行一些数据增强方法,降低样本不均衡比例</li></ul><p><strong>常规弱增强</strong></p><ul><li>随机缩放，随机按 [0.5, 2.0] 调整图像大小。</li><li>随机翻转，以 0.5 的概率水平翻转图像。</li><li>随机裁剪，从图像中随机裁剪一个区域（513 × 513，769 × 769）</li></ul><p><strong>常规强增强</strong></p><ul><li>Identity，返回原始图像。</li><li>反相，将图像像素反相。</li><li>自动对比度，最大化（正常化）图像对比度。</li><li>均衡，均衡图像直方图。</li><li>高斯模糊，使用高斯核对图像进行模糊处理。</li><li>对比度，以 [0.05, 0.95] 调整图像对比度。</li><li>锐度，以 [0.05, 0.95] 的幅度调整图像的锐度。</li><li>色彩，通过 [0.05, 0.95] 增强图像的色彩平衡</li><li>亮度，以 [0.05, 0.95] 调节图像亮度</li><li>色调，以 [0.0, 0.5] 的幅度抖动图像的色调</li><li>Posterize，将每个像素降低到 [4,8] 位。</li><li>日晒化，将图像中所有高于阈值 [1,256] 的像素反相。</li><li>CutMix，</li></ul><p><strong>空间几何变换</strong></p><ul><li><p>翻转，翻转包括水平翻转和垂直翻转。</p></li><li><p>crop，裁剪图片的感兴趣区域（ROI），通常在训练的时候，会采用随机裁剪的方法，下图为随机裁剪4次的效果。</p></li><li><p>旋转，对图像做一定角度对旋转操作，看看效果。</p></li><li><p>缩放变形，随机选取图像的一部分，然后将其缩放到原图像尺度。</p></li><li><p>仿射变换，同时对图片做裁剪、旋转、转换、模式调整等多重操作。</p></li><li><p>视觉变换，对图像应用一个随机的四点透视变换。</p></li><li>分段仿射（PiecewiseAffine），分段仿射在图像上放置一个规则的点网格，根据正态分布的样本数量移动这些点及周围的图像区域。</li></ul><p><strong>噪声类</strong></p><p>随机噪声是在原来的图片的基础上，随机叠加一些噪声。</p><ul><li>高斯噪声</li><li><p>CoarseDropout，在面积大小可选定、位置随机的矩形区域上丢失信息实现转换，所有通道的信息丢失产生黑色矩形块，部分通道的信息丢失产生彩色噪声。</p></li><li><p>SimplexNoiseAlpha，产生连续单一噪声的掩模后，将掩模与原图像混合。</p></li><li><p>FrequencyNoiseAlpha。在频域中用随机指数对噪声映射进行加权，再转换到空间域。在不同图像中，随着指数值逐渐增大，依次出现平滑的大斑点、多云模式、重复出现的小斑块。</p></li></ul><p><strong>模糊类</strong></p><p>减少各像素点值的差异实现图片模糊，实现像素的平滑化。</p><ul><li>高斯模糊</li><li><p>ElasticTransformation，根据扭曲场的平滑度与强度逐一地移动局部像素点实现模糊效果。</p></li><li><p>HSV对比度变换，通过向HSV空间中的每个像素添加或减少V值，修改色调和饱和度实现对比度转换。</p></li><li><p>RGB颜色扰动，将图片从RGB颜色空间转换到另一颜色空间，增加或减少颜色参数后返回RGB颜色空间。</p></li><li><p>随机擦除法，对图片上随机选取一块区域，随机地擦除图像信息。</p></li><li><p>超像素法（Superpixels），在最大分辨率处生成图像的若干个超像素，并将其调整到原始大小，再将原始图像中所有超像素区域按一定比例替换为超像素，其他区域不改变。</p></li><li><p>转换法（invert），按给定的概率值将部分或全部通道的像素值从v设置为255-v。</p></li><li><p>边界检测（EdgeDetect），检测图像中的所有边缘，将它们标记为黑白图像，再将结果与原始图像叠加。</p></li><li><p>GrayScale，将图像从RGB颜色空间转换为灰度空间，通过某一通道与原图像混合。</p></li><li><p>锐化（sharpen）与浮雕（emboss），对图像执行某一程度的锐化或浮雕操作，通过某一通道将结果与图像融合。</p></li></ul><h3 id="Mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION"><a href="#Mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION" class="headerlink" title="Mixup: BEYOND EMPIRICAL RISK MINIMIZATION"></a>Mixup: BEYOND EMPIRICAL RISK MINIMIZATION</h3><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/1.png" alt=""></p><ul><li>代码：<a href="https://github.com/facebookresearch/mixup-cifar10">GitHub - facebookresearch/mixup-cifar10: mixup: Beyond Empirical Risk Minimization</a></li><li>代码1：<a href="https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch/blob/master/Residual-Attention-Network/train_mixup.py">https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch/blob/master/Residual-Attention-Network/train_mixup.py</a></li><li>论文：<a href="https://arxiv.org/pdf/1710.09412.pdf">https://arxiv.org/pdf/1710.09412.pdf</a></li><li>参考链接：<a href="https://www.zhihu.com/question/308572298，https://blog.csdn.net/u013841196/article/details/81049968，https://blog.csdn.net/sinat_36618660/article/details/101633504?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;utm_relevant_index=1">https://www.zhihu.com/question/308572298，https://blog.csdn.net/u013841196/article/details/81049968，https://blog.csdn.net/sinat_36618660/article/details/101633504?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;utm_relevant_index=1</a></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/2.jpg" alt=""></p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/3.jpg" alt=""></p><ul><li><p>对于输入的一个batch的待测图片images，我们将其和随机抽取的图片进行融合，融合比例为lam，得到混合张量inputs；</p></li><li><p>第1步中图片融合的比例lam是[0,1]之间的随机实数，符合beta分布，相加时两张图对应的每个像素值直接相加，即 inputs = lam <em> images + (1-lam) </em> images_random</p></li><li><p>将上面得到的混合张量inputs传递给model得到输出张量outpus，随后计算损失函数时，我们针对两个图片的标签分别计算损失函数，然后按照比例lam进行损失函数的加权求和，即loss = lam <em> criterion(outputs, targets_a) + (1 - lam) </em> criterion(outputs, targets_b)；</p></li><li><p><strong>Mixup核心思想：两张图片采用比例混合，label也需要按照比例混合</strong></p></li><li><p>计算损失函数有两个视角：首先应用于目标检测</p><ul><li>对相应的lable进行线性加权，由于lable采用one-hot编码可以理解为对k个类别的每个类给出样本属于该类的概率。加权以后就变成了”two-hot”，也就是认为样本同时属于混合前的两个类别</li><li>不混合label，而是用加权的输入在两个label上分别计算<strong>cross-entropy loss</strong>，最后把两个loss加权作为最终的loss。由于cross-entropy loss的性质，这种做法和把label线性加权是等价的（代码采用这种方式）</li></ul></li><li><p>代码实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">for i,(images,target) in enumerate(train_loader):</span><br><span class="line">    # 1.input output</span><br><span class="line">    images = images.cuda(non_blocking=True)</span><br><span class="line">    target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)</span><br><span class="line">    # 2.mixup  </span><br><span class="line">    # 一般情况下，config.alpha=1，是个超参数</span><br><span class="line">    alpha=config.alpha</span><br><span class="line">    lam = np.random.beta(alpha,alpha)</span><br><span class="line">    index = torch.randperm(images.size(0)).cuda()</span><br><span class="line">    inputs = lam*images + (1-lam)*images[index,:]</span><br><span class="line">    targets_a, targets_b = target, target[index]</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)</span><br><span class="line">    # 3.backward</span><br><span class="line">    optimizer.zero_grad()   # reset gradient</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()        # update parameters of net</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/4.jpg" alt=""></p><h3 id="Cutout-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutou"><a href="#Cutout-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutou" class="headerlink" title="Cutout:Improved Regularization of Convolutional Neural Networks with Cutou"></a>Cutout:Improved Regularization of Convolutional Neural Networks with Cutou</h3><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/5.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/uoguelph-mlrg/Cutout">https://github.com/uoguelph-mlrg/Cutout</a></li><li>论文：<a href="https://arxiv.org/pdf/1708.04552.pdf">https://arxiv.org/pdf/1708.04552.pdf</a></li></ul><h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>这篇文章的出发点除了解决遮挡问题外，还有从dropout上得到启发（所以也称为Cutout）。众所周知，Dropout随机隐藏一些神经元，最后的网络模型相当于多个模型的集成。类似于dropout的思路，这篇文章将drop用在了输入图片上，并且drop掉连续的区域——即矩形区域。通过<strong>patch的遮盖</strong>让网络学习到遮挡的特征。cutout不仅能够让模型学习到如何辨别他们，同时还能更好地结合上下文从而关注一些局部次要的特征。作为一个正则化方法，防止CNN过拟合。cutcout方法很简单，就是在训练的时候，在随机位置应用一个方形矩阵。作者认为这种技术鼓励网络去利用整个图片的信息，而不是依赖于小部分特定的视觉特征。</p><p>Cutout 出发点和随机擦除一样，也是模拟遮挡，目的是提高泛化能力，实现上比<strong>Random Erasing(类似的数据增强方法)</strong>简单，随机选择一个固定大小的正方形区域，然后采用全0填充就OK了，当然为了避免填充0值对训练的影响，应该要对数据进行中心归一化操作，norm到0。</p><h4 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h4><p>需要注意的是作者发现cutout区域的大小比形状重要，所以cutout只要是正方形就行，非常简单。具体操作是利用固定大小的矩形对图像进行遮挡，在矩形范围内，所有的值都被设置为0，或者其他纯色值。而且擦除矩形区域存在一定概率不完全在原图像中的（论文设置50%）</p><ul><li><p>代码实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class Cutout(object):</span><br><span class="line">    &quot;&quot;&quot;Randomly mask out one or more patches from an image.</span><br><span class="line">    Args:</span><br><span class="line">        n_holes (int): Number of patches to cut out of each image. default 1</span><br><span class="line">        length (int): The length (in pixels) of each square patch.  default 16</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, n_holes, length):</span><br><span class="line">        self.n_holes = n_holes</span><br><span class="line">        self.length = length</span><br><span class="line">    def __call__(self, img):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            img (Tensor): Tensor image of size (C, H, W).</span><br><span class="line">        Returns:</span><br><span class="line">            Tensor: Image with n_holes of dimension length x length cut out of it.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        h = img.size(1)</span><br><span class="line">        w = img.size(2)</span><br><span class="line">        mask = np.ones((h, w), np.float32)</span><br><span class="line">        for n in range(self.n_holes):</span><br><span class="line">            y = np.random.randint(h)</span><br><span class="line">            x = np.random.randint(w)</span><br><span class="line">            y1 = np.clip(y - self.length // 2, 0, h)</span><br><span class="line">            y2 = np.clip(y + self.length // 2, 0, h)</span><br><span class="line">            x1 = np.clip(x - self.length // 2, 0, w)</span><br><span class="line">            x2 = np.clip(x + self.length // 2, 0, w)</span><br><span class="line">            mask[y1: y2, x1: x2] = 0.</span><br><span class="line">        mask = torch.from_numpy(mask)</span><br><span class="line">        mask = mask.expand_as(img)</span><br><span class="line">        img = img * mask</span><br><span class="line">        return img</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/6.jpg" alt=""></p><h3 id="CutMix-Regularization-Strategy-to-Train-Strong-Classifiers-with-Localizable-Features"><a href="#CutMix-Regularization-Strategy-to-Train-Strong-Classifiers-with-Localizable-Features" class="headerlink" title="CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features"></a>CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</h3><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/7.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/clovaai/CutMix-PyTorch">https://github.com/clovaai/CutMix-PyTorch</a></li><li>论文：<a href="https://arxiv.org/pdf/1905.04899v2.pdf">https://arxiv.org/pdf/1905.04899v2.pdf</a></li></ul><h4 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h4><ul><li><p><strong>区域的丢弃策略（Reginal dropout strategies）</strong>能够增强卷积神经网络分类器的性能。</p><ul><li>优点： 该策略能够使得模型更有效的关注目标的明显部分，有好的泛化和目标定位能力。</li><li>缺点： 利用黑色像素或者随机噪声填充移除区域，这样的操作在训练过程中容易导致信息的缺失和无效性。</li><li><strong>解决方法： 提出了CutMix——使用训练集中的图像填补移除区域</strong></li></ul></li><li><p><strong>CutMix最大程度的利用了同一张图像上的两种不同图像信息。具有更好的分类性能和目标定位功能。CutMix在填充了训练集中的其他照片的同时，label也进行了相同比例转换。</strong></p></li><li><strong>CutMix采用了cutout的局部融合思想，并不是采用全0的mask填充，而是采用另外一张图片的相同大小区域填充，就是混合两张图的局部区域，并且采用了mixup的混合label策略，看起来比较make sense</strong></li></ul><h4 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/8.jpg" alt=""></p><ul><li><p>代码</p><ul><li><p>整体算法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">for i, (input, target) in enumerate(train_loader):</span><br><span class="line">   # measure data loading time</span><br><span class="line">   data_time.update(time.time() - end)</span><br><span class="line">   input = input.cuda()</span><br><span class="line">   target = target.cuda()</span><br><span class="line">   r = np.random.rand(1)</span><br><span class="line">   if args.beta &gt; 0 and r &lt; args.cutmix_prob:</span><br><span class="line">      # 1.设定lambda的值，服从beta分布</span><br><span class="line">      lam = np.random.beta(args.beta, args.beta)  # args.beta 超参数 default 1</span><br><span class="line">      # 2.找到两个随机样本</span><br><span class="line">      rand_index = torch.randperm(input.size()[0]).cuda()</span><br><span class="line">      target_a = target</span><br><span class="line">      target_b = target[rand_index]</span><br><span class="line">      # 3.生成裁剪区域B</span><br><span class="line">      bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)</span><br><span class="line">      # 4.将原有的样本A中的B区域，替换成样本B中的B区域</span><br><span class="line">      input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]</span><br><span class="line">      # 5.根据裁剪区域坐标框的值调整lamda的值</span><br><span class="line">      lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))</span><br><span class="line">      # 6.将生成的新的训练样本丢到模型中进行训练</span><br><span class="line">      output = model(input)</span><br><span class="line">      # 7.按lamda值分配权重</span><br><span class="line">      loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>裁剪区域B的坐标值函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def rand_bbox(size, lam):</span><br><span class="line">    W = size[2]</span><br><span class="line">    H = size[3]</span><br><span class="line">    # 论文里的公式2：求出B的rw,rh</span><br><span class="line">    cut_rat = np.sqrt(1. - lam)</span><br><span class="line">    cut_w = np.int(W * cut_rat)</span><br><span class="line">    cut_h = np.int(H * cut_rat)</span><br><span class="line">    # 论文里的公式2：求出B的rx,ry</span><br><span class="line">    cx = np.random.randint(W)</span><br><span class="line">    cy = np.random.randint(H)</span><br><span class="line">    # 限制坐标区域不超过样本大小</span><br><span class="line">    bbx1 = np.clip(cx - cut_w // 2, 0, W)</span><br><span class="line">    bby1 = np.clip(cy - cut_h // 2, 0, H)</span><br><span class="line">    bbx2 = np.clip(cx + cut_w // 2, 0, W)</span><br><span class="line">    bby2 = np.clip(cy + cut_h // 2, 0, H)</span><br><span class="line">    # 返回裁剪B区域的坐标值</span><br><span class="line">    return bbx1, bby1, bbx2, bby2</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/9.jpg" alt=""></p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/10.jpg" alt=""></p><ul><li>热力图：<strong>CutMix</strong>的操作使得模型能够从一幅图像上的局部视图上识别出两个目标，提高训练的效率。由图可以看出，<strong>Cutout</strong>能够使得模型专注于目标较难区分的区域（腹部），但是有一部分区域是没有任何信息的，会影响训练效率；<strong>Mixup</strong>的话会充分利用所有的像素信息，但是会引入一些非常不自然的伪像素信息。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Mixup:将随机的两张样本按比例混合，分类的结果按比例分配；</li><li>Cutout:随机的将样本中的部分区域cut掉，并且填充0像素值，分类的结果不变；</li><li><p>CutMix:就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值，分类结果按一定的比例分配</p></li><li><p>上述三种数据增强的区别：cutout 和 cutmix 就是填充区域像素值的区别；mixup和cutmix是混合两种样本方式上的区别：mixup是将两张图按比例进行插值来混合样本，cutmix是采用cut部分区域再补丁的形式去混合图像，不会有图像混合后不自然的情形</p></li><li><p>CutMix优点：</p><ul><li>在训练过程中不会出现非信息像素，从而能够提高训练效率；</li><li>保留了regional dropout的优势，能够关注目标的non-discriminative parts；</li><li>通过要求模型从局部视图识别对象，对cut区域中添加其他样本的信息，能够进一步增强模型的定位能力；</li><li>不会有图像混合后不自然的情形，能够提升模型分类的表现；</li><li>训练和推理代价保持不变。</li></ul></li><li><p><strong>more data enhancement</strong></p><ul><li><a href="https://cloud.tencent.com/developer/article/1904288">https://cloud.tencent.com/developer/article/1904288</a></li><li><a href="https://www.zhihu.com/question/319291048">https://www.zhihu.com/question/319291048</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Data_Enhancemenct </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers"><a href="#Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers" class="headerlink" title="Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with  Transformers"></a>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with  Transformers</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/fudan-zvg/SETR">https://github.com/fudan-zvg/SETR</a></li><li>论文：<a href="https://arxiv.org/abs/2012.15840">https://arxiv.org/abs/2012.15840</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>最新的语义分割方法采用具有编码器-解码器体系结构的全卷积网络（FCN）。编码器逐渐降低空间分辨率，并通过更大的感受野学习更多的抽象/语义视觉概念。由于上下文建模对于分割至关重要，因此最新的工作集中在通过扩张/空洞卷积或插入注意力模块来增加感受野。但是，基于编码器-解码器的FCN体系结构保持不变。</p><p>在本文中，我们旨在通过将语义分割视为序列到序列的预测任务来提供替代视角。具体来说，我们部署一个纯 transformer（即，不进行卷积和分辨率降低）将图像编码为一系列patch。通过在 transformer的每一层中建模全局上下文，此编码器可以与简单的解码器组合以提供功能强大的分割模型，称为SEgmentation TRansformer（SETR）。</p><p><strong>一个标准的FCN分割模型有一个编码器-解码器结构：编码器用于特征表示学习，而解码器用于编码器产生的特征表示的像素级分</strong>类。编码器由堆叠的卷积层组成，特征图的分辨率逐渐降低，编码器能够以逐渐增加的感受野学习更多的抽象/语义视觉概念。<br><strong>优点</strong>：translation equivariance：尊重了成像过程的本质，支持了模型对看不见的图像数据的泛化能力<br><strong>局部性</strong>：通过跨空间共享参数来控制模型的复杂性。<br><strong>缺点</strong>：感受野有限，难以学习无约束场景图像中的语义分割的长期依赖信息。<br><strong>解决方法：</strong><br>直接操作卷积运算：大内核尺寸（large kernel sizes），atrous卷积和图像/特征金字塔。<br>注意力模块集成到FCN架构中：对特征图中所有像素的全局交互进行建模。当应用于语义分割时，通常是是将注意力模块与位于顶部的注意力层结合到FCN架构中。不改变FCN模型结构的本质：<strong>编码器下采样输入的空间分辨率，用于区分语义类别的低分辨率特征映射；解码器将特征表示上采样为全分辨率分割映射</strong>。<br>本文中，我们用纯transformer 取代空间分辨率逐渐降低的基于堆叠卷积层的编码器，这种编码器将输入图像视为由学习到的面片嵌入表示的图像面片序列，并使用全局自关注建模对该序列进行转换，以进行有区别的特征表示学习。</p><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><ul><li><p>问题</p><p>典型的语义分割Encoder-Decoder结构以多次下采样损失空间分辨率为代价来抽取局部/全局特征。网络Layer一旦固定,每一层的感受野是受限的,因此要获得更大范围的语义信息,理论上需要更大的感受野即更深的网络结构。</p><p>如何既能够抽取<strong>全局的语义信息,</strong>又能尽量<strong>不损失分辨率,</strong>一直是语义分割的<strong>难点</strong></p></li><li><p>解决方法</p><ul><li>用常用于NLP领域的transformer作为Encoder来抽取全局的语义信息(整个过程不损失image分辨率),代替传统FCN的编码部分,从序列-序列学习的角度,为语义分割问题提供了一种新的视角；</li><li>将图像序列化处理,利用Transformer框架、完全用注意力机制来实现Encoder的功能；</li><li>提出三种复杂度不同的Decoder结构</li></ul></li><li><p>整体网络架构</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/2.jpg" alt=""></p></li><li><p><strong>part 1：图像序列化处理</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/5.jpg" alt=""></p></li></ul><p>image to sequenc，因为NLP中transformation的输入是一维序列,所以需要把图像(H <em> W </em> C)转换成 1D序列。<br>1）： 按pixel-wise进行flatten。考虑到计算量问题所以此方法不通。<br>2）： 按patch-wise进行flatten。本文采用此方法。</p><p>1、输入图像的大小 H <em> W </em> 3（256 <em> 256 </em> 3）的大小，patch_size = 16 <em> 16，因此图像序列化为 256/16 </em> 256 /16 = 256个 （16 <em> 16 </em> 3）的图片大小</p><p>2、向量化后的patch<code>p_i</code>经过<code>Linear Projection</code>function得到向量<code>e_i</code> ，旁边注释<code>e_i</code>是patch embedding,<code>p_i</code>是position embedding。</p><ul><li><strong>part 2：Transformer</strong></li></ul><p>这边采用的是纯 Transformer 的encoder结构，只不过中间重复叠用了24次，具体的使用可以查看 PIT， PVT，Swin Transformer 的总结文档。</p><ul><li><p><strong>part3 Decoder</strong></p><p>本文就提出了三种不一样的 decoder 的设计，分别如下</p><ul><li><p>Navite Upsampling（Naive）</p><p>2-layer：（1 <em> 1）conv  +  sync batch norm（w/ReLU）+ （1 </em> 1）conv</p><p>将Transformer输出的特征维度降到分类类别数后经过双线性上采样恢复原分辨率</p></li><li><p>Progressive UPsampling (PUP）</p><p>交替使用卷积层和两倍上采样操作，为了从<code>H/16 × W/16 × 1024</code> 恢复到<code>H × W × 19</code>(19是cityscape的类别数) 需要4次操作,以恢复到原分辨率。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/6.jpg" alt=""></p></li><li><p>Multi-Level feature Aggregation (MLA)</p><p>首先将Transformer的输出<code>&#123;Z1,Z2,Z3…ZLe&#125;</code>均匀分成M等份,每份取一个特征向量。如下图,24个transformer的输出均分成4份,每份取最后一个,即<code>&#123;Z6,Z12,Z18,Z24&#125;</code> .后面的Decoder只处理这些取出的向量。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/7.jpg" alt=""></p></li></ul><p>具体步骤：</p><p>1.2D —&gt; 3D。将<code>ZL</code> 从2D <code>(H × W)/256 × C</code>恢复到3D <code>H/16 × W/16 × C</code></p><p>2.经过3-layer的卷积<code>1 × 1, 3 × 3, and 3 × 3</code></p><p>3.双线性上采样<code>4×</code></p><p>4.自上而下的融合。以增强<code>Zl</code> 之间的相互联系,如上图最后一个<code>Zl</code>理论上拥有全部上面三个feature的信息,融合即cat</p><p>5.<code>3 × 3</code></p><p>6.双线性插值<code>4×</code> 恢复至原分辨率。</p></li><li><p><strong>损失函数设计</strong></p><p>totalloss = auxiliary loss + main loss</p><p>其中main loss为<code>CrossEntropyLoss</code> ,auxiliary loss在17年CVPR有提及</p></li></ul><h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h3><ul><li>We reformulate the image semantic segmentation problem from a sequence-to-sequence learning perspective, offering an alternative to the dominating encoder-decoder FCN model design.</li><li>As an instantiation, we exploit the transformer framework to implement our fully attentive feature representation encoder by sequentializing images.</li><li>To extensively examine the self-attentive feature presentations, we further introduce three different decoder designs with varying complexities. Extensive experiments show that our SETR models can learn superior feature representations as compared to different FCNs with and without attention modules, yielding new state of the art on ADE20K (50.28%), Pascal Context (55.83%) and competitive results on Cityscapes. Particularly, our entry is ranked the 1stplace in the highly competitive ADE20K test server leaderboard.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>在Cityscapes/ADE20K/PASCAL Context三个数据集上进行了实验。实验结果优于用传统FCN(with &amp; without attention module)抽特征的方法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/4.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/3.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><ul><li>卷积操作的感受野有限是传统FCN体系结构的一个内在限制。为突破该限制，逐渐提出两类方法<ul><li>改变卷积：包括增大卷积核kernel_size、Non-local(跟本文有点像，每次抽取的都是全局特征)和特征金字塔。例如DeepLab引入空洞卷积/SPP/ASPP。</li><li>将注意力模块集成到FCN体系结构中：一次对所有像素的全局信息抽取特征。例如PSANet提出点向空间注意模块、DANet嵌入channel attention和spatial attention。</li></ul></li><li>有人提到，本文是把ViT模型原封不动迁移过来了，替换了encoder，虽带来了精度的提升但模型的计算量和参数量都非常大。</li></ul><blockquote><p>ViT(Vision Transformer)首次证明了纯基于transformer的图像分类模型可以达到sota。</p></blockquote><ul><li>CNN是通过不断地堆积卷积层来完成对图像从局部信息到全局信息的提取,不断堆积的卷积层慢慢地扩大了感受野直至覆盖整个图像;但是transformer并不假定从局部信息开始,而且一开始就可以拿到全局信息,学习难度更大一些,但transformer学习长依赖的能力更强。</li><li>CNN结构更适合底层特征,Transformer更匹配高层语义。二者无绝对差别,就是看问题的尺度差异,本质都是消息传递。</li><li>现在对transformer理解还不充分,为啥Transformer之后没有改变分辨率,还要用Decoder来恢复原image的分辨率(得看看transformer那篇论文,或者评论区有好心人解答一下嘛)。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(CVPR) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(CVPR) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransFuse:Fusing_Transformers_and_CNNs_for_Medical_Image_Segmentation</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation"><a href="#TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation" class="headerlink" title="TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation"></a>TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/Rayicer/TransFuse">https://github.com/Rayicer/TransFuse</a></li><li>论文：<a href="https://arxiv.org/abs/2102.08005">https://arxiv.org/abs/2102.08005</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>传统CNN网络很难捕获长距离的依赖关系，而且一味的加深网络的深度会带来大量的计算冗余。</p><p>文章提出了一种并行分支的TransFuse网络，<strong>结合transformer和CNN两种网络架构，能同时捕获全局依赖关系和低水平的空间细节</strong>，文中还提出了一种BiFusion module用来混合两个分支所提取的图像特征。使用TransFuse，可以以较浅的方式有效地捕获全局依赖性和low-level空间细节</p><ul><li>transformer：good at modeling global context，<strong>but</strong> lack of spatial inductive-bias in modelling local information and limitations in capturing fine-grained details</li><li>CNN：low-level spatial details can be efficiently captured <strong>but</strong> lack of efficiency in capturing global context information</li></ul><p>TransFuse在多个医学分割任务中达到SOTA，并在降低参数和提高推理速度方面得到很大的提升。</p><ul><li><strong>advantage：</strong>firstly, by leveraging the merits of CNNs and Transformers, we argue that  TransFuse can capture global information without building very deep nets while  preserving sensitivity on low-level context; secondly, our proposed BiFusion  module may simultaneously exploit different characteristics of CNNs and  Transformers during feature extraction, thus making the fused representation  powerful and compact.</li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><ul><li><p><strong>整体网络架构</strong>，TransFuse包含两个分支，左边是transformer分支，右边是CNN分支，模型通过BiFusion层整合两个分支的特征，然偶经过上采样和attention-gated skip-connection输出分割结果</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/2.jpg" alt=""></p></li><li><p><strong>Transformer Branch</strong><br>Transformer Branch是一个完整的编解码结构，编码器部分使用的是transformer结构，解码器部分使用的是SERT中提到的渐进上采样（PUP）结构。</p></li><li><p><strong>CNN Branch</strong><br>CNN Branch使用ResNet的第四层，第三层和第二层的输出作为这一分支的输出，由于transformer可以捕获全局的上下文信息，故而CNN Branch并不需要设计的很深 。</p></li><li><p><strong>BiFusion Module</strong><br>BiFusion Module主要由通道注意力和空间注意力组成，对Transformer Branch做通道注意力，对CNN Branch做空间注意力。然后经过卷积，相乘，拼接，残差操作，实现两个分支的特征融合。</p><ul><li><p>通道注意力</p><p>特征的每一个通道都代表着一个专门的检测器，因此，通道注意力是关注什么样的特征是有意义的。为了汇总空间特征，作者采用了全局平均池化和最大池化两种方式来分别利用不同的信息。<strong>通道注意力聚焦在“什么”是有意义的输入图像</strong></p></li><li><p>空间注意力</p><p>空间注意力模块来关注哪里的特征是有意义的，<strong>空间注意力聚焦在“哪里”是最具信息量的部分</strong>，这是对通道注意力的补充。为了计算空间注意力，沿着通道轴应用平均池化和最大池操作，然后将它们连接起来生成一个有效的特征描述符</p></li><li><p>通道注意力和空间注意力详细请看 代表性论文 <strong>CBAM: Convolutional Block Attention Module</strong></p></li></ul></li><li><p>最后通过上采样和attention-gated skip-connection输出分割结果。</p></li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li><p>本文采用四个数据集进行验证，分别是，Polyp Segmentation，Skin Lesion Segmentation， Hip Segmentation，Prostate Segmentation</p></li><li><p>定量结果</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/4.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/5.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/3.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>文章使用transformer结构捕捉图像的全局上下文信息，并利用这一优点减小CNN结构的层数，只是用很少的卷积层提取局部空间信息作为transformer的补充，并通过BiFusion进行特征融合，最后通过Attention-gate，上采样输出分割结果。文中一共出现四种注意力机制。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer"><a href="#Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer" class="headerlink" title="Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer"></a>Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/zhiheLu/CWT-for-FSS">https://github.com/zhiheLu/CWT-for-FSS</a></li><li>论文：<a href="https://arxiv.org/abs/2108.03032">https://arxiv.org/abs/2108.03032</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>得益于大型的标签数据和深度学习算法的发展，语义分割方法近几年取得了很大的进步。但这些方法有两个局限：</p><p><strong>1）过度依赖带标签数据，而这些数据的获得通常消耗大量人力物力；</strong></p><p><strong>2）训练好的模型并不能处理训练过程中未见的新类别。</strong></p><p>面对这些局限，小样本语义分割被提出来，它的目的是通过对少量样本的学习来分割新类别。一般来说，小样本语义分割方法是通过用训练数据模拟测试环境进行元学习使得训练的模型有很好的泛化能力，从而在测试时可以仅仅利用几个样本的信息来迭代模型完成对新类别的分割。具体地，小样本分割模型是在大量的模拟任务上进行训练，每个模拟任务有两个数据组：Support set and Query set。Support set 是有标签的K-shot样本，而Query set只在训练的时候有标签。这样的模拟任务可以有效地模拟测试环境</p><ul><li>针对元学习的概念，可以参考：<a href="https://zhuanlan.zhihu.com/p/289043310">https://zhuanlan.zhihu.com/p/289043310</a></li><li>针对小样本学习的一些概念，可以参考：<a href="https://zhuanlan.zhihu.com/p/84290146">https://zhuanlan.zhihu.com/p/84290146</a></li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><p>一个<strong>小样本分类系统</strong>一般由三部分构成：<strong>编码器，解码器和分类器</strong>。其中，前两个模块模型比较复杂，最后一个分类器结构简单。我们发现现存的小样本分类方法通常在<strong>元学习</strong>的过程中更新所有模块或者除编码器外的模块，而所利用更新模块的数据仅仅有几个样本。在这样的情况下，我们认为模型更新的参数量相比于数据提供的信息量过多，从而不足以优化模型参数。基于此分析，我们提出了一个全新的元学习训练范式，即只对分类器进行元学习。为了方便读者更好的理解，论文给出了两种方法的对比，如下图，图(a)为传统方法，要训练三个模块，图(b)为本文方法，编码器解码器在经过大量有标记数据训练后便冻结，只调整分类器权重</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/2.jpg" alt=""></p><p>具体地，我们采用常规的分割方法对编码器和解码器进行训练，训练后在元学习的过程中不在更新。这是基于我们的假设：在大量标签数据训练下的模型可以提取有辨别性的特征，对一些新类别也有效，这也可以解释很多方法直接使用ImageNet预训练的模型进行特征提取。在分析数据的时候，我们发现Support set和Query set的数据有时有较大的类内差异，如下图。同样的类别，不同的角度即可产生很大的区别。这就使得利用Support set迭代的模型不能很好地作用在Query set上。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/3.jpg" alt=""></p><p>为了解决这个问题，我们提出了<strong>Classifier Weight Transformer</strong>来动态地利用Query set的特征信息来进一步更新分类器模块，从而提升分割任务性能。一个少镜头分割模型通常由三个模块组成：编码器、解码器和分类器。为了学习适应新类别，现有方法通常在编码器在 ImageNet 上进行预训练后对整个模型进行元学习 [23]。在情节训练阶段，模型的所有三个部分都是元学习的。训练后，给定一个带有带注释的支持集图像和用于测试的查询图像的新类，该模型有望使所有三个部分都适应新类。由于只有很少的带注释的支持集图像和复杂且相互关联的三部分，这种适应通常是次优的。为了克服这些限制，我们提出了一个分两个阶段的简单而有效的训练范式。在第一阶段，我们通过监督学习对编码器和解码器进行预训练，以获得更强的特征表示。在第二阶段，连同冻结的编码器和解码器，我们仅对分类器进行元训练。这是因为我们认为预训练的特征表示部分（即编码器和解码器）足以泛化到任何看不见的类别；因此，少样本语义分割的关键在于调整二元分类器（分离前景和背景像素），而不是从少样本样本中调整整个模型。我们方法的概述如下图所示。具体的算法参考原文。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/4.jpg" alt=""></p><p>者假设，一个学习了大量图片和信息的传统分割网络已经能够从任何一张图片中捕捉到充分的，有利于区分背景和前景的信息，无论训练时是否遇到了同类的图。那么面对少样本的新类时，只要对分类器进行元学习即可。 对于分类器的学习，作者提出了一种分类器权重转移方法CWT，根据每一张查询集图象，临时调整分类器参数。借助Transformer的思想，作者将分类器权重转化为Query，将图象提取出来的特征转化为Key和Value，然后根据这三个值调整分类器权重，最后通过残差连接，与原分类器参数求和     </p><p>本文提出的元学习名为episodic training。一般来说，本文提出的元学习针对是小样本的类进行学习。元学习分两步</p><ul><li>第一步是内循环，和预训练一样，根据支持集上的图片和mask进行训练，不过只修改分类器参数。文中指出，当新类样本数够大时，只使用外循环，即只更新分类器，就能匹敌SOTA，但是当面对小样本时，表现就不尽如人意。</li><li>第二步是外循环，根据每一副查询图片，微调分类器参数，而且微调后的参数只针对这一张查询图片，不能用于其他查询图象，也不覆盖修改原分类器参数。</li></ul><h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h3><ul><li>We propose a novel model training paradigm for few-shot semantic  segmentation. Instead of meta-learning the whole, complex segmentation model, we  focus on the simplest classifier part to make new-class adaptation more  tractable. </li><li>We introduce a novel meta-learning algorithm that leverages a  Classifier Weight Transformer (CWT) for adapting dynamically the classifier  weights to every query sample.</li><li>Extensive experiments with two popular  backbones (ResNet-50 and ResNet-101) show that the proposed methodyieldsa  newstate-of-the-artperformance, often surpassing existing alternatives,  especially on 5-shot case, by a large margin. Further, under a more challenging  yet practical cross-domain setting, the margin becomes even bigger.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>在两个标准小样本分割数据集PASCAL和COCO上，我们的方法在大多数情况下取得了最优的结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/5.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/6.jpg" alt=""></p><ul><li>跨数据集的情景下测试了我们模型的性能，可以看出我们的方法展示了很好的鲁棒性</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/7.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/7.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>提出了一种新的元学习训练范式来解决小样本语义分割问题。相比于现有的方法，这种方法更加简洁有效，只对分类器进行元学习。为了解决类内差异问题，我们提出<strong>Classifier Weight Transformer</strong>来利用Query特征信息来迭代分类器，从而获得更加鲁棒的分割效果。通过大量的实验，我们证明了方法的有效性。针对我们现在的创新点，找到存在的问题，采取简便的方法提升性能未必不是一个新的思路，就拿现在这个而言，并没有对编码器-解码-分类器三个进行元学习，而是只针对其中最简单的分类器进行创新，也采纳了transformer，也算是transformer的一个应用把。果然现在在cv领域transformer大放异彩，针对这种思想，之前也看过类似输入网络的图像大小对性能的影响，也看过动态预测调整输入图片大小的预测，也是一种思路，所以咋创新方面不用单单停留在网络的结构上，而是应该放在整体的架构的，因为任何一处的改变都可能对最后的分割性能产生不一样的影响。道阻且长，望君继续努力。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(ICCV) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(ICCV) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SOTR-Segmenting-Objects-with-Transformers</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="SOTR-Segmenting-Objects-with-Transformers"><a href="#SOTR-Segmenting-Objects-with-Transformers" class="headerlink" title="SOTR: Segmenting Objects with Transformers"></a>SOTR: Segmenting Objects with Transformers</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/first.png" alt=""></p><ul><li><p>代码：<a href="https://github.com/easton-cau/SOTR">https://github.com/easton-cau/SOTR</a></p></li><li><p>论文：<a href="https://arxiv.org/abs/2108.06747">https://arxiv.org/abs/2108.06747</a> </p></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><ul><li><p>首先作者研究了实例分割的发展历程，以及各个阶段方法的大概总结，提出个各个阶段的优点与不足的，在实例分割这块，主要的方法就是包括</p><p>Top-down instance segmentation 和 Bottom-up instance segmentation</p><ul><li><strong>Top-down instance segmentation</strong><ul><li><strong>proposal-based</strong>方法：基于目标检测，在得到目标检测框之后再在框内做语义分割分割前景背景，由于这种方法需要借助目标检测中的区域提议，因此该方法称为<strong>proposal-based</strong>方法.，这种方法就是遵循先检测后分割的范式的。缺点如下 例如 Mak-RCNN<ul><li>1）由于有限的感受野，CNN在高级视觉语义信息中相对缺乏 特征的连贯性来关联实例 , 导致对大对象的次优结果；</li><li>2）分割质量和推理速度都严重依赖对象检测 器，在复杂场景中性能较差。</li></ul></li></ul></li><li><p><strong>Bottom-up instance segmentation</strong></p><ul><li><strong>proposal-free方法</strong>：在语义分割图的基础上，将像素聚集到不同的实例上。这类方法的基本思想是利用CNN学到每个像素实例级的特征，接着用一种聚合方法将像素聚合成实例。这种方法通常分两步，一个是分割，一个是聚合。语义分割图获得之后，将像素一步步的聚合到不同的实例中。学习每像素嵌入(per-pixel embedding)和实例感知 特征(instance-aware features)，然后使用后处理技术依次分组，根据嵌入特征(embedding characteristics)将它们转换为实例，只要缺点如下<ul><li>不稳定的聚类（例如碎片化和联合掩码）以及对不同场景数据集的泛化能力差，当场景非常复杂并且一张图像中存在密集的物体时，背景像素上不可避免地 会损失大量的计算和时间</li></ul></li></ul></li><li><p><strong>STOR</strong>：就是一种 (Bottom-up) 自底向上的model模型，也就是基于<strong>proposal-free方法</strong>，有效地学习了位置敏感特征(position-sensitive features )，动态生成实例掩码 (dynamically generates instance masks )，无需后处理分组以及边界框位置和尺度的界限，<strong>SOTR</strong> 以图像为输入，结合 CNN 和 Transformer 模块来 提取特征，并直接对类概率和实例掩码进行预测。</p></li></ul></li><li><p><strong>Transformer</strong></p><ul><li><p>近几年来，由于 Transformer 的崛起，其在cv领域的重要性可想而知，在cv领域上，很多人试图完全 替代 卷积运算 或 将类 CNN 架构与transformer结合用于视觉任务中的特征提取，它可以轻松捕 获全局范围特征(global-range characteristics)并自然地对长距离语义依赖项进行建模</p><ul><li>self-attention，作为transformers 的关键机制，广泛地聚 合了来自整个输入域的特征和位置信息。因此基于transformer的模型可以更好地区分具有相同语义类 别的重叠实例，这使得它们比CNN更适合高级视觉任务。但是他也有不足：<ul><li>（1）典型的attention在提取<strong>(low-level features)</strong>方低级特征方面表现不佳，导致对小对象的错误预测</li><li>（2）由于广泛的特征图(feature map)，需要大量的内存和时间，尤其是在训练阶段</li><li><strong>（1）的问题可以通过结合 CNN 主干得到有效解决</strong></li></ul></li></ul></li><li><p>为了降低传统 self-attention 的内存和计算复杂度，我们提出了 <strong>twin attention</strong>（双注意力），一种替代的自注意 力自回归块，通过将全局空间 attention (注意力) 分解为独立的垂直和水平 attention (注意力) 来显着 减少计算和内存。与原始 Transformer 相比，这种精心设计的架构在计算和内存方面具有显着的节省，尤其是在用于密集预测（如实例分割）的大输入上。</p></li></ul></li><li><p>我们提出了一种创新的自底向上模型(bottom-up model)，称为 <strong>SOTR</strong>，巧妙地结 合了 CNN 和 Transformer 的优点。具体地说，我们采用Transformer模 型来获 取全局依赖关系并提取高级特征(<strong>high-level features</strong>) 以用于后续的函数头部（<strong>Functional heads</strong>）的预 测。其简化了分割管道，建 立在附加了两个并行子任务的替代 CNN 主干上：（1）通过transformer预测每个实例的类别和（2） 动态生成具有多个的(segmentation mask)分割掩码级上采样模块。 SOTR 可以分别通过特征金字塔 网络 (FPN) 和<strong>twin Transformer</strong>有效地提取较低级别的特征表示并捕获远程上下文依赖关系</p></li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><p><strong>SOTR 是一种 CNN-transformer 混合实例分割模型</strong>，可以同时学习 2D 平面信息表示并轻松捕获远程信息。 它遵循直接分割范式，首先将输入特征图划分为     <strong>patches</strong> （补丁），然后在动态分割每个实例的同时预测每个<strong>patches</strong> （补丁）的类别。 具体来说，我们的模型主要由三部分组成：1）从输入图像中提取图像特征的主干，尤其是低级和局部特征，2）一个用于建模全局和语义依赖关系的 <strong>Transformer</strong>  ，它附加了功能头以 分别预测每个<strong>patches</strong> （补丁）的类别和卷积核，以及 3) 一个多级上采样模块，通过在生成特征图和相应的卷积核之间执行动态卷积操作来生成最终的分割掩码(segmentation mask)。 总体框架如<strong>图 2</strong> 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-2.jpg" alt=""></p><p>SOTR 可以分别通过特征金字塔网络 (FPN) 和双Transformer有效地提取较低级别的特征表示并捕获远程上下文依赖关系。同时，与原始Transformer相比，所提出的双Transformer具有时间和资源效率，因为只涉及一行和一列注意力来编码像素。</p><ul><li><strong>twin attention（双注意力）</strong>机制，用稀疏表示来简化 <strong>attention(注意力)</strong> 矩阵。 我们的策略主要将感受野限制为固定步幅的设计块模式。 它首先计算每列中的 <strong>attention(注意力)</strong>，同时保持不同列中的元素独立。 该策略可以在水平尺度上聚合元素之间的上下文信息（见图 3（1））。 然后，在每一行内执行类似的 <strong>attention(注意力) </strong>以充分利用垂直尺度上的特征交互（如图 3（2）所示）。 两个尺度中的注意力依次连接为最后一个尺度，具有全局感受野，涵盖了两个维度的信息</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-3.png" alt=""></p><ul><li><strong>作者称之为 Twins</strong>，代码写的很复杂，但其实就是提前将行维度和列维度分别整合到前一个维度再输入 Attention 模块。这样做可以将时间复杂度降低为：</li></ul><script type="math/tex; mode=display">O((H \times W)^2) \quad to  \quad O(H \times W^2 + W \times H^2)^1</script><ul><li><strong>Functional heads</strong></li></ul><p>来自 <strong>Transformer</strong> 模块的特征图（feature maps）被输入到不同的<strong>函数头（functional heads）</strong>以进行后续预测。 <strong>类头(class head)</strong> 包括 单个线性层(linear)以输出 $N ×N ×M $ 的分类结果，<strong>其中 M 是类的数量</strong>。 由于每个patch(补丁) 只为一个中心落入patch(补丁) 的单个对象分配一个类别，如 YOLO [32]，我们利用多级预测并在不同特征级别共享头部，以进一步提高不同尺度对象的模型性能和效率 . <strong>核头(kernel head)</strong> 也由一个线性层(linear)组成，与 <strong>类头(class head)</strong>并行输出一个 $N×N×D$ 张量用于后续的 掩码(mask) 生成，其中张量表示具有D个参数的 $ N×N $ 卷积核。 在训练期间，<strong>Focal Loss [26]</strong> 被应用于分类，而这些卷积核的所有监督都来自最终的掩码(mask) 损失。</p><ul><li><strong>Mask</strong></li></ul><p>为了构建实例感知和位置敏感分割的掩码特征表示，一种直接的方法是对不同尺度的每个特征图进行预测（[36, 12] 等）。 但是，它会增加时间和资源。 受 <strong>Panoptic FPN [22]</strong> 的启发，我们设计了<strong>多级上采样模块(multi-level upsampling module)</strong>，将来自每个 <strong>FPN 级</strong>和 <strong>transformer</strong> 的特征合并为一个统一的掩码特征。 首先，从 <strong>transformer</strong>模块获得带有位置信息的相对低分辨率特征图P5，并结合<strong>FPN中的P2-P4</strong>执行融合。 对于每个尺度的特征图，操作了 3×3 Conv、Group Norm [39] 和 ReLU 的几个阶段。 然后P3-P5被双线性上采样 2×、4×、8×，分别为  $(\frac{H}{4},\frac{W}{4})$分辨率。 最后，将处理后的 P2-P5 加在一起后，执行逐点卷积和上采样以创建最终统一的 $ H×W$ 特征图。</p><p>例如掩码(mask)预测，SOTR 通过对上述统一特征图执行动态卷积运算为每个 patch(补丁) 生成掩码(mask)。 给定来自内核头部(kernel head)的预测卷积核  $K \quad \epsilon \quad K^{N \times N \times D} $  ，每个内核负责相应 patch(补丁)中实例的掩码(mask)生成。 具体操作可以表示如下：</p><script type="math/tex; mode=display">Z^{H \times W \times N^2} = F^{H \times W \times C} * K^{N \times N  \times D}</script><p>其中 <strong>*</strong> 表示卷积操作，Z是最终生成的掩码，维度为 $ H×W×N^2 $。 需要注意的是，D 的取值取决于卷积核的形状，即D等于$λ^2C$，其中 <strong>λ</strong> 为核大小。 最终的实例分割掩码可以由 <strong>Matrix NMS [37]</strong> 生成，每个掩码都由 <strong>Dice Loss [30]</strong> 独立监督。，</p><ul><li><p>代码的具体实现可以查看github上提供的源代码。参考知乎老哥的讲解：</p><p><a href="https://zhuanlan.zhihu.com/p/424036708">https://zhuanlan.zhihu.com/p/424036708</a> </p></li></ul><h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul><li>We introduce an innovative CNN-transformer-hybrid instance segmentation  framework, termed SOTR. It can effectively model local connectivity and  longrange dependencies leveraging CNN backbone and transformer encoder in the  input domain to make them highly expressive. What’s more, SOTR considerably  streamlines the overall pipeline by directly segmenting object instances without  relying on box detection. </li><li>We devise the twin attention, a new  position-sensitive self-attention mechanism, which is tailored for our  transformer. This well-designed architecture enjoys a significant saving in  computation and memory compared with original transformer, especially on large  inputs for a dense prediction like instance segmentation. </li><li>Apart from pure  transformer based models, the proposed SOTR does not need to be pre-trained on  large datasets to generalize inductive biases well. Thus, SOTR is easier applied  to insufficient amounts of data. </li><li>The performance of SOTR achieves 40.2% of AP  with the ResNet-101-FPN backbone on the MS COCO benchmark, outperforming most of  state-of-the-art approaches in accuracy. Furthermore, SOTR demonstrates  significantly better performance on medium (59.0%) and large objects (73.0%),  thanks to the extraction of global information by twin transformer.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>定量的结果：SOTR 在 MS COCO 数据集上表现良好，并超越了最先进的实例分割方法。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-7.jpg" alt=""></p><ul><li>定性结果：比起其他的分割方法，SOTR具有较好的分割性能</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-4.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>rethingking的话，我觉得，现在transformer那么火爆的时刻，本文其实相对简单，但是其的创新点很突出，很新颖。面对某些具体的任务，了解现在主流的方法并且找出他们各自的优点与不足，然后主要针对这些不足提出新的解决方案。在transformer广泛应用于cv领域后，找到transformer在cv领域后的不足之处，就比如提取low-level farture map特征和计算量内存上面存在不足，所以本文也是比较针对这个方面进行了研究，所以最后才可以提出一种创新的分割框架SOTR</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(ICCV) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(ICCV) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析UNet</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>研究一个深度学习算法，可以先看网络结构，看懂网络结构后，再Loss计算方法、训练方法等。本文主要针对UNet的网络结构进行讲解</p><p>卷积神经网络被大规模的应用在分类任务中，输出的结果是整个图像的类标签。但是UNet是像素级分类，输出的则是每个像素点的类别，且不同类别的像素会显示不同颜色，UNet常常用在生物医学图像上，而该任务中图片数据往往较少。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。</p><ul><li><strong>优点</strong><ul><li>输出结果可以定位出目标类别的位置；</li><li>由于输入的训练数据是patches，这样就相当于进行了数据增强，从而解决了生物医学图像数量少的问题，数据增强有利于模型的训练</li></ul></li><li><strong>缺点</strong><ul><li>训练过程较慢，网络必须训练每个patches，由于每个patches具有较多的重叠部分，这样持续训练patches，就会导致相当多的图片特征被多次训练，造成资源的浪费，导致训练时间加长且效率会低下。但是也会认为网络对这个特征进行多次训练，会对这个特征影响十分深刻，从而准确率得到改进。但是这里你拿一张图片复制100次去训练，很可能会出现过拟合的现象，对于这张图片确实十分敏感，但是拿另外一张图片来就可能识别不出了啦</li><li>定位准确性和获取上下文信息不可兼得，大的patches需要更多的max-pooling，这样会减少定位准确性，因为最大池化会丢失目标像素和周围像素之间的空间关系，而小patches只能看到很小的局部信息，包含的背景信息不够。</li></ul></li></ul><h4 id="网络结构原理"><a href="#网络结构原理" class="headerlink" title="网络结构原理"></a>网络结构原理</h4><p>UNet网络结构，最主要的两个特点是：U型网络结构和Skip Connection跳层连接。</p><p>UNet网络结构分为三个部分，原理图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/1.jpg" alt=""></p><ul><li><p>第一部分是<strong>主干特征提取部分</strong>，我们可以利用<strong>主干部分</strong>获得一个又一个的<strong>特征层</strong>，Unet的主干特征提取部分与VGG相似，为卷积和最大池化的堆叠。<strong>利用主干特征提取部分我们可以获得五个初步有效特征层</strong>，在第二步中，我们会利用这五个有效特征层可以进行特征融合。</p><ul><li><p>下采样</p></li><li><p>左边特征提取网络：使用conv和pooling，就是每次向下采样之前都会进行两次的卷积操作，然后向下采样，然后再进行两次卷积操作，以此往复，向下连续采样五次</p></li></ul></li><li><p>第二部分是<strong>加强特征提取部分</strong>，我们可以利用主干部分获取到的<strong>五个初步有效特征层</strong>进行上采样，并且进行特征融合，获得一个最终的，融合了<strong>所有特征的有效特征层</strong>。</p><ul><li><p>上采样</p></li><li><p>右边网络为特征融合网络：使用上采样产生的特征图与左侧特征图进行concatenate操作</p></li><li><p>Skip Connection中间四条灰色的平行线，Skip Connection就是在上采样的过程中，融合下采样过过程中的feature map。Skip Connection用到的融合的操作也很简单，就是将feature map的通道进行叠加，俗称Concat。</p></li><li><p>Concat操作也很好理解，举个例子：一本大小为10cm<em>10cm，厚度为3cm的书A，和一本大小为10cm</em>10cm，厚度为4cm的书B。将书A和书B，边缘对齐地摞在一起。这样就得到了，大小为10cm*10cm厚度为7cm的一摞书（就是直接把书叠起来的意思）</p></li><li><p>对于feature map，一个大小为<strong>256 <em> 256 </em> 64</strong>的feature map，即feature map的w（宽）为256，h（高）为256，c（通道数）为64。和一个大小为<strong>256 <em> 256 </em> 32</strong>的feature map进行Concat融合，就会得到一个大小为<strong>256 <em> 256 </em> 96</strong>的feature map。</p><p>在实际使用中，Concat融合的两个feature map的大小不一定相同，例如<strong>256 <em> 256 </em> 64</strong>的feature map和<strong>240 <em> 240 </em> 32</strong>的feature map进行Concat。</p><p>这种时候，就有两种办法：</p><ul><li><p>第一种：将大<strong>256 <em> 256 </em> 64</strong>的feature map进行裁剪，裁剪为<strong>240 <em> 240 </em> 64</strong>的feature map，比如上下左右，各舍弃8 pixel，裁剪后再进行Concat，得到<strong>240 <em> 240 </em> 96</strong>的feature map。</p></li><li><p>第二种：将小<strong>240 <em> 240 </em> 32</strong>的feature map进行padding操作，padding为<strong>256 <em> 256 </em> 32</strong>的feature map，比如上下左右，各补8 pixel，padding后再进行Concat，得到<strong>256 <em> 256 </em> 96</strong>的feature map。</p></li></ul><p>UNet采用的Concat方案就是第二种，将小的feature map进行padding，padding的方式是补0，一种常规的常量填充。</p></li></ul></li><li><p>第三部分是<strong>预测部分</strong>，我们会利用<strong>最终获得的最后一个有效特征层</strong>对每一个特征点进行分类，相当于对每一个像素点进行分类。<strong>（将最后特征层调整通道数，也就是我们要分类个数）</strong></p><ul><li>最后再经过两次卷积操作，生成特征图，再用两个卷积核大小为<strong>1*1</strong>的卷积做分类得到最后的两张heatmap，例如第一张表示第一类的得分，第二张表示第二类的得分heatmap，然后作为softmax函数的输入，算出概率比较大的softmax，然后再进行loss，反向传播计算。</li></ul></li></ul><h4 id="网络代码实现"><a href="#网络代码实现" class="headerlink" title="网络代码实现"></a>网络代码实现</h4><p>按照UNet的网络结构分parts去实现Unet结构，<strong>采取一种搭积木的方式，先定义各个独立的模块，最后组合拼接就可以！</strong></p><h5 id="DoubleConv模块"><a href="#DoubleConv模块" class="headerlink" title="DoubleConv模块"></a>DoubleConv模块</h5><p>如下图所示模块，连续的两个卷积的操作，在整个UNet网络中，主干特征提取网络和加强特征网络中各自使用了五次，每一层都会采取这个操作，故可以提取出来：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/2.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class DoubleConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.double_conv(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>nn.Sequential 是一个时许的容器，会将里面的 modle 逐一执行，执行顺序为：<strong>卷积-&gt;BN-&gt;ReLU-&gt;卷积-&gt;BN-&gt;ReLU。</strong></p></li><li><p>in_channels, out_channels，输入输出通道定义为参数，增强扩展使用</p></li><li><p>卷积 nn.Conv2d 的输出：</p><ul><li><p><strong>nn. Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,dilation=1, groups=1, bias=True, padding_mode= ‘zeros’ )</strong></p><ul><li>in_channels:输入的四维张量[N, C, H, W]中的C，也就是说输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。</li><li>out_channels:也很好理解，即期望的四维输出张量的channels数，不再多说。</li><li>kernel_size:卷积核的大小，一般我们会使用5x5、3x3这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）。<br>stride = 1:卷积核在图像窗口上每次平移的间隔，即所谓的步长。这个概念和Tensorflow等其他框架没什么区别，不再多言。</li><li>padding:这是Pytorch与Tensorflow在卷积层实现上最大的差别，padding也就是指图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。需要注意的是这里的填充包括图像的上下左右，以padding=1为例，若原始图像大小为32 <em> 32，那么padding后的图像大小就变成了34 </em> 34，而不是33*33。<br>Pytorch不同于Tensorflow的地方在于，Tensorflow提供的是padding的模式，比如same、valid，且不同模式对应了不同的输出图像尺寸计算公式。而Pytorch则需要手动输入padding的数量，当然，Pytorch这种实现好处就在于输出图像尺寸计算公式是唯一的，</li><li>dilation:这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）。更形象和直观的图示可以观察Github上的Dilated convolution animations，展示了dilation=2的情况。</li><li>groups:决定了是否采用分组卷积，groups参数可以参考groups参数详解</li><li>bias:即是否要添加偏置参数作为可学习参数的一个，默认为True。</li><li>padding_mode:即padding的模式，默认采用零填充。</li></ul></li><li><p>输出通道就是 out_channels</p></li><li><p>输出的 <strong>X * X</strong> 计算公式：</p><script type="math/tex; mode=display">O = （I - K + 2P）/ S +1</script></li></ul></li></ul><pre><code>- I 为输入feature map的大小，O为输出feature map的大小，K为卷积核的大小，P为padding的大小，S为步长</code></pre><h5 id="Down（下采样模块）"><a href="#Down（下采样模块）" class="headerlink" title="Down（下采样模块）"></a>Down（下采样模块）</h5><p>UNet的下采样模块有着4次的下采样过程，过程如下</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/3.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Down(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.maxpool_conv(x)</span><br></pre></td></tr></table></figure><ul><li>代码很简单，就是一个maxpool池化层，进行下采样，然后接一个DoubleConv模块。</li><li>到这里，左边的网络完成！！</li></ul><h5 id="Up（上采样模块）"><a href="#Up（上采样模块）" class="headerlink" title="Up（上采样模块）"></a>Up（上采样模块）</h5><p>上采样模块就是出来常规的上采样操作以外，还需要进行特征融合，</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/4.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Up(nn.Module):</span><br><span class="line">   </span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)</span><br><span class="line">        self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    def forward(self, x1, x2):</span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        # input is CHW</span><br><span class="line">        diffY = x2.size()[2] - x1.size()[2]</span><br><span class="line">        diffX = x2.size()[3] - x1.size()[3]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,</span><br><span class="line">                        diffY // 2, diffY - diffY // 2])</span><br><span class="line">     </span><br><span class="line">        x = torch.cat([x2, x1], dim=1)</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><ul><li><p>初始化函数里定义的上采样方法（反卷积）以及卷积采用DoubleConv</p></li><li><p>反卷积，顾名思义，就是反着卷积。卷积是让featuer map越来越小，反卷积就是让feature map越来越大，</p><p>下面蓝色为原始图片，周围白色的虚线方块为padding结果，通常为0，上面绿色为卷积后的图片。</p><p>这个示意图，就是一个从 <strong>2 * 2</strong>的feature map  —-&gt;  <strong>4 * 4 </strong>的feature map过程。</p><p>在forward前向传播函数中，x1接收的是<strong>上采样</strong>的数据，x2接收的是<strong>特征融合</strong>的数据。特征融合方法就是，上文提到的，先对小的feature map进行padding，再进行concat。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/6.gif" alt=""></p></li></ul><h5 id="OutConv模块"><a href="#OutConv模块" class="headerlink" title="OutConv模块"></a><strong>OutConv模块</strong></h5><p>用上述的DoubleConv模块、Down模块、Up模块就可以拼出UNet的主体网络结构了。UNet网络的输出需要根据分割数量，整合输出通道。</p><p>利用前面的模块，我们可以获取输入进来的图片的特征，此时，我们需要利用特征获得预测结果</p><p>利用特征获得预测结果的过程为：</p><ul><li><strong>利用一个1x1卷积进行通道调整，将最终特征层的通道数调整成num_classes。</strong>  <strong>（即对每一个像素点进行分类）</strong></li></ul><p>这个过程简单，顺便也包装一下吧</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class OutConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><p>到这里，所有的积木已经完成了，接下来就是搭建的过程了。</p><h5 id="UNet模块"><a href="#UNet模块" class="headerlink" title="UNet模块"></a>UNet模块</h5><p>到这里，按照UNet网络结构，设置每个模块的输入输出通道个数以及调用顺序，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from nets.net_of_me.unet_parts import *</span><br><span class="line">class UNet(nn.Module):</span><br><span class="line">    def __init__(self, n_channels, n_classes, bilinear=False):</span><br><span class="line">        super(UNet, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, 64)</span><br><span class="line">        self.down1 = Down(64, 128)</span><br><span class="line">        self.down2 = Down(128, 256)</span><br><span class="line">        self.down3 = Down(256, 512)</span><br><span class="line">        self.down4 = Down(512, 1024)</span><br><span class="line">        self.up1 = Up(1024, 512, bilinear)</span><br><span class="line">        self.up2 = Up(512, 256, bilinear)</span><br><span class="line">        self.up3 = Up(256, 128, bilinear)</span><br><span class="line">        self.up4 = Up(128, 64, bilinear)</span><br><span class="line">        self.outc = OutConv(64, n_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        return logits</span><br></pre></td></tr></table></figure><h4 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练网络的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.backends.cudnn as cudnn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">from nets.unet_training import CE_Loss, Dice_loss, LossHistory</span><br><span class="line">from utils.dataloader import DeeplabDataset, deeplab_dataset_collate</span><br><span class="line">from utils.metrics import f_score</span><br><span class="line"></span><br><span class="line">def get_lr(optimizer):</span><br><span class="line">    for param_group in optimizer.param_groups:</span><br><span class="line">        return param_group[&#x27;lr&#x27;]</span><br><span class="line"></span><br><span class="line">def fit_one_epoch(net,epoch,epoch_size,epoch_size_val,gen,genval,Epoch,cuda):</span><br><span class="line">    net = net.train()</span><br><span class="line">    total_loss = 0</span><br><span class="line">    total_f_score = 0</span><br><span class="line"></span><br><span class="line">    val_toal_loss = 0</span><br><span class="line">    val_total_f_score = 0</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(gen):</span><br><span class="line">        if iteration &gt;= epoch_size:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        #进行训练</span><br><span class="line">        outputs = net(imgs)</span><br><span class="line">        loss    = CE_Loss(outputs, pngs, num_classes = NUM_CLASSES)</span><br><span class="line">        if dice_loss:</span><br><span class="line">            main_dice = Dice_loss(outputs, labels)</span><br><span class="line">            loss      = loss + main_dice</span><br><span class="line"></span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">        waste_time = time.time() - start_time #训练epoch需要的时间</span><br><span class="line">        start_time = time.time()</span><br><span class="line"></span><br><span class="line">        if (iteration % 50 == 0):</span><br><span class="line">            print(&quot;epoch = &#123;&#125; and loss = &#123;&#125; and waste_time = &#123;&#125;&quot;.format(epoch,loss.item(),waste_time))</span><br><span class="line">            #写入日志文件</span><br><span class="line">            with open(&quot;log/train_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch,loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Training&#x27;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Validation&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(genval):</span><br><span class="line">        if iteration &gt;= epoch_size_val:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line"># 开始训练</span><br><span class="line">            outputs = net(imgs)</span><br><span class="line">            #计算损失函数</span><br><span class="line">            val_loss = CE_Loss(outputs, pngs, num_classes=NUM_CLASSES)</span><br><span class="line">            if dice_loss:</span><br><span class="line">                main_dice = Dice_loss(outputs, labels)</span><br><span class="line">                val_loss = val_loss + main_dice</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">            val_toal_loss += val_loss.item()</span><br><span class="line">            val_total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">            if (iteration % 50 == 0):</span><br><span class="line">                print(&quot;epoch = &#123;&#125; and val_loss = &#123;&#125; &quot;.format(epoch, val_loss.item()))</span><br><span class="line">                # 写入日志文件</span><br><span class="line">                with open(&quot;log/val_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                    f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch, val_loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Validation&#x27;)</span><br><span class="line">    print(&#x27;Epoch:&#x27; + str(epoch + 1) + &#x27;/&#x27; + str(Epoch))</span><br><span class="line">    print(&#x27;Total Loss: %.4f || Val Loss: %.4f &#x27; % (total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line">    print(&#x27;Saving state, iter:&#x27;, str(epoch + 1))</span><br><span class="line">    torch.save(model.state_dict(), &#x27;model/Epoch%d-Total_Loss%.4f-%.4f.pth&#x27; % ((epoch + 1), total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    #------------------------------#</span><br><span class="line">    #   输入图片的大小</span><br><span class="line">    #------------------------------#</span><br><span class="line">    inputs_size = [512,512,3]</span><br><span class="line">    #---------------------#</span><br><span class="line">    #   分类个数+1</span><br><span class="line">    #   2+1</span><br><span class="line">    #---------------------#</span><br><span class="line">    NUM_CLASSES = 21</span><br><span class="line">    #   Cuda的使用</span><br><span class="line">    #-------------------------------#</span><br><span class="line">    Cuda = True</span><br><span class="line">    #linux服务器</span><br><span class="line">    dataset_path = &quot;/data/xwd/pro_datas/VOCdevkit/VOC2007&quot;</span><br><span class="line"></span><br><span class="line">    #网络</span><br><span class="line">    model = UNet(n_channels=inputs_size[-1], n_classes=NUM_CLASSES).train()</span><br><span class="line"></span><br><span class="line">    if Cuda:</span><br><span class="line">        net = torch.nn.DataParallel(model)</span><br><span class="line">        cudnn.benchmark = True</span><br><span class="line">        net = net.cuda()</span><br><span class="line"></span><br><span class="line">    # 打开训练数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/train.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        train_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    # 打开验证数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/val.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        val_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    if True:</span><br><span class="line">        lr = 1e-4</span><br><span class="line">        Init_Epoch = 0</span><br><span class="line">        Interval_Epoch = 5</span><br><span class="line">        Batch_size = 4</span><br><span class="line"></span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr)  #优化器</span><br><span class="line">        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.92) #学习率的调整</span><br><span class="line">        </span><br><span class="line">        #封装数据</span><br><span class="line">        train_dataset = DeeplabDataset(train_lines, inputs_size, NUM_CLASSES, True, dataset_path)</span><br><span class="line">        val_dataset = DeeplabDataset(val_lines, inputs_size, NUM_CLASSES, False, dataset_path)</span><br><span class="line">        gen = DataLoader(train_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                         drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line">        gen_val = DataLoader(val_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                             drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line"></span><br><span class="line">        epoch_size = len(train_lines) // Batch_size</span><br><span class="line">        epoch_size_val = len(val_lines) // Batch_size</span><br><span class="line"></span><br><span class="line">        if epoch_size == 0 or epoch_size_val == 0:</span><br><span class="line">            raise ValueError(&quot;数据集过小，无法进行训练，请扩充数据集。&quot;)</span><br><span class="line"></span><br><span class="line">        for epoch in range(Init_Epoch, Interval_Epoch):</span><br><span class="line">            fit_one_epoch(model, epoch, epoch_size, epoch_size_val, gen, gen_val, Interval_Epoch, Cuda)</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>封装数据集</strong>的办法主要采用：自定义类继承Dataset，下面展示的是他的伪代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># ================================================================== #</span><br><span class="line">#                Input pipeline for custom dataset                 #</span><br><span class="line"># ================================================================== </span><br><span class="line"># You should build your custom dataset as below.</span><br><span class="line">class CustomDataset(torch.utils.data.Dataset):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Initialize file paths or a list of file names. </span><br><span class="line">        pass</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span><br><span class="line">        # 2. Preprocess the data (e.g. torchvision.Transform).</span><br><span class="line">        # 3. Return a data pair (e.g. image and label).</span><br><span class="line">        pass</span><br><span class="line">    def __len__(self):</span><br><span class="line">        # You should change 0 to the total size of your dataset.</span><br><span class="line">        return 0 </span><br><span class="line"># You can then use the prebuilt data loader. </span><br><span class="line">custom_dataset = CustomDataset()</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,</span><br><span class="line">                                           batch_size=64, </span><br><span class="line">                                           shuffle=True)</span><br></pre></td></tr></table></figure><ul><li><strong>init</strong>函数是这个类的初始化函数，根据指定的图片路径，读取所有图片数据，</li><li><strong>len</strong>函数可以返回数据的多少，这个类实例化后，通过len()函数调用。</li><li><strong>getitem</strong>函数是数据获取函数，在这个函数里你可以写数据怎么读，怎么处理，并且可以一些数据预处理、数据增强都可以在这里进行</li></ul><p>下面的是自定义的这个方法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">class DeeplabDataset(Dataset):</span><br><span class="line">    def __init__(self,train_lines,image_size,num_classes,random_data,dataset_path):</span><br><span class="line">        super(DeeplabDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.train_lines    = train_lines</span><br><span class="line">        self.train_batches  = len(train_lines)</span><br><span class="line">        self.image_size     = image_size</span><br><span class="line">        self.num_classes    = num_classes</span><br><span class="line">        self.random_data    = random_data</span><br><span class="line">        self.dataset_path   = dataset_path</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.train_batches</span><br><span class="line"></span><br><span class="line">    def rand(self, a=0, b=1):</span><br><span class="line">        return np.random.rand() * (b - a) + a</span><br><span class="line"></span><br><span class="line">    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=1.5, val=1.5):</span><br><span class="line">        label = Image.fromarray(np.array(label))</span><br><span class="line"></span><br><span class="line">        h, w = input_shape</span><br><span class="line">        # resize image</span><br><span class="line">        rand_jit1 = rand(1-jitter,1+jitter)</span><br><span class="line">        rand_jit2 = rand(1-jitter,1+jitter)</span><br><span class="line">        new_ar = w/h * rand_jit1/rand_jit2</span><br><span class="line"></span><br><span class="line">        scale = rand(0.5,1.5)</span><br><span class="line">        if new_ar &lt; 1:</span><br><span class="line">            nh = int(scale*h)</span><br><span class="line">            nw = int(nh*new_ar)</span><br><span class="line">        else:</span><br><span class="line">            nw = int(scale*w)</span><br><span class="line">            nh = int(nw/new_ar)</span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        label = label.resize((nw,nh), Image.NEAREST)</span><br><span class="line">        label = label.convert(&quot;L&quot;)</span><br><span class="line">        </span><br><span class="line">        # flip image or not</span><br><span class="line">        flip = rand()&lt;.5</span><br><span class="line">        if flip: </span><br><span class="line">            image = image.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">            label = label.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">        </span><br><span class="line">        # place image</span><br><span class="line">        dx = int(rand(0, w-nw))</span><br><span class="line">        dy = int(rand(0, h-nh))</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, (w,h), (128,128,128))</span><br><span class="line">        new_label = Image.new(&#x27;L&#x27;, (w,h), (0))</span><br><span class="line">        new_image.paste(image, (dx, dy))</span><br><span class="line">        new_label.paste(label, (dx, dy))</span><br><span class="line">        image = new_image</span><br><span class="line">        label = new_label</span><br><span class="line"></span><br><span class="line">        # distort image</span><br><span class="line">        hue = rand(-hue, hue)</span><br><span class="line">        sat = rand(1, sat) if rand()&lt;.5 else 1/rand(1, sat)</span><br><span class="line">        val = rand(1, val) if rand()&lt;.5 else 1/rand(1, val)</span><br><span class="line">        x = cv2.cvtColor(np.array(image,np.float32)/255, cv2.COLOR_RGB2HSV)</span><br><span class="line">        x[..., 0] += hue*360</span><br><span class="line">        x[..., 0][x[..., 0]&gt;1] -= 1</span><br><span class="line">        x[..., 0][x[..., 0]&lt;0] += 1</span><br><span class="line">        x[..., 1] *= sat</span><br><span class="line">        x[..., 2] *= val</span><br><span class="line">        x[x[:,:, 0]&gt;360, 0] = 360</span><br><span class="line">        x[:, :, 1:][x[:, :, 1:]&gt;1] = 1</span><br><span class="line">        x[x&lt;0] = 0</span><br><span class="line">        image_data = cv2.cvtColor(x, cv2.COLOR_HSV2RGB)*255</span><br><span class="line">        return image_data,label</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        if index == 0:</span><br><span class="line">            shuffle(self.train_lines)  </span><br><span class="line">        annotation_line = self.train_lines[index]</span><br><span class="line">        name = annotation_line.split()[0]</span><br><span class="line">        # 从文件中读取图像</span><br><span class="line">        jpg = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;JPEGImages&quot;), name + &quot;.jpg&quot;))</span><br><span class="line">        png = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;SegmentationClass&quot;), name + &quot;.png&quot;))</span><br><span class="line"></span><br><span class="line">        if self.random_data:</span><br><span class="line">            jpg, png = self.get_random_data(jpg,png,(int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        else:</span><br><span class="line">            jpg, png = letterbox_image(jpg, png, (int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        png = np.array(png)</span><br><span class="line">        png[png &gt;= self.num_classes] = self.num_classes</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        #   转化成one_hot的形式</span><br><span class="line">        #   在这里需要+1是因为voc数据集有些标签具有白边部分</span><br><span class="line">        #   我们需要将白边部分进行忽略，+1的目的是方便忽略。</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        seg_labels = np.eye(self.num_classes+1)[png.reshape([-1])]</span><br><span class="line">        seg_labels = seg_labels.reshape((int(self.image_size[1]),int(self.image_size[0]),self.num_classes+1))</span><br><span class="line">        jpg = np.transpose(np.array(jpg),[2,0,1])/255</span><br><span class="line">        return jpg, png, seg_labels</span><br></pre></td></tr></table></figure><p>我这边设置的epoch并不算很大，采用3090的显卡也是运行了一段时间是时间，可以看到网络，loss实在逐渐在收敛的：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/7.jpg" alt=""></p><p>采用训练好的模型进行预测，看看结果如何：</p><p>这边采用的是在网络上copy的图片预处理和后续处理的代码，本人目前对图片处理还是比较菜，把别人的代码贴在这里，最后给出自己的预测结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line">import colorsys</span><br><span class="line">import copy</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from PIL import Image</span><br><span class="line">from torch import nn</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">#-------------------------------------------#</span><br><span class="line">#   使用自己训练好的模型预测需要修改2个参数</span><br><span class="line">#   model_path和num_classes都需要修改！</span><br><span class="line">#   如果出现shape不匹配</span><br><span class="line">#   一定要注意训练时的model_path和num_classes数的修改</span><br><span class="line">#--------------------------------------------#</span><br><span class="line">class Unet(object):</span><br><span class="line">    _defaults = &#123;</span><br><span class="line">        &quot;model_path&quot;        : &#x27;model\Epoch2-Total_Loss1.0039-0.8573.pth&#x27;, #保存的训练模型的路径</span><br><span class="line">        &quot;model_image_size&quot;  : (512, 512, 3), #输入图片的大小</span><br><span class="line">        &quot;num_classes&quot;       : 21, </span><br><span class="line">        &quot;cuda&quot;              : True,</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        #   blend参数用于控制是否</span><br><span class="line">        #   让识别结果和原图混合</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        &quot;blend&quot;             : True</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   初始化UNET</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        self.__dict__.update(self._defaults)</span><br><span class="line">        self.device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">        self.generate()</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   获得所有的分类</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def generate(self):</span><br><span class="line">        self.net = UNet(n_channels=self.model_image_size[-1],n_classes=self.num_classes).eval()</span><br><span class="line"></span><br><span class="line">        # 加载本地的模型参数</span><br><span class="line">        state_dict = torch.load(self.model_path,self.device)</span><br><span class="line">        self.net.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line">        if self.cuda:</span><br><span class="line">            self.net = nn.DataParallel(self.net) #可以调用多个GPU，帮助加速训练</span><br><span class="line">            self.net = self.net.to(self.device)</span><br><span class="line"></span><br><span class="line">        print(&#x27;&#123;&#125; model loaded.&#x27;.format(self.model_path))</span><br><span class="line"></span><br><span class="line">        if self.num_classes &lt;= 21:</span><br><span class="line">            self.colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),                              (0, 128, 128),  (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), </span><br><span class="line">                           (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), </span><br><span class="line">                           (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64,128),(128, 64, 12)]</span><br><span class="line">        else:</span><br><span class="line">            # 画框设置不同的颜色</span><br><span class="line">            hsv_tuples = [(x / len(self.class_names), 1., 1.)</span><br><span class="line">                        for x in range(len(self.class_names))]</span><br><span class="line">            self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))</span><br><span class="line">            self.colors = list(</span><br><span class="line">                map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),</span><br><span class="line">                    self.colors))</span><br><span class="line"></span><br><span class="line">    def letterbox_image(self ,image, size):</span><br><span class="line">        image = image.convert(&quot;RGB&quot;)</span><br><span class="line">        iw, ih = image.size</span><br><span class="line">        w, h = size</span><br><span class="line">        scale = min(w/iw, h/ih)</span><br><span class="line">        nw = int(iw*scale)</span><br><span class="line">        nh = int(ih*scale)</span><br><span class="line"></span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, size, (128,128,128))</span><br><span class="line">        new_image.paste(image, ((w-nw)//2, (h-nh)//2))</span><br><span class="line">        return new_image,nw,nh</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   检测图片，处理图片</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def detect_image(self, image):</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        image = image.convert(&#x27;RGB&#x27;)</span><br><span class="line">        </span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   对输入图像进行一个备份，后面用于绘图</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        old_img = copy.deepcopy(image)</span><br><span class="line"></span><br><span class="line">        orininal_h = np.array(image).shape[0]</span><br><span class="line">        orininal_w = np.array(image).shape[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   进行不失真的resize，添加灰条，进行图像归一化</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        image, nw, nh = self.letterbox_image(image,(self.model_image_size[1],self.model_image_size[0]))</span><br><span class="line">        a = np.array(image).shape</span><br><span class="line">        images = [np.array(image)/255]</span><br><span class="line"></span><br><span class="line">        images = np.transpose(images,(0,3,1,2))</span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   图片传入网络进行预测</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            images = torch.from_numpy(images).type(torch.FloatTensor) #转化为tensor</span><br><span class="line">            if self.cuda:</span><br><span class="line">                #images =images.cuda()</span><br><span class="line">                images = images.cpu()</span><br><span class="line"></span><br><span class="line">            pr = self.net(images)</span><br><span class="line"></span><br><span class="line">            pr = pr[0]</span><br><span class="line">            pr1 = pr[1]</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            #   取出每一个像素点的种类</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy().argmax(axis=-1)</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            #   将灰条部分截取掉</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            pr = pr[int((self.model_image_size[0]-nh)//2):int((self.model_image_size[0]-nh)//2+nh), </span><br><span class="line">                    int((self.model_image_size[1]-nw)//2):int((self.model_image_size[1]-nw)//2+nw)]</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   创建一副新图，并根据每个像素点的种类赋予颜色</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        seg_img = np.zeros((np.shape(pr)[0],np.shape(pr)[1],3))</span><br><span class="line">        for c in range(self.num_classes):</span><br><span class="line">            seg_img[:,:,0] += ((pr[:,: ] == c )*( self.colors[c][0] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,1] += ((pr[:,: ] == c )*( self.colors[c][1] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,2] += ((pr[:,: ] == c )*( self.colors[c][2] )).astype(&#x27;uint8&#x27;)</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片转换成Image的形式</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        image = Image.fromarray(np.uint8(seg_img)).resize((orininal_w,orininal_h))</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片和原图片混合</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        if self.blend:</span><br><span class="line">             image = Image.blend(old_img,image,0.7)      </span><br><span class="line">        return image,  old_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    unet = Unet()</span><br><span class="line">    while True:</span><br><span class="line">        img = input(&#x27;Input image filename:&#x27;)</span><br><span class="line">        try:</span><br><span class="line">            image = Image.open(img)</span><br><span class="line">        except:</span><br><span class="line">            print(&#x27;Open Error! Try again!&#x27;)</span><br><span class="line">            continue</span><br><span class="line">        else:</span><br><span class="line">            r_image,old_image= unet.detect_image(image)</span><br><span class="line">            old_image.show()</span><br><span class="line">            r_image.show()</span><br></pre></td></tr></table></figure><p>调用这个函数，得到的预测结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/8.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/9.jpg" alt=""></p><h4 id="语义分割的MIOU指标"><a href="#语义分割的MIOU指标" class="headerlink" title="语义分割的MIOU指标"></a>语义分割的MIOU指标</h4><p>语义分割的标准度量。其计算所有类别交集和并集之比的平均值.，语义分割说到底也还是一个分割任务，既然是一个分割的任务，预测的结果往往就是四种情况：</p><ul><li><p>true positive（TP）：预测正确, 预测结果是正类, 真实是正类 </p></li><li><p>false positive（FP）：预测错误, 预测结果是正类, 真实是负类</p></li><li><p>true negative（TN）：预测错误, 预测结果是负类, 真实是正类</p></li><li><p>false negative（FN）：预测正确, 预测结果是负类, 真实是负类  </p></li></ul><p>mIOU 的定义：计算真实值和预测值两个集合的交集和并集之比。这个比例可以变形为TP（交集）比上TP、FP、FN之和（并集）。即：mIOU=TP/(FP+FN+TP)。</p><p>计算公式：</p><script type="math/tex; mode=display">MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{p_{ii}}{\sum_{j=0}^kp_{ij} + \sum_{j=0}^kp_{ji} - p_{ii}}</script><p>等价于：</p><script type="math/tex; mode=display">MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{TP}{FN+FP+TP}</script><p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是基于全局的评价。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/10.jpg" alt=""></p><p><code>MIoU</code>：计算两圆交集（橙色部分）与两圆并集（红色+橙色+黄色）之间的比例，理想情况下两圆重合，比例为1。</p><p>计算本网络的MIoU可以采样训练好的模型进行计算，计算的结果比例越接近1效果越好。</p><p>代码实现后续把，hhhhhhhhhhhhhhhhhhhhh。。。。。。。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉的基本知识介绍</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p><strong>计算机视觉方向：图像分类，图像检测，目标检测，图像分割，图像生成，目标跟踪，超分辨率重构，关键点定位，图像降噪，多模态，图像加密，视频编解码，3D视觉等等</strong></p><h3 id="图像基本概念"><a href="#图像基本概念" class="headerlink" title="图像基本概念"></a>图像基本概念</h3><h4 id="颜色空间"><a href="#颜色空间" class="headerlink" title="颜色空间"></a>颜色空间</h4><ul><li>颜色空间也称彩色模型，用于描述色彩</li><li>常见的颜色空间包括：RGB（常用3通道）、CMYK、YUV（摄像头）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/1.png" alt=""></p><h4 id="RGB色彩模式"><a href="#RGB色彩模式" class="headerlink" title="RGB色彩模式"></a>RGB色彩模式</h4><ul><li>RGB色彩模式是工业界的一种颜色标准</li><li>通过对红（R）、绿（G）、蓝（B）三个颜色通道的变化以及它们相互之间的叠加来<br>得到各式各样的颜色的</li><li>红、绿、蓝三个<strong>颜色通道</strong>每种色各分为256阶亮度，（R，G，B）三维就是一个像素点，（0，0，0）黑，（255，255，255）白</li><li>H <em> W </em> C  H:长，W:宽，C:通道</li></ul><h4 id="HSV色彩模式"><a href="#HSV色彩模式" class="headerlink" title="HSV色彩模式"></a>HSV色彩模式</h4><ul><li>色相（Hue）：指物体传导或反射的波长。更常见的是以颜色如红色，橘色或绿色来辨识，取0到360度的数值来衡量</li><li>饱和度（Saturation）：又称色度，是指色彩的强度或纯度，取值范围为0%~100%</li><li>明度（Value）：表示颜色明亮的程度，取值范围为0%（黑）到100%（白）</li></ul><h4 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h4><ul><li><p>灰度图通常由一个unit8、unit16、单精度类型或者双精度类型的数组描述，也就是上面的 C=1，通道为 1</p></li><li><p>M*N的矩阵，矩阵中每一个元素与图像的一个像素点相对应</p></li><li><p>通常0代表黑色，1、255或65635（为数据矩阵的取值范围上限）代表白色</p><blockquote><p>浮点算法：Gray=R<em>0.3+G</em>0.59+B<em>0.11<br>整数方法：Gray=（R</em>30+G<em>59+B</em>11）/100<br>移位方法：Gray=（R<em>28+G</em>151+B*77）&gt;&gt;8<br>平均值法：Gray=（R+G+B）/3<br>仅取绿色：Gray=G</p></blockquote></li></ul><h3 id="图像处理基本概念"><a href="#图像处理基本概念" class="headerlink" title="图像处理基本概念"></a>图像处理基本概念</h3><h4 id="亮度，对比度，饱和度"><a href="#亮度，对比度，饱和度" class="headerlink" title="亮度，对比度，饱和度"></a>亮度，对比度，饱和度</h4><ul><li>亮度：图像的明亮程度，在单色图像中，最高的值应该对应于白色，最低的值应当对应于黑色；</li><li>对比度：图像暗和亮的落差值，即图像最大灰度级和最小灰度级之间的差值，差异范围越大代表对比越大，差异范围越小代表对比越小</li><li>饱和度：图像颜色种类的多少，饱和度越高，颜色种类越多，外观上看起来图像会更鲜艳</li><li>对于亮度和对比度，可以从RGB图上进行数据增强</li><li>对于饱和度，可以从HSV/HSI/HSL色彩空间上进行增强</li></ul><h4 id="图像平滑-降噪"><a href="#图像平滑-降噪" class="headerlink" title="图像平滑/降噪"></a>图像平滑/降噪</h4><ul><li>图像平滑是指用于突出图像的宽大区域、低频成分、主干部分或抑制图像噪声和干扰高频成分的图像处理方法，使图像亮度平缓渐变，减小突变梯度，改善图像质量。会出现边缘没有，轮廓结构不明显了<ul><li>归一化块滤波器（Normalized Box Filter）</li><li>高斯滤波器（Gaussian Filter）</li><li>中值滤波器（Median Filter）</li><li>双边滤波（Bilateral Filter）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/2.png" alt=""></p><h4 id="图像锐化-增强"><a href="#图像锐化-增强" class="headerlink" title="图像锐化/增强"></a>图像锐化/增强</h4><ul><li>图像锐化与图像平滑是相反的操作，锐化是通过增强高频分量来减少图像中的模糊，增强图像细节边缘和轮廓，增强灰度反差，便于后期对目标的识别和处理。</li><li>锐化处理在增强图像边缘的同时也增加了图像的噪声。</li><li>方法包括：微分法和高通滤波法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/3.jpg" alt=""></p><h4 id="边缘提取算子"><a href="#边缘提取算子" class="headerlink" title="边缘提取算子"></a>边缘提取算子</h4><ul><li><p>图像中的高频和低频的概念理解、</p></li><li><p>通过微分的方式计算图像的边缘（色差或者灰度值做差）</p><blockquote><p>Roberts算子<br>Prewitt算子<br>sobel算子<br>Canny算子<br>Laplacian算子<br>等等</p></blockquote></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/4.jpg" alt=""></p><h4 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h4><ul><li>直方图均衡化是将原图像通过某种变换，得到一幅灰度直方图为均匀分布的新图像的方法。</li><li>对在图像中像素个数多的灰度级进行展宽，而对像素个数少的灰度级进行缩减，从而达到清晰图像的目的。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/5.jpg" alt=""></p><h4 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h4><ul><li><p>图像滤波可以更改或者增强图像。</p></li><li><p>通过滤波，可以强调一些特征或者去除图像中一些不需要的部分。</p></li><li><p>滤波是一个邻域操作算子，利用给定像素周围的像素的值决定此像素的最终的输出值</p></li><li><p>常见的应用包括去噪、图像增强、检测边缘、检测角点、模板匹配等</p><blockquote><p>均值滤波<br>中值滤波<br>高斯滤波<br>双边滤波<br>等等</p></blockquote></li></ul><h4 id="形态学运算"><a href="#形态学运算" class="headerlink" title="形态学运算"></a>形态学运算</h4><ul><li>腐蚀：腐蚀的效果是把图片”变瘦”，其原理是在原图的小区域内取局部最小值。</li><li>膨胀：膨胀与腐蚀相反，取的是局部最大值，效果是把图片”变胖”</li><li>开运算：先腐蚀后膨胀（因为先腐蚀会分开物体，这样容易记住），可以分离物体，消除小区域</li><li>闭运算：先膨胀后腐蚀（先膨胀会使白色的部分扩张，以至于消除/“闭合”物体里面的小黑洞）</li><li>形态学梯度：膨胀图减去腐蚀图，得到轮廓图</li><li>顶帽：原图减去开运算后的图</li><li>黑帽：闭运算后的图减去原图</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/6.jpg" alt=""></p><h3 id="卷积神经网络概念介绍"><a href="#卷积神经网络概念介绍" class="headerlink" title="卷积神经网络概念介绍"></a>卷积神经网络概念介绍</h3><ul><li><p>由卷积核构建，卷积核简称为卷积，也称为滤波器。卷积的大小可以在实际需要时自定义其长和宽（1 <em> 1, 3 </em> 3, 5 * 5）。</p></li><li><p>卷积神经网：以卷积层为主的深度网络结构</p></li><li><strong>卷积层，激活层，BN层，池化层，全连接层（FC层），损失层</strong></li></ul><h4 id="卷积层定义"><a href="#卷积层定义" class="headerlink" title="卷积层定义"></a>卷积层定义</h4><ul><li>对图像和滤波矩阵做内积（逐个元素相乘再求和）的操作<ul><li><strong>nn.Conv2d（in channels，out channels，kernel_size，stride=1，padding=0，dilation=1，groups=1，bias=True）</strong></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/7.jpg" alt=""></p><h4 id="常见的卷积操作"><a href="#常见的卷积操作" class="headerlink" title="常见的卷积操作"></a>常见的卷积操作</h4><ul><li>分组卷积（group参数）</li><li>空洞卷积（dilation参数）</li><li>深度可分离卷积（分组卷积+1×1卷积）。</li><li>反卷积（torch.nn.ConvTranspose2d）</li><li>可变形卷积等等</li></ul><h4 id="理解卷积层的重要概念"><a href="#理解卷积层的重要概念" class="headerlink" title="理解卷积层的重要概念"></a>理解卷积层的重要概念</h4><ul><li><p>感受野（Receptive Field），指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。</p></li><li><p>参数量：参与计算参数的个数，占用内存空间</p></li><li><p>FLOPS：每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</p></li><li><p>FLOPs：浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。</p></li><li><p>MAC：乘加次数，用来衡量计算量。</p></li><li><p>从感受野不变+减少参数量的角度压缩卷积层，压缩卷积层参数&amp;&amp;计算量</p><blockquote><p>采用多个3×3卷积核代替大卷积核<br>采用深度可分离卷积<br>通道Shuffle<br>Pooling层<br>Stride=2<br>等等</p></blockquote></li><li><p>常见卷积层组合结构：<strong>堆叠，跳连，并连</strong></p></li></ul><h4 id="池化层（下采样）"><a href="#池化层（下采样）" class="headerlink" title="池化层（下采样）"></a>池化层（下采样）</h4><ul><li><p>对图片进行压缩（降采样）的一种方法，如max pooling, average pooling等</p></li><li><p>对输入的特征图进行压缩</p><ul><li>一方面使特征图变小，简化网络计算复杂度；</li><li>一方面进行特征压缩，提取主要特征</li></ul></li><li>最大池化（Max Pooling）、平均池化（Average Pooling）等口 </li><li>nn.MaxPool2d（kernel_size，stride=None，padding=0，dilation=1，return_indices=False，ceil_mode=False）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/8.jpg" alt=""></p><h4 id="上采样层"><a href="#上采样层" class="headerlink" title="上采样层"></a>上采样层</h4><ul><li>Resize，如双线性插值直接缩放，类似于图像缩放，概念可见最邻近插值算法和双线性插值算法——图像缩放</li><li>Deconvolution，也叫Transposed Convolution</li><li>实现函数<ul><li>nn.functi onal.interpolate（input，size=None，scale_factor=None，mode=’nearest’，align_corners=None）</li><li>nn.ConvTranspose2d（in channels，out channels，kernel_size，stride=1，padding=0，output padding=0，bias=True）</li></ul></li></ul><h4 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h4><ul><li><p>激活函数的作用就是，在所有的隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数了，因而神经网络的表达能力更加强大了。</p></li><li><p>激活函数：为了增加网络的非线性，进而提升网络的表达能力，<strong>详细见另外一篇博客</strong></p></li><li>ReLU函数、Leakly ReLU函数、ELU函数等</li><li>torch.nn.ReLU（inplace=True）</li></ul><h4 id="BatchNorm层"><a href="#BatchNorm层" class="headerlink" title="BatchNorm层"></a>BatchNorm层</h4><ul><li>通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</li><li>Batchnorm是归一化的一种手段，它会减小图像之间的绝对差异，突出相对差异，加快训练速度</li><li>不适用的问题：image-to-image以及对噪声敏感的任务</li><li>nn.BatchNorm2d（num features，eps=1e-05，momentum=0.1，affine=True，track running_stats=True）</li></ul><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><ul><li><p>口连接所有的特征，将输出值送给分类器（如softmax分类器）（线性）</p><ul><li>对前层的特征进行一个加权和，（卷积层是将数据输入映射到隐层特征空间）将特征空间通过线性变换映射到样本标记空间（也就是label）</li><li>可以通过1×1卷积+global average pooling代替</li><li>可以通过全连接层参数冗余</li><li>全连接层参数和尺寸相关</li></ul></li><li><p>nn.Linear（in features，out features，bias）</p></li></ul><h4 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h4><ul><li>在不同的训练过程中随机扔掉一部分神经元</li><li>测试过程中不使用随机失活，所有的神经元都激活</li><li>为了防止或减轻过拟合而使用的函数，它一般用在全连接层</li><li>nn.dropout</li></ul><h4 id="损失层"><a href="#损失层" class="headerlink" title="损失层"></a>损失层</h4><ul><li><p>损失函数：在深度学习中，损失反映模型最后预测结果与实际真值之间的差距，可以用来分析训练过程的好坏、模型是否收敛等，例如均方损失、交叉熵损失等。</p></li><li><p>损失层：设置一个损失函数用来比较网络的输出和目标值，通过最小化损失来驱动网络的训练</p></li><li><p>网络的损失通过前向操作计算，网络参数相对于损失函数的梯度则通过反向操作计算</p></li><li><p>分类问题损失（分类分割）</p><ul><li>nn.BCELoss；nn.CrossEntropyLoss等等</li></ul></li><li><p>回归问题损失（推测，回归）</p><ul><li>nn.L1Loss；nn.MSELoss；nn.SmoothL1Loss等等</li></ul></li></ul><h3 id="经典卷积神经网络结构"><a href="#经典卷积神经网络结构" class="headerlink" title="经典卷积神经网络结构"></a>经典卷积神经网络结构</h3><p><strong>堆叠，跳连，并连</strong> ，轻量型网络结构，多分支网络结构，attention网络结构</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/9.jpg" alt=""></p><h3 id="其他重要概念"><a href="#其他重要概念" class="headerlink" title="其他重要概念"></a>其他重要概念</h3><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><ul><li><p>学习率作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。</p></li><li><p>合适的学习率能够使目标函数在合适的时间内收敛到局部最小值</p></li><li><p>学习率大，震荡，恐怕到达不了最佳收敛值，学习率小收敛缓慢，消耗时间（如下图到最低点，学习率大小可以看作步长）</p></li><li><p>torch.optim.Ir scheduler</p><blockquote><p>ExponentialLR<br>ReduceLROnPlateau<br>CyclicLR<br>等等</p></blockquote></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/10.jpg" alt=""></p><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>反向传播优化参数</p><ul><li>GD、BGD、SGD、MBGD<ul><li>引入了随机性和噪声</li></ul></li><li>Momentum、NAG等<ul><li>加入动量原则，具有加速梯度下降的作用</li></ul></li><li>AdaGrad，RMSProp，Adam、AdaDelta<ul><li>自适应学习率</li></ul></li><li>torch.optim.Adam</li></ul><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul><li><p>模型出现过拟合现象时，降低模型复杂度</p></li><li><p>L1正则：参数绝对值的和</p></li><li>L2正则：参数的平方和（Pytorch自带，weight decay）</li><li>optimizer=torch.optim.SGD（model.parameters），Ir=0.01，weight_decay=0.001）</li></ul><h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><ul><li><p><strong>参数与超参数</strong></p><ul><li><p><strong>参数：</strong>模型f(x, θ)中的θ 称为模型的参数，可以通过优化算法进行学习。</p></li><li><p><strong>超参数：</strong>用来定义模型结构或优化策略。</p></li></ul></li><li><p><strong>batch_size 批处理</strong></p><ul><li>每次处理的数据数量。</li></ul></li><li><p><strong>epoch 轮次</strong></p><ul><li>把一个数据集，循环运行几轮。</li></ul></li><li><p><strong>transforms 变换</strong></p><ul><li>主要是将图片转换为tensor，旋转图片，以及正则化。</li></ul></li><li><p><strong>nomalize 正则化</strong></p><ul><li>模型出现过拟合现象时，降低模型复杂度</li></ul></li><li><p>前向传播</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/11.png" alt=""></p></li><li><p>反向传播</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/12.jpg" alt=""></p></li><li><p>梯度下降</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/13.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用激活函数（激励函数）理解和总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%EF%BC%89%E7%90%86%E8%A7%A3%E5%92%8C%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%EF%BC%89%E7%90%86%E8%A7%A3%E5%92%8C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近自己刚刚开始学习神经网络的相关知识，在学习构建一个自定义网络的时候，对于在forward函数中突然出现的Relu函数有点奇怪，然后就去百度查询了一波，原来这就是前面了解概念的时候所说的激活函数也就是激励函数，自己也是在百度和知乎上了解的更加透彻一点点，现在就自己的理解和参考一些别人的说法进行一定的总结，这次总结就主要是如下几点</p><ul><li>什么是激活函数</li><li>激活函数的作用（为什么就需要激活函数嘞）</li><li>有哪些常用的激活函数，都各自有什么性质和特点</li><li>在应用中该如何选择合适的激活函数</li></ul><h3 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h3><p>想要了解什么是激活函数，应该先了解神经网络的基本模型，单一的额神经元模型如图1所示。神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/1.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 1 单个神经元结构结构</center><h3 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h3><p>查阅了相关资料和学习，大家普遍对神经网络中的激活函数的额作用主要集中在下面这个观点</p><ul><li><strong>激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题。</strong></li></ul><p>下面举个例子，这个例子是我知乎上看到一位博主写的，个人觉得很不错，就搬过来了，在这里也加入了自己的思考，进一步理解。</p><p>首先我们现在有这么一个需求，就是二分类的问题，如果我要将下图的三角形和圆形进行正确的分类，也就是分隔开来，如图2所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/2.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 2</center><p>利用我们单层的感知机, 下图图3直线是由<img src="https://www.zhihu.com/equation?tex=w_%7B1%7Dx_%7B1%7D+%2B+w_%7B2%7Dx_%7B2%7D%2Bb%3D0+" alt="[公式]">得到，那么该感知器实现预测的功能步骤如下，就是我已经训练好了一个感知器模型，后面对于要预测的样本点，带入模型中，如果<img src="https://www.zhihu.com/equation?tex=y%3E0" alt="[公式]">,那么就说明是直线的右侧，也就是正类（我们定义是三角形），如果<img src="https://www.zhihu.com/equation?tex=y%3C0" alt="[公式]">,那么就说明是直线的左侧，也就是负类（我们定义是圆形)。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/3.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 3</center><p>利用我们上面单层的感知机, 用它可以任意划出一条线, 把平面分割成两部分，如图4所示，很容易能够看出，我给出的样本点根本不是线性可分的，一个感知器无论得到的直线怎么动，都不可能完全正确的将三角形与圆形区分出来，也就是说一条线性结构的直线是无法将三角形和长方形完全分割开来的。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/4.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 4</center><p>此刻我们很容易想到用多个感知器来进行组合（也就是可以多条线性的直线），以便获得更大的分类问题，好的，下面我们上图，看是否可行，如图5，</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/5.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 5</center><p>好的，我们已经得到了多感知器分类器了，那么它的分类能力是否强大到能将非线性数据点正确分类开呢~我们来分析一下：</p><p>我们能够得到</p><script type="math/tex; mode=display">y = W_{2-1}(w_{1-11}x_1+w_{1-21}x_2+b_{1-1}) +  W_{2-2}(w_{1-12}x_1+w_{1-22}x_2+b_{1-2}) + W_{2-3}(w_{1-13}x_1+w_{1-23}x_2+b_{1-3})</script><p>哎呀呀，不得了，这个式子看起来非常复杂，估计应该可以处理我上面的情况了吧，哈哈哈哈~不一定额，我们来给它变个形.上面公式合并同类项后等价于下面公式：</p><script type="math/tex; mode=display">y = x_1(w_{2-1}w_{1-11} + w_{2-2}w_{1-12} + w_{2-3}w_{1-13}) + x_2(w_{2-1}w_{1-21} + w_{2-2}w_{1-22} + w_{2-3}w_{1-23}) + w_{2-1}b_{1-1} + w_{2-2}b_{1-2} +  w_{2-3}b_{1-3}</script><p><strong>啧啧，估计大家都看出了，不管它怎么组合，最多就是线性方程的组合，最后得到的分类器本质还是一个线性方程，该处理不了的非线性问题，它还是处理不了。</strong></p><p><strong>就好像下图图6，直线无论在平面上如果旋转，都不可能完全正确的分开三角形和圆形点：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/6.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 6</center><p>既然是非线性问题，总有线性方程不能正确分类的地方，那么抛开神经网络中神经元需不需要激活函数这点不说，如果没有激活函数，仅仅是线性函数的组合解决的问题太有限了，碰到非线性问题就束手无策了.那么加入激活函数是否可能能够解决呢？</p><p>在上面线性方程的组合过程中（在加入阶跃激活函数的时候），我们其实类似在做三条直线的组合，如下图7：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/7.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 7</center><p>在这里就可以大声说出，激活函数就是来解决非线性因素的，没有太大的问题，就拿sigmoid例子说上面的场景，如图8sigmoid函数</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/8.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 8</center><p><strong>通过这个激活函数映射之后，输出很明显就是一个非线性函数！能不能解决一开始的非线性分类问题不清楚，但是至少说明有可能啊，上面不加入激活函数神经网络压根就不可能解决这个问题</strong></p><p>同理，扩展到多个神经元组合的情况时候，表达能力就会更强~对应的组合图9如下：（现在已经升级为三个非线性感知器在组合了）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/9.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 9</center><p>跟上面线性组合相对应的非线性组合如下，图10：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/10.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 10</center><p><strong>这看起来厉害多了，是不是~最后再通过最优化损失函数的做法，我们能够学习到不断学习靠近能够正确分类三角形和圆形点的曲线，到底会学到什么曲线，不知道到底具体的样子，也许是下面图11这个</strong>，那么随着不断训练优化，我们也就能够解决非线性的问题了。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/11.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 11</center><p><strong>所以到这里为止，我们就解释了这个观点，加入激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题。</strong></p><h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><h4 id="Sigmoid函数，是比较常用的非线性激活函数"><a href="#Sigmoid函数，是比较常用的非线性激活函数" class="headerlink" title="Sigmoid函数，是比较常用的非线性激活函数"></a><strong>Sigmoid函数</strong>，是比较常用的非线性激活函数</h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script></li><li><p>值域：（0，1）；导数值域（0，0.25）</p></li><li><p>函数图像，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/12.jpg" alt=""></p></li><li><p>优点</p><ul><li>值域为(0，1），可以放到模型最后一层，作为模型的概率输出</li><li>特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1</li></ul></li><li><p>缺点</p><ul><li><p>在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。如果我们初始化神经网络的权值为 [ 0 , 1 ]  之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 ( 1 , + ∞ ) (1,+∞)(1,+∞) 区间内的值，则会出现梯度爆炸情况。</p></li><li><p>函数输出不是以0为中心的（不是zero-centered输出问题），这样会使权重更新效率降低。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：如</p><script type="math/tex; mode=display">f = w^Tx + b</script><p>那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。</p></li><li><p>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间</p></li></ul></li></ul><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a><strong>tanh函数</strong></h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script></li><li><p>值域：（-1，1）当|x|&gt;3时，函数容易饱和；导数值域（0，1）当|x|&gt;3时，梯度几乎为0</p></li><li><p>函数图像，如下图：（蓝色是Tanh原函数，紫色是导函数图像）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/13.jpg" alt=""></p></li><li><p>优点</p><ul><li>函数均值为0，一般作为图像生成的最后一层激活函数。</li><li>整个函数是以0为中心的，这个特点比sigmod的好，解决了Sigmoid函数的不是zero-centered输出问题</li></ul></li><li><p>缺点</p><ul><li>仍然存在梯度消失问题；涉及指数运算，复杂度高一些</li></ul></li></ul><h4 id="Relu函数"><a href="#Relu函数" class="headerlink" title="Relu函数"></a><strong>Relu函数</strong></h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = max(0,x)</script></li><li><p>值域：当x<0时，函数值为0，当x>0时，函数值跟x线性增长; 当x<0时，导数值域：导函数值为0，当x>0时，导函数值为1</p></li><li><p>函数图像：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/14.jpg" alt=""></p></li><li><p>优点</p><ul><li>解决了gradient vanishing问题 (在正区间)</li><li>计算速度非常快，只需要判断输入是否大于0</li><li>收敛速度远快于sigmoid和tanh</li></ul></li><li><p>缺点</p><ul><li>当输入是负数的时候，ReLU是完全不被激活的，这就表明一旦输入到了负数，ReLU就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。</li><li>ReLU函数的输出要么是0，要么是正数，这也就是说，ReLU函数也不是以0为中心的函数。</li></ul></li></ul><p>尽管存在这两个问题，<strong>ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！</strong></p><h4 id="ELU函数"><a href="#ELU函数" class="headerlink" title="ELU函数"></a><strong>ELU函数</strong></h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">f(n) = \begin{cases}x, & \text {x > 0}\\\alpha(e^x-1), & \text {otherwise}\end{cases}</script></li><li><p>函数图像：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/15.png" alt=""></p></li><li><p>ELU函数是针对ReLU函数的一个改进型，相比于ReLU函数，在输入为负数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。这样可以消除ReLU死掉的问题，不过还是有梯度饱和和指数运算的问题。不会有Dead ReLU问题和 输出的均值接近0，zero-centered。理论上虽然好于ReLU，但在实际使用中目前并没有好的证据证明ELU总是优于ReLU。</p></li></ul><h4 id="PReLU函数"><a href="#PReLU函数" class="headerlink" title="PReLU函数"></a><strong>PReLU函数</strong></h4><ul><li><p>函数表达式</p><script type="math/tex; mode=display">f = max(\alpha x,x)</script></li><li><p>函数图像：</p></li></ul><p>  <img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/16_1.png" alt=""></p><ul><li><p>PReLU也是针对ReLU的一个改进型，在负数区域内，PReLU有一个很小的斜率，这样也可以避免ReLU死掉的问题。相比于ELU，PReLU在负数区域内是线性运算，斜率虽然小，但是不会趋于0，这算是一定的优势吧。</p><p>我们看PReLU的公式，里面的参数α一般是取0~1之间的数，而且一般还是比较小的，如零点零几。当α=0.01时，我们叫PReLU为Leaky ReLU，是PReLU的一种特殊情况吧</p></li></ul><h4 id="Maxout函数"><a href="#Maxout函数" class="headerlink" title="Maxout函数"></a><strong>Maxout函数</strong></h4><ul><li>函数表达式：</li></ul><script type="math/tex; mode=display">\sigma(x) = max(W_1x+W_2x+b)</script><ul><li>我们可以看到，Sigmoid函数实际上就是把数据映射到一个(−1,1)的空间上，也就是说，Sigmoid函数如果用来分类的话，只能进行二分类，而这里的softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。Softmax回归模型是logistic回归模型在多分类问题上的推广，在多分类问题中，待分类的类别数量大于2，且类别之间互斥。比如我们的网络要完成的功能是识别0-9这10个手写数字，若最后一层的输出为[0,1,0, 0, 0, 0, 0, 0, 0, 0]，则表明我们网络的识别结果为数字1。</li></ul><h3 id="应用中如何选择激活函数"><a href="#应用中如何选择激活函数" class="headerlink" title="应用中如何选择激活函数"></a>应用中如何选择激活函数</h3><ul><li>深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。</li><li>如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.</li><li>最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.</li><li>在较深层的神经网络中，选用relu激活函数能使梯度更好地传播回去，但当使用softmax作为最后一层的激活函数时，其前一层最好不要使用relu进行激活，而是使用tanh作为替代，否则最终的loss很可能变成Nan；</li><li>当选用高级激活函数时，建议的尝试顺序为ReLU-&gt;ELU-&gt;PReLU-&gt;MPELU，因为前两者没有超参数，而后两者需要自己调节参数使其更适应构建的网络结构。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 激活函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络的来龙去脉</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/</url>
      
        <content type="html"><![CDATA[<h2 id="神经网络的来龙去脉"><a href="#神经网络的来龙去脉" class="headerlink" title="神经网络的来龙去脉"></a>神经网络的来龙去脉</h2><p>神经，名呼其曰，就是动物的神经系统，从外界的条件触感和感知到大脑中枢的控制再到控制神经做出一系列的反应。</p><p>其实，在人工只能领域的神经网络而言，大部分的神经网络都可以用<strong>深度</strong> <strong>（depth）</strong>，和<strong>连接结构（connection）</strong>，但是具体的会具体说明。笼统的说，神经网络是可以分为有监督，无监督，半监督的神经网络，其实在这个分类下，忘忘也是你中有我我中有你的的一个局面，在学习的过程中有时候不必要去抠字眼。下面自己在浏览学习后，对神经网络的一点总结。</p><p>发展历程：</p><p>感知机 ==》多层感知机 ==》深度神经网络 ==》卷积神经网络</p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络即指人工神经网络，或称作连接模型，它是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。这种网络依靠系统的复杂程度，通过调整内部大量节点之间相互连接的关系，从而达到处理信息的目的。神经网络用到的算法是向量乘法，采用符号函数及其各种逼近。<strong>并行、容错、可以硬件实现以及自我学习特性</strong>，是神经网络的几个基本优点，也是神经网络计算方法与传统方法的<strong>区别所在</strong>。</p><h2 id="神经网络发展"><a href="#神经网络发展" class="headerlink" title="神经网络发展"></a>神经网络发展</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是Rosenblatt。但是，Rosenblatt的单层感知机有一个严重得不能再严重的问题，对于计算稍微复杂的函数其计算力显得无能为力。</p><h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人发明的多层感知机（multilayer perceptron)克服。多层感知机，顾名思义，就是有多个隐含层的感知机。</p><p>多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这就是我们现在所说的神经网络( NN)！多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。多层感知机给我们带来的启示是，<strong>神经网络的层数直接决定了它对现实的刻画能力——利用每层更少的神经元拟合更加复杂的函数。</strong></p><p>即便大牛们早就预料到神经网络需要变得更深，但是有一个梦魇总是萦绕左右。随着神经网络层数的加深，<strong>优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优</strong>。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，“<strong>梯度消失”现象更加严重</strong>。具体来说，我们常常使用 sigmoid 作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/575577-20190802141514879-1961811671.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 1</center><p>传统意义上的多层神经网络包含三层：</p><ul><li>输入层</li><li>隐藏层</li><li>输出层</li></ul><p>其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适，多层神经网络做的步骤是：特征映射到值，特征是人工挑选。</p><h3 id="深度神经网络-（DNN）"><a href="#深度神经网络-（DNN）" class="headerlink" title="深度神经网络 （DNN）"></a>深度神经网络 （DNN）</h3><p>传统的人工神经网络（ANN）由三部分组成：输入层，隐藏层，输出层，这三部分各占一层。而深度神经网络的“深度”二字表示它的隐藏层大于2层，这使它有了更深的抽象和降维能力。</p><p>2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层(参考论文：Hinton G E, Salakhutdinov R R. Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006, 313(5786):504-507.)，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了 sigmoid，形成了如今 DNN 的基本形式。<strong>单从结构上来说，全连接的DNN和上图的多层感知机是没有任何区别的</strong>。值得一提的是，今年出现的高速公路网络（highway network）和深度残差学习（deep residual learning）进一步避免了梯度弥散问题，网络层数达到了前所未有的一百多层（深度残差学习：152层）</p><h3 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h3><p>如下图2所示，<strong>我们看到全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接</strong>，带来的潜在问题是<strong>参数数量的膨胀</strong>。假设输入的是一幅像素为1K<em>1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是<em>*通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。</em></em></p><p>对于图像，如果没有卷积操作，学习的参数量是灾难级的。CNN之所以用于图像识别，正是由于CNN模型限制了参数的个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被称为前向神经网络(Feed-forward Neural Networks)。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/e186f18d73fdafa8d4a5e75ed55ed4a3_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 2</center><p>下面，通过一个例子简单说明卷积神经网络的结构。假设如下图3，m-l 是输入层，我们需要识别一幅彩色图像，这幅图像具有四个通道ARGB（透明度和红绿蓝，对应了四幅相同大小的图像），假设卷积核大小为100 <em> 100，共使用100个卷积核w1到w100（从直觉来看，每个卷积核应该学习到不同的结构特征）。用w1在ARGB图像上进行卷积操作，可以得到隐含层的第一幅图像；这幅隐含层图像左上角第一个像素是四幅输入图像左上角100</em>100区域内像素的加权求和，以此类推。同理，算上其他卷积核，隐含层对应100幅“图像”。每幅图像对是对原始图像中不同特征的响应。按照这样的结构继续传递下去。CNN中还有max-pooling等操作进一步提高鲁棒性。</p><p>在这个例子里，我们注意到输入层到隐含层的参数瞬间降低到了100 <em> 100 </em> 100=10`6个！这使得我们能够用已有的训练数据得到良好的模型。题主所说的适用于图像识别，正是由于CNN模型限制参数了个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/440765dbaab356739fb855834f901e7d_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 3卷积神经网络隐含层（摘自Theano教程）</center><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/c71cd39abe8b0dd29e229f37058404da_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 4一个典型的卷积神经网络结构，注意到最后一层实际上是一个全连接层（摘自Theano教程）</center><p>典型的卷积神经网络由3部分构成：</p><ul><li><strong>卷积层</strong>：负责提取图像中的局部特征</li><li><strong>池化层</strong>：大幅降低参数量级(降维)</li><li><strong>全连接层</strong>：类似传统神经网络的部分，用来输出想要的结果。</li></ul><h4 id="1）卷积：提取特征"><a href="#1）卷积：提取特征" class="headerlink" title="1）卷积：提取特征"></a>1）卷积：提取特征</h4><p>卷积层的运算过程如下图，用一个卷积核扫完整张图片：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/v2-6428cf505ac1e9e1cf462e1ec8fe9a68_720w.gif" alt=""></p><p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p><p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/Inkedv2-08a3c438b08715ce15592c7bd0d923ae_720w_LI.jpg" alt=""></p><p><strong>总结：卷积层的通过卷积核的过滤提取出图片中局部的特征，跟上面提到的人类视觉的特征提取类似。</strong></p><h4 id="2）池化层（下采样）：数据降维，避免过拟合"><a href="#2）池化层（下采样）：数据降维，避免过拟合" class="headerlink" title="2）池化层（下采样）：数据降维，避免过拟合"></a>2）池化层（下采样）：数据降维，避免过拟合</h4><p>池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/v2-15e89ec6a866be1f7130655527079786_720w.gif" alt=""></p><p>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p><p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p><p><strong>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</strong></p><h4 id="3）全连接层：输出结果"><a href="#3）全连接层：输出结果" class="headerlink" title="3）全连接层：输出结果"></a>3）全连接层：输出结果</h4><p>这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。</p><p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/575577-20190802141514879-1961811671.png" alt=""></p><p>典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：</p><p><strong>卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/1628932130(1" alt="">.jpg)</p><h4 id="4）相关重点"><a href="#4）相关重点" class="headerlink" title="4）相关重点"></a>4）相关重点</h4><p><strong>1、卷积神经网络有2大特点</strong></p><ul><li>能够有效的将大数据量的图片降维成小数据量</li><li>能够有效的保留图片特征，符合图片处理的原则</li></ul><p><strong>2、卷积神经网络的擅长处理领域</strong></p><p>卷积神经网络 – 卷积神经网络最擅长的就是图片的处理</p><p><strong>3、卷积神经网络*解决了什么问题？*</strong></p><p>在卷积神经网络出现之前，图像对于人工智能来说是一个难题，有2个原因：</p><ul><li>图像需要处理的数据量太大，导致成本很高，效率很低</li><li>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高</li></ul><p><strong>A.需要处理的数据量太大</strong></p><p>图像是由像素构成的，每个像素又是由颜色构成的。现在随随便便一张图片都是 1000×1000 像素以上的， 每个像素都有RGB 3个参数来表示颜色信息。假如我们处理一张 1000×1000 像素的图片，我们就需要处理3百万个参数！</p><blockquote><p><strong>1000×1000×3=3,000,000</strong></p></blockquote><p>这么大量的数据处理起来是非常消耗资源的，而且这只是一张不算太大的图片！</p><p><strong>卷积神经网络 – CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。</strong></p><p><strong>更重要的是：我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。</strong></p><p><strong>B.保留图像特征</strong></p><p>假如一张图像中有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，<strong>图像的内容（本质）并没有发生变化，只是位置发生了变化</strong>。</p><p>所以当我们移动图像中的物体，用传统的方式的得出来的参数会差异很大！这是不符合图像处理的要求的。</p><p><strong>而 CNN 解决了这个问题，他用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。</strong></p><h3 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3><p>全连接的DNN还存在着另一个问题——无法对时间序列上的变化进行建模。然而，<strong>样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要</strong>。对了适应这种需求，就出现了题主所说的另一种神经网络结构——<strong>循环神经网络RNN。</strong></p><p>在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络（Feed-forward Neural Networks）。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！表示成图就是这样的</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/bef6a6073d311e79cad53eb47757af9d_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 5 RNN网络结构</center><p>我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/c2eb9099048761fd25f0e90aa66d363a_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 6 RNN在世间上展开</center><p>完美，<strong>（t+1）时刻网络的最终结果O（t+1）是该时刻输入和所有历史共同作用的结果！</strong>这就达到了对时间序列建模的目的。</p><p>不知题主是否发现，RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，<strong>“梯度消失”现象又要出现了，只不过这次发生在时间轴上</strong>。对于 t 时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。</p><p>为了解决时间上的梯度消失，机器学习领域发展出了<strong>长短时记忆单元LSTM，通过门的开关实现时间上记忆功能，并防止梯度消失</strong>，一个LSTM单元长这个样子：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/a8f4582707b70d41f250fdf0a43812fb_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 7 LSTM单元</center>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>马在棋盘上的概率</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<h5 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h5><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/1.jpg" alt=""></p><p>大意的意思就是在一个棋盘上，马按照象棋中马走日的规则，可以选择走K此后，最后还是留在棋盘的概率。</p><h5 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h5><ul><li><p>首先一匹马在任意位置可以选择八个方向走动，称之为方向向量，分别为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int[] dx = &#123;-2, -1, 1, 2, 2, 1, -1, -2&#125;;    //方向导向数组</span><br><span class="line">int[] dy = &#123;1, 2, 2, 1, -1, -2, -2, -1&#125;;</span><br></pre></td></tr></table></figure></li><li><p>其次由于棋盘是有限的，所以如果选择的步伐超出了棋盘，则视为无效。</p></li><li><p>现在给个例子，模拟一下其的走动方位，N=4，k=3，r=0，c=0</p><ul><li><p>1、一开始的时候，这匹马所在的位置</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/2.jpg" alt=""></p></li><li><p>2、走第一步的时候后可以选择的落脚点：只有两个方向是可以选择的呢，其余的都是超出了棋盘的范围</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/1-2.jpg" alt=""></p><p>走完第一步后的矩阵如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/3.jpg" alt=""></p></li><li><p>3、仿照上述过程，第二步的走向如下</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/4.jpg" alt=""></p></li></ul></li></ul><pre><code>​     走完第二步后的矩阵如下：![](https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/5.jpg)​    </code></pre><ul><li><p>第三步可选的落脚点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/6.jpg" alt=""></p><p>走完第三步后的矩阵：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/7.jpg" alt=""></p></li><li><p>到这里，就已经完成了这匹马在棋盘上面的全部可能走法了，最后一个还有 2+2+6+6+2+2 = 20次还留着棋盘上，所以概率就为 20 / 8 <em> 8 </em> 8</p></li></ul><ul><li><p>状态转移方程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[r][c][steps]:表示马在位置（r，c）移动了 steps 次后还留在棋盘上的概率</span><br></pre></td></tr></table></figure><p>根据马的移动，得到如下递归方程 如下</p></li></ul><script type="math/tex; mode=display">dp[r][c][steps] = \sum_{dr，dc}dp[r+dr][c+dc][steps-1] / 8.0</script><p>dr，dc就是上面说的方向向量的数组，根据这个递归处理的方程，我们可以采取一个二维数组进行编写，即一新一旧，一步一步更新这些数组，最后求和即可</p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">//力扣上的主函数</span><br><span class="line">public double knightProbability(int N, int K, int r, int c) &#123;</span><br><span class="line">       double[][] dp_old = new double[N][N]; //dp_old数组，dp[x][y]表示第i次到达dp[x][y]的方案数</span><br><span class="line">       double[][] dp_new = new double[N][N]; //dp_new数组，dp[i][j]表示第i+1次到达dp[x][y]的方案数</span><br><span class="line">       //初始化</span><br><span class="line">       dp_old[r][c] = 1;   //一开始的位置</span><br><span class="line">       for(int i = 0; i &lt; K; i++)&#123; //K次</span><br><span class="line">           for (int x = 0; x &lt; N; x++) &#123;</span><br><span class="line">               for (int y = 0; y &lt; N; y++) &#123;</span><br><span class="line">                   //四面八方累加dp</span><br><span class="line">                   dp_new[x][y] = computeSumFromDirection(dp_old, x, y);</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           //更新两个数组</span><br><span class="line">           dp_old = dp_new;</span><br><span class="line">           dp_new = new double[N][N];</span><br><span class="line">       &#125;</span><br><span class="line">       //遍历这个数组的总和就是落在棋盘内所有格子的方案数</span><br><span class="line">       double in = 0;</span><br><span class="line">       for (int x = 0; x &lt; N; x++) &#123;</span><br><span class="line">           for (int y = 0; y &lt; N; y++) &#123;</span><br><span class="line">               in += dp_old[x][y];</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return in;</span><br><span class="line">   &#125;</span><br><span class="line">//每次的八个方向的走法，并且判断是否越界</span><br><span class="line">   public double computeSumFromDirection(double[][] dp_old, int x, int y)&#123;</span><br><span class="line">       double sum = 0;</span><br><span class="line">       int[] dx = &#123;-2, -1, 1, 2, 2, 1, -1, -2&#125;;    //方向导向数组</span><br><span class="line">       int[] dy = &#123;1, 2, 2, 1, -1, -2, -2, -1&#125;;</span><br><span class="line">       for (int i = 0; i &lt; dx.length; i++)&#123;</span><br><span class="line">           if(!check(dp_old, x + dx[i], y + dy[i])) continue;</span><br><span class="line">           sum += dp_old[x + dx[i]][y + dy[i]];</span><br><span class="line">       &#125;</span><br><span class="line">       return sum / 8.0;</span><br><span class="line">   &#125;</span><br><span class="line">//给定位置判断是否越界</span><br><span class="line">   public boolean check(double[][] dp_old, int x, int y)&#123;     //越界判断</span><br><span class="line">       return x &gt;= 0 &amp;&amp; x &lt; dp_old.length &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; dp_old[0].length;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>学习使我快乐！！！</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/8.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最长递增子序列</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划—两道有趣的题目"><a href="#动态规划—两道有趣的题目" class="headerlink" title="动态规划—两道有趣的题目"></a>动态规划—两道有趣的题目</h2><h3 id="one：eetcode-300最长递增子序列"><a href="#one：eetcode-300最长递增子序列" class="headerlink" title="one：eetcode 300最长递增子序列"></a>one：eetcode 300最长递增子序列</h3><p><a href="https://leetcode-cn.com/problems/longest-increasing-subsequence/submissions/">https://leetcode-cn.com/problems/longest-increasing-subsequence/submissions/</a></p><h4 id="1、题目大意"><a href="#1、题目大意" class="headerlink" title="1、题目大意"></a>1、题目大意</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/1.jpg" alt=""></p><h4 id="2、分析"><a href="#2、分析" class="headerlink" title="2、分析"></a>2、分析</h4><p>看到题目，判断某一个数组的最长递增子序列，<strong>注意并不是连续的</strong>。</p><h4 id="3、方法1：完全递归"><a href="#3、方法1：完全递归" class="headerlink" title="3、方法1：完全递归"></a>3、方法1：完全递归</h4><ul><li><p>现在假设下标 <strong>i</strong> 结尾的数组的最唱递增子序列为 max，</p><p>若nums[i+1]&gt;nums[i] ; 则下标 <strong>i+1</strong> 结尾的数组的最唱递增子序列为 max+1，否则为 max</p></li><li><p>所以这个题目是可以拆解子问题的，有子问题最后堆砌到最终答案</p></li><li><p>设 函数  <strong>fun(n,nums)</strong>  : 表示在数组nums下，以n作为下标的最大递增序列</p><p>得到递归方程 ：<strong>fun(n,nums) = fun(j,nums)+1</strong> 其中 <strong>0&lt;=j&lt;i</strong> 并且 <strong>dp[j]&lt;dp[i] j为【0，i】</strong>里面的任意值，需要遍历</p></li><li><p>拿下图为递归树（简约哈）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/2.png" alt=""></p></li><li><p><strong>代码：</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">//方法一完全递归</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //直接遍历以每个下标的结尾的最大递增序列，再取其中的最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = fmax(ans,fun(i,nums));</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line">//定义递归函数</span><br><span class="line">//fun(n,nums) : 表示在数组nums下，以n作为下标的最大递增序列</span><br><span class="line">//fun(n,nums) = fun(j,nums)+1 其中 0&lt;=j&lt;i 并且 dp[j]&lt;dp[i] j为[0，i]里面的任意值，需要遍历</span><br><span class="line">int fun(int n,int* nums)&#123;</span><br><span class="line">    //最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    //递归出口,下标为0，即是返回1</span><br><span class="line">    if(n == 0)&#123;</span><br><span class="line">        ans = 1;</span><br><span class="line">        return ans;</span><br><span class="line">    &#125;</span><br><span class="line">    //按照递归方程开始，开始递归求解 </span><br><span class="line">    for(int i=0;i&lt;n;i++)&#123;</span><br><span class="line">        if(nums[i]&lt;nums[n])&#123;</span><br><span class="line">            ans = fmax(ans,fun(i,nums)+1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然，这样递归的结果就是，提交超时，hhhhhhh。但是没关系，思路对了，下面进行优化。</p><h4 id="4、方法2：记忆化加递归"><a href="#4、方法2：记忆化加递归" class="headerlink" title="4、方法2：记忆化加递归"></a>4、方法2：记忆化加递归</h4><ul><li><p>在方法一的基础上，我们可以记录一个记忆化的数组，在递归刚刚开始的时候去判断这个数组是否有值，有的话直接递归返回了，若没有，则进行递归</p></li><li><p>再拿上一张图片来看，比如递归到数字 2 的时候，我们记录好递归到数字 2 的记忆数组值，当下次在别的树枝上需要递归数字 2 的时候，便可以直接取了，而不用继续遍历了，效率会高很多</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/2.png" alt=""></p></li><li><p><strong>代码</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">//方法二，在方法1的基础上：变为 递归+记忆化</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //记忆化数组建立</span><br><span class="line">    int remenber[numsSize];</span><br><span class="line">    memset(remenber, -1,sizeof(int)* numsSize);</span><br><span class="line">    //直接遍历以每个下标的结尾的最大递增序列，再取其中的最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = fmax(ans,fun1(i,nums,remenber));</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line">//定义递归函数</span><br><span class="line">//fun1(n,nums) : 表示在数组nums下，以n作为下标的最大递增序列</span><br><span class="line">//fun1(n,nums) = fun1(j,nums)+1  其中 0&lt;j&lt;n</span><br><span class="line">int fun1(int n,int* nums,int *remenber)&#123;</span><br><span class="line">    //先判断记忆化数组里面是否有这个值，有直接返回，不用继续递归了</span><br><span class="line">    if(remenber[n]!=-1)return remenber[n];</span><br><span class="line">    //最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    //递归出口,下标为0，即是返回1</span><br><span class="line">    if(n == 0)&#123;</span><br><span class="line">        ans = 1;</span><br><span class="line">        return ans;</span><br><span class="line">    &#125;</span><br><span class="line">    //按照递归方程开始，开始递归求解 </span><br><span class="line">    for(int i=0;i&lt;n;i++)&#123;</span><br><span class="line">        if(nums[i]&lt;nums[n])&#123;</span><br><span class="line">            ans = fmax(ans,fun1(i,nums,remenber)+1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //同时添加到记忆化数组</span><br><span class="line">    remenber[n] = ans;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这次提交，是可以通过的，足见时间效率上提高了可不少。</p><h4 id="5、方法3：动态规划"><a href="#5、方法3：动态规划" class="headerlink" title="5、方法3：动态规划"></a>5、方法3：动态规划</h4><ul><li><p>根据上述两个方法的分析的，可以很容易得到动态规划的状态转移方程</p></li><li><p>dp[i] : 表示以 i 为下标结尾的数组的最长递增字符串</p></li><li><p>状态转移方程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i] = max(dp[j]+1) 其中 0&lt;=j&lt;i 并且 dp[j]&lt;dp[i] j为[0，i]里面的任意值，需要遍历</span><br></pre></td></tr></table></figure></li><li><p><strong>代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //最后的最大值</span><br><span class="line">    int ansMax = 1;</span><br><span class="line">    //1、建立dp数组</span><br><span class="line">    int dp[numsSize];</span><br><span class="line">    memset(dp, 0, sizeof(int) * numsSize);</span><br><span class="line">    //2、递归遍历，封装dp数组</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        //记录遍历j属于【0，i】里面的最大值。初始值为1，表示本身</span><br><span class="line">        int max = 1;</span><br><span class="line">        for (int j = 0; j &lt; i;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(nums[j]&lt;nums[i]) max = fmax(max, dp[j] + 1);</span><br><span class="line">        &#125;</span><br><span class="line">        //给dp赋值</span><br><span class="line">        dp[i] = max;</span><br><span class="line">        //每次比较，取最大</span><br><span class="line">        ansMax = dp[i] &gt; ansMax ? dp[i] : ansMax;</span><br><span class="line">    &#125;</span><br><span class="line">    return ansMax;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个是比较常规的方法了：时间复杂度是 <strong>O（N*N）</strong></p></li></ul><h4 id="6、方法4：贪心-二分查找"><a href="#6、方法4：贪心-二分查找" class="headerlink" title="6、方法4：贪心+二分查找"></a>6、方法4：贪心+二分查找</h4><ul><li>考虑一个简单的贪心，如果我们要使上升子序列尽可能的长，则我们需要让序列上升得尽可能慢，因此我们希望每次在上升子序列最后加上的那个数尽可能的小。</li><li>基于上面的贪心思路，我们维护一个数组 dp[i] ，表示长度为 <strong>i</strong> 的最长上升子序列的末尾元素的最小值，用 <strong>len</strong> 记录目前最长上升子序列的长度，起始时<strong>len =1，d[1]=nums[0]</strong>。</li></ul><ul><li><p>由定义知dp数组必然是一个递增数组,  对原数组<strong>nums</strong>进行迭代, 依次判断每个数<strong>num</strong>将其插入dp数组相应的位置:</p><ol><li><strong>num &gt; dp[len]</strong>, 表示num比所有已知递增序列的尾数都大, 将num添加入dp 数组尾部, 并将最长递增序列长度len加1</li><li><strong>dp[i-1] &lt; num &lt;= dp[i]</strong>, 只更新相应的dp[i]=num</li></ol></li><li><p>以 nums=[4,10,3,8,9]：</p><p>1)第一步插入4，则 dp=[4]</p><p>2) 第二步插入10，则dp=[4，10]</p><p>3) 第三步插入3，原数组4的位置更新为3 则dp=[4，10]==》dp=[3，10]</p><p>4) 第四步插入8，原数组10的位置更新为8 则dp=[4，10]==》dp=[3，8]</p><p>5) 第五步插入9，则dp=[3，8，9] </p><p>所以最后的答案为 <strong>len(dp) = 3</strong>;</p></li><li><p><strong>代码：</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI3(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">int lengthOfLI(int* nums, int numsSize)&#123;</span><br><span class="line">    //1、建立dp</span><br><span class="line">    int dp[numsSize];</span><br><span class="line">    memset(dp, 0, sizeof(int) * numsSize);</span><br><span class="line">    int index = 0;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;   </span><br><span class="line">        //直接二分查找dp中的第一个大于等于nums[i]的值</span><br><span class="line">        int left = 0, right = index;</span><br><span class="line">        while(left&lt;right)&#123;</span><br><span class="line">            int mid = (left + right) / 2;</span><br><span class="line">            if(dp[mid]&lt;nums[i])left = mid + 1;</span><br><span class="line">            else right = mid;</span><br><span class="line">        &#125;</span><br><span class="line">        dp[left] = nums[i];</span><br><span class="line">        if(right == index) index++;</span><br><span class="line">    &#125;</span><br><span class="line">    return index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="two-：354-俄罗斯套娃"><a href="#two-：354-俄罗斯套娃" class="headerlink" title="two ：354 俄罗斯套娃"></a>two ：354 俄罗斯套娃</h3><p><a href="https://leetcode-cn.com/problems/russian-doll-envelopes/">https://leetcode-cn.com/problems/russian-doll-envelopes/</a></p><h4 id="1、题目大意-1"><a href="#1、题目大意-1" class="headerlink" title="1、题目大意"></a>1、题目大意</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/3.jpg" alt=""></p><h4 id="2、分析-1"><a href="#2、分析-1" class="headerlink" title="2、分析"></a>2、分析</h4><ul><li><p>根据题目的要求，如果我们选择了 k 个信封，它们的</p></li><li><p>宽度依次为 w0, w1, ···, w k-1 高度依次为 h0, h1,···, h k-1 ，那么需要满足如下了两个条件：</p><script type="math/tex; mode=display">W0<W1<···<Wk-1</script><script type="math/tex; mode=display">h0<h1<···<hk-1</script><p>同时控制 w 和 h 两个维度并不是那么容易，因此我们考虑固定一个维度，再在另一个维度上进行选择。例如，我们固定 w 维度，那么我们将数组<strong>envelopes</strong> 中的所有信封按照 w 升序排序。这样一来，我们只要按照信封在数组中的出现顺序依次进行选取，就一定保证满足：</p><script type="math/tex; mode=display">W0≤W1≤···≤Wk-1</script></li><li><p>然而小于等于 ≤ 和小于 &lt;还是有区别的，但我们不妨首先考虑一个简化版本的问题：</p><p>如果我们保证所有信封的 w 值互不相同，那么我们可以设计出一种得到答案的方法吗？</p><p>在 w 值互不相同的前提下，小于等于≤ 和小于 &lt; 是等价的，那么我们在排序后，就可以完全忽略 w 维度，只需要考虑 h 维度了。此时，我们需要解决的问题即为：</p><p>给定一个序列，我们需要找到一个最长的子序列，使得这个子序列中的元素严格单调递增，即上面要求的：</p><script type="math/tex; mode=display">h0<h1<···<hk-1</script><p>那么这个问题就是经典的「最长严格递增子序列」问题，问题得到解决，</p></li><li><p>当我们解决了简化版本的问题之后，我们来想一想使用上面的方法解决原问题，会产生什么错误？当 w 值相同时，如果我们不规定 h 值的排序顺序，那么可能会有如下的情况：</p><p>排完序的结果为 [(w, h)] = [(1, 1), (1, 2), (1, 3), (1, 4)][(w,h)]=[(1,1),(1,2),(1,3),(1,4)]，由于这些信封的 w 值都相同，不存在一个信封可以装下另一个信封，那么我们只能在其中选择 1 个信封。然而如果我们完全忽略 w 维度，剩下的 h 维度为 [1, 2, 3, 4][1,2,3,4]，这是一个严格递增的序列，那么我们就可以选择所有的 4 个信封了，这就产生了错误。</p><p>因此，我们必须要保证对于每一种 w 值，我们最多只能选择 1 个信封。</p><p>我们可以将 h 值作为排序的第二关键字进行降序排序，这样一来，对于每一种 w 值，其对应的信封在排序后的数组中是按照 h 值递减的顺序出现的，那么这些 h 值不可能组成长度超过 1 的严格递增的序列，这就从根本上杜绝了错误的出现。</p></li><li><p>因此我们就可以得到解决本题需要的方法：</p><ul><li><p>首先我们将所有的信封按照 w 值第一关键字升序、h 值第二关键字降序进行排序；</p></li><li><p>随后我们就可以忽略 w 维度，求出 h 维度的最长严格递增子序列，其长度即为答案。</p></li></ul></li></ul><ul><li>至此分析完了，归根到底就是<strong>最长递增子序列</strong>的问题了</li></ul><h4 id="3、代码"><a href="#3、代码" class="headerlink" title="3、代码"></a>3、代码</h4><p><strong><em>直接show code no say say</em></strong></p><ul><li><p>常规动态规划</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int* p[4];</span><br><span class="line">    int points[4][4] = &#123;&#123;5,4&#125;,&#123;6,4&#125;,&#123;6,7&#125;,&#123;2,3&#125;&#125;;</span><br><span class="line">    p[0] = &amp;points[0][0];</span><br><span class="line">    p[1] = &amp;points[1][0];</span><br><span class="line">    p[2] = &amp;points[2][0];</span><br><span class="line">    p[3] = &amp;points[3][0];</span><br><span class="line">    int envelopesSize = 4;</span><br><span class="line">    int envelopesColSize[] = &#123;2,2,2,2&#125;;</span><br><span class="line">    int ans = maxEnvelopes1(p,envelopesSize,envelopesColSize);</span><br><span class="line">    printf(&quot;%d&quot;, ans);</span><br><span class="line">&#125;</span><br><span class="line">//方法一：普通动态规划</span><br><span class="line">//dp[i]：表示以下标i为结尾的最大增序列  再次遍历取其最大</span><br><span class="line">//状态转移方程 dp[i] = dp[j]+1 其中 j&lt;i 并且排好序的envelopes中envelopes[j][1]     &lt;envelopes[i][1] </span><br><span class="line">int maxEnvelopes(int** envelopes, int envelopesSize, int* envelopesColSize)&#123;</span><br><span class="line">    //1、先排序，首先按照第一列升序排序，若第一列的值相同，则按照第二列的值降序排序</span><br><span class="line">    qsort(envelopes, envelopesSize, sizeof(int*), compare);</span><br><span class="line">    //2、构建dp数组</span><br><span class="line">    int dp[envelopesSize+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0]) * (envelopesSize + 1));</span><br><span class="line">    //3、根据状态转移方程，递推求dp</span><br><span class="line">    for (int i = 1; i &lt;= envelopesSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        //遍历【1，i】位置 找符合条件的最大dp[j]+1</span><br><span class="line">        int max = 1;</span><br><span class="line">        for (int j = 1; j &lt; i;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(envelopes[i-1][1]&gt;envelopes[j-1][1]) max = fmax(max, dp[j] + 1);</span><br><span class="line">        &#125;</span><br><span class="line">        //赋值给dp</span><br><span class="line">        dp[i] = max;</span><br><span class="line">    &#125;</span><br><span class="line">    //4、取其最大</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; sizeof(dp) / sizeof(dp[0]); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = dp[i] &gt; ans ? dp[i] : ans;</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//第一个参数升序 第二个参数降序</span><br><span class="line">int compare(const void *a, const void *b)&#123;</span><br><span class="line">    int* num1 = *(int**)a;</span><br><span class="line">    int* num2 = *(int**)b;</span><br><span class="line">    if(num1[0]==num2[0]) return num2[1] - num1[1];</span><br><span class="line"> else  return num1[0] - num2[0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>基于二分查找的动态规划</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int* p[4];</span><br><span class="line">    int points[4][4] = &#123;&#123;5,4&#125;,&#123;6,4&#125;,&#123;6,7&#125;,&#123;2,3&#125;&#125;;</span><br><span class="line">    p[0] = &amp;points[0][0];</span><br><span class="line">    p[1] = &amp;points[1][0];</span><br><span class="line">    p[2] = &amp;points[2][0];</span><br><span class="line">    p[3] = &amp;points[3][0];</span><br><span class="line">    int envelopesSize = 4;</span><br><span class="line">    int envelopesColSize[] = &#123;2,2,2,2&#125;;</span><br><span class="line">    int ans = maxEnvelopes1(p,envelopesSize,envelopesColSize);</span><br><span class="line">    printf(&quot;%d&quot;, ans);</span><br><span class="line">&#125;</span><br><span class="line">//方法二：基于二分查找的动态规划</span><br><span class="line">int maxEnvelopes1(int** envelopes, int envelopesSize, int* envelopesColSize) &#123;</span><br><span class="line">    if (envelopesSize == 0)  return 0;</span><br><span class="line">    //1、先排序，首先按照第一列升序排序，若第一列的值相同，则按照第二列的值降序排序</span><br><span class="line">    qsort(envelopes, envelopesSize, sizeof(int*), compare);</span><br><span class="line">    //2、构建dp数组</span><br><span class="line">    int dp[envelopesSize], indexSize = 0;</span><br><span class="line">    dp[indexSize++] = envelopes[0][1];</span><br><span class="line">    for (int i = 1; i &lt; envelopesSize; ++i) &#123;</span><br><span class="line">        int num = envelopes[i][1];</span><br><span class="line">        if (num &gt; dp[indexSize - 1])  dp[indexSize++] = num;</span><br><span class="line">        else &#123;</span><br><span class="line">            //在dp中寻找第一个大于等于num的值的下标，进而替换她</span><br><span class="line">            int index = lower_bound(dp, indexSize, num);</span><br><span class="line">            dp[index] = num;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return indexSize;</span><br><span class="line">&#125;</span><br><span class="line">//二分查找</span><br><span class="line">int lower_bound(int* arr, int arrSize, int val) </span><br><span class="line">&#123;</span><br><span class="line">    int left = 0, right = arrSize - 1;</span><br><span class="line">    while (left &lt;= right) &#123;</span><br><span class="line">        int mid = (left + right) &gt;&gt; 1;</span><br><span class="line">        if (val &lt; arr[mid])  right = mid - 1;</span><br><span class="line">        else if (val &gt; arr[mid]) left = mid + 1;</span><br><span class="line">        else  return mid;</span><br><span class="line">    &#125;</span><br><span class="line">    if (arr[left] &gt;= val)  return left;</span><br><span class="line">    return -1;</span><br><span class="line">&#125;</span><br><span class="line">//第一个参数升序 第二个参数降序</span><br><span class="line">int compare(const void *a, const void *b)&#123;</span><br><span class="line">    int* num1 = *(int**)a;</span><br><span class="line">    int* num2 = *(int**)b;</span><br><span class="line">    if(num1[0]==num2[0])&#123;</span><br><span class="line">        return num2[1] - num1[1];</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return num1[0] - num2[0];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="summary："><a href="#summary：" class="headerlink" title="summary："></a>summary：</h3><p>套路：遇到这种数组这种问题，经常想到以 <strong>下标i</strong> 为结尾作为的子问题，直接定义 <strong>dp【i】</strong>：为以 <strong>i</strong> 作为下标结尾的数组怎么怎么。。。。 </p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/4.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
            <tag> 子序列问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BFS和DFS模板</title>
      <link href="%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/BFS%E5%92%8CDFS%E6%A8%A1%E6%9D%BF/"/>
      <url>%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/BFS%E5%92%8CDFS%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<p><strong>首先，总结一定的模板做题是十分有作用的，善于总结才是我们加强算法能力的表现。做总结可以提高我们的代码能力，可以比较快速解决算法问题，也会更加清晰算法的流程！！十分有必要！！</strong></p><p><strong>BFS的模板：</strong></p><ul><li>1、如果不需要确定当前遍历到了哪一层，BFS 模板如下。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while queue 不空：</span><br><span class="line">    cur = queue.pop()</span><br><span class="line">    for 节点 in cur的所有相邻节点：</span><br><span class="line">        if 该节点有效且未访问过：</span><br><span class="line">            queue.push(该节点)</span><br></pre></td></tr></table></figure><ul><li>2、如果要确定当前遍历到了哪一层，BFS 模板如下。 这里增加了 level 表示当前遍历到二叉树中的哪一层了，也可以理解为在一个图中，现在已经走了多少步了。size 表示在当前遍历层有多少个元素，也就是队列中的元素数，我们把这些元素一次性遍历完，即把当前层的所有元素都向外走了一步。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">level = 0</span><br><span class="line">while queue 不空：</span><br><span class="line">    size = queue.size()</span><br><span class="line">    while (size --) &#123;</span><br><span class="line">        cur = queue.pop()</span><br><span class="line">        for 节点 in cur的所有相邻节点：</span><br><span class="line">            if 该节点有效且未被访问过：</span><br><span class="line">                queue.push(该节点)</span><br><span class="line">    &#125;</span><br><span class="line">    level ++;</span><br></pre></td></tr></table></figure><p><strong>DFS模板（回溯）</strong></p><ul><li>1、最本质的法宝是“画图”，千万不能偷懒，拿纸和笔“画图”能帮助我们更好地分析递归结构，这个“递归结构”一般是“树形结构”，而符合题意的解正是在这个“树形结构”上进行一次“深度优先遍历”，这个过程有一个形象的名字，叫“搜索”；我们写代码也几乎是“看图写代码”，所以“画树形图”很重要。</li><li>2、然后使用一个状态变量，一般我习惯命名为 path、pre ，在这个“树形结构”上使用“深度优先遍历”，根据题目需要在适当的时候把符合条件的“状态”的值加入结果集；这个“状态”可能在叶子结点，也可能在中间的结点，也可能是到某一个结点所走过的路径。</li><li>3、在某一个结点有多个路径可以走的时候，使用循环结构。当程序递归到底返回到原来执行的结点时，“状态”以及与“状态”相关的变量需要“重置”成第 1 次走到这个结点的状态，这个操作有个形象的名字，叫“回溯”，“回溯”有“恢复现场”的意思：意即“回到当时的场景，已经走过了一条路，尝试走下一条路”。第 2 点中提到的状态通常是一个列表结构，因为一层一层递归下去，需要在列表的末尾追加，而返回到上一层递归结构，需要“状态重置”，因此要把列表的末尾的元素移除，符合这个性质的列表结构就是“栈”（只在一头操作）。</li><li>4、当我们明确知道一条路走不通的时候，例如通过一些逻辑计算可以推测某一个分支不能搜索到符合题意的结果，可以在循环中 continue 掉，这一步操作叫“剪枝”。“剪枝”的意义在于让程序尽量不要执行到更深的递归结构中，而又不遗漏符合题意的解。因为搜索的时间复杂度很高，“剪枝”操作得好的话，能大大提高程序的执行效率。“剪枝”通常需要对待搜索的对象做一些预处理，例如第 47 题、第 39 题、第 40 题、第 90 题需要对数组排序。“剪枝”操作也是这一类问题很难的地方，有一定技巧性。总结一下：“回溯” = “深度优先遍历” + “状态重置” + “剪枝”，写好“回溯”的前提是“画图”。因此，非要写一个模板，我想它可能长这个样子：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def backtrack(待搜索的集合, 递归到第几层, 状态变量 1, 状态变量 2, 结果集):</span><br><span class="line">    # 写递归函数都是这个套路：先写递归终止条件</span><br><span class="line">    if 可能是层数够深了:</span><br><span class="line">        # 打印或者把当前状态添加到结果集中</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    for 可以执行的分支路径 do           //分支路径</span><br><span class="line">        </span><br><span class="line">        # 剪枝</span><br><span class="line">        if 递归到第几层, 状态变量 1, 状态变量 2, 符合一定的剪枝条件:</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line">        对状态变量状态变量 1, 状态变量 2 的操作（#）</span><br><span class="line">   </span><br><span class="line">        # 递归执行下一层的逻辑</span><br><span class="line">        backtrack(待搜索的集合, 递归到第几层, 状态变量 1, 状态变量 2, 结果集)</span><br><span class="line"></span><br><span class="line">        对状态变量状态变量 1, 状态变量 2 的操作（与标注了 # 的那一行对称，称为状态重置）</span><br><span class="line">        </span><br><span class="line">    end for</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小tips </tag>
            
            <tag> DFS </tag>
            
            <tag> BFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>背包问题</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划（Dynamic Programming，简称DP）动态规划常常适用于有<strong>重叠子问题</strong>和<strong>最优子结构</strong>性质的问题，动态规划方法所耗时间往往远少于朴素解法。</p><p>动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。 通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量： 一旦某个给定子问题的解已经算出，则将其<strong>记忆化</strong>存储，以便下次需要同一个子问题解之时直接查表。 这种做法在重复子问题的数目关于输入的规模呈<strong>指数增长</strong>时特别有用。虽然抽象后进行求解的思路并不复杂，但具体的形式千差万别，找出问题的子结构以及通过子结构重新构造最优解的过程很难统一，为了解决动态规划问题，只能靠多练习、多思考了。</p><p><strong><em>\</em>动态规划问题满足三大重要性质**</strong></p><p><strong>最优子结构性质：</strong>如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态规划算法解决问题提供了重要线索。</p><p><strong>子问题重叠性质：</strong>子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。</p><p><strong>无后效性</strong>：将各阶段按照一定的次序排列好之后，对于某个给定的阶段状态，它以前各阶段的状态无法直接影响它未来的决策，而只能通过当前的这个状态。换句话说，每个状态都是过去历史的一个完整总结。这就是无后向性，又称为无后效性。</p><p><strong><em>重点</em></strong>   dp数组的含义 + 状态转移方程 （具体问题具体分析）</p><p><strong>首先背包问题是我们接触动态规划比不可取的经典问题，重要的问题说三遍，经典经典经典。</strong></p><ul><li>0-1背包问题</li><li>完全背包问题</li><li>多重背包问题</li></ul><h2 id="0-1背包问题"><a href="#0-1背包问题" class="headerlink" title="0-1背包问题"></a>0-1背包问题</h2><h3 id="1、题目描述"><a href="#1、题目描述" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p>​    这里你有N件物品和一个容量为M的背包，这N件物品的第 i 件物品的 价值是V[i] ，重量是W[i].问题是拿取这N件物品的哪几件时，使得背包可以装下（<u>意思就是物品的重量总和小于或等于M</u>）且价值最大。</p><p><strong>关键问题</strong>：其实这堆物品在你选择的时候无非就是两种路子可以选择：<strong>选 or 不选</strong></p><p>第一步：构建dp数组的含义，<code>dp[i][j]</code>  ：代表的是前 <strong>i</strong> 个物品加入容量为 <strong>j</strong> 的背包里面价值总和的最大值</p><p>第二步：分析状态转移方程</p><p>​    对于一个物品来说：要么选要么不选，</p><ul><li><p>选择这个物品：就是第<strong>i</strong>件物品放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j-w[i]] + V[i]   （j&gt;W[i]）</span><br></pre></td></tr></table></figure></li><li><p>不选择这个物品：就是舍弃第<strong>i</strong>件物品，不放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j]</span><br></pre></td></tr></table></figure><p>经过这两步的分析，可以得出这个问题的状态转移方程（即为重要，very very important）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i-1][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 4; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,2,4,3,7&#125;; //物品的价值 前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,5,5&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(j&lt;W[i])&#123;</span><br><span class="line">                dp[i][j] = dp[i - 1][j];  //装不下第i件物品，只能不要咯</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                dp[i][j] = fmax(dp[i - 1][j], dp[i - 1][j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、深入分析理解"><a href="#3、深入分析理解" class="headerlink" title="3、深入分析理解"></a>3、深入分析理解</h3><p>这个代码就是自下而上的方法，思路也是比较简单，就是不断遍历，不断填充dp表：</p><p>第一：初始化时候的表格：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti1.jpg" alt=""></p><p>第二：当 <strong>i</strong>=1的时候，只有物品1能够选择，如果背白容量够的话，那么此时的最大价值就是物品1的价值了</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti2.jpg" alt=""></p><p>第三：当<strong>i</strong>=2的时候，根据状态转移方程，此时取<strong>i</strong>=2，<strong>j</strong>=3的时候有如下转换：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti3.jpg" alt=""></p><p>最后，根据这样的规则：逐一填表得：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti4.jpg" alt=""></p><p>这样，就可以得到最后的结果：13了，我们也可以根据状态转移方程方向得到选择的物品是第1 2 4号物品。</p><p>到此，分析时间复杂度为填表的时间为<strong>O(N*M)</strong>  ， 空间复杂度为<strong>O(N*M)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti5.jpg" alt=""></p><h3 id="4、优化"><a href="#4、优化" class="headerlink" title="4、优化"></a>4、优化</h3><p>在这个问题上，其实时间上没什么好优化的了，只能从空间上进行一点优化 ，</p><p>首先我们再看看状态转移方程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i-1][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti6.jpg" alt=""></p><p>可以明显看出，在填<strong>i+1</strong>行的数据的时候，只用到了第<strong>i</strong>行的数据，根本就没有用到<strong>i-1</strong>行的数据，换句话说，填某一行的数据的时候只与其前一行有关，根据这个规律，我们就可以使用将二维dp降为一维dp，缩减空间。此所谓滚动数组。</p><p><strong>总结</strong>：<code>dp[i][j]</code>所依赖的值必须是没有更新的，所以后到前。（ps：完全背包正好相反）</p><p>此时状态转移方程  <code>dp[j]</code> : 表示容量不超过 <strong>j</strong> 的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[j] = Max(dp[j],dp[j-w[i]] + V[i])  j&gt;W[i]</span><br></pre></td></tr></table></figure><p><strong>代码实现：</strong></p><p>和上面的代码有一点区别，在填充dp数组的第二层循环的时候，不应该从前到后（左到右），而应该从后到前（右到左），因为如果选择从前到后（左到右），会导致前面的值被修改，而后面的的值确实依赖前面的值的，要保证后面值得依赖是不变了。所以在第二轮扫描得时候需要从后到前扫描。（下图看做一行滴数据哈）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti7.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 4; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,2,4,3,7&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,5,5&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化得</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = M; j &gt;= W[i]; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时时间复杂度 <strong>O(N*M）</strong>    空间复杂度<strong>O(M)</strong>   空间复杂度优化了挺多哦</p><h2 id="完全背包问题"><a href="#完全背包问题" class="headerlink" title="完全背包问题"></a>完全背包问题</h2><h3 id="1、题目描述-1"><a href="#1、题目描述-1" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p> 有N种物品和一个容量为M的背包，每种物品都就可以选择任意多个，第<strong>i</strong>种物品的价值为 V[i]，重量是 W[i]，求解：选哪些物品放入背包，可因使得这些物品的价值最大，并且体积总和不超过背包容量。</p><p><strong>分析：</strong></p><p>完全背包问题是在0-1背包问题的基础上略有不同，不同的是在0-1背包问题中，某一件物品要么取一件要么不取，但是在完全背包的问题中，某一件物品可以无限（任意）的取。</p><p>从物品的选择角度说也不是 <strong>选 OR 不选 </strong>的问题了，而是选 0 1 2 3 4 ，，，件的问题了，</p><p>默默嘀咕：曾经的我刚刚 接触的时候，贪心（有手就行），事实证明我还是年轻，打扰了，贪心解决不了。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti8.jpg" alt=""></p><p><strong>动态规划方法：</strong></p><p>第一步：构建dp数组的含义，<code>dp[i][j]</code>  ：代表的是前 <strong>i</strong> 个物品加入容量为 <strong>j</strong> 的背包里面价值总和的最大值</p><p>第二步：分析状态转移方程</p><p>​    对于一件新的物品来说：可以选可以不选，</p><ul><li><p>选择这个新物品：此时（拿了 <strong>i</strong> 号物品，我们还是继续拿 <strong>i</strong> 号物品）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i][j-w[i]] + V[i]   （j&gt;W[i]）</span><br></pre></td></tr></table></figure></li><li><p>不选择这个物品：就是舍弃全部的第 <strong>i</strong> 件物品，不放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j]</span><br></pre></td></tr></table></figure><p>经过这两步的分析，可以得出这个问题的状态转移方程（即为重要，very very important）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、代码实现-1"><a href="#2、代码实现-1" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,5,8&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//没有优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(j&lt;W[i])&#123;</span><br><span class="line">                dp[i][j] = dp[i - 1][j];  //装不下第i件物品，只能不要咯</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                //可以装下，可以选择 拿或者不拿  只是将 0-1背包问题中的 i-1 改为 i</span><br><span class="line">                dp[i][j] = fmax(dp[i - 1][j], dp[i][j - W[i]] + V[i]); </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注解</strong> ： 其实这边也还可以利用另外一个状态转移方程解决（自个摸索把）直接给出答案</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">f[i][j]=max(f[i-1][j-k*V[i]]+k*W[i],f[i][j])     0&lt;=k*c[i]&lt;=j</span><br><span class="line"></span><br><span class="line">代码：</span><br><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;5,8&#125;; //物品的价值</span><br><span class="line">    int W[] = &#123;5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//f[i][j]=max(f[i-1][j-k*V[i]]+k*W[i],f[i][j])     0&lt;=k*c[i]&lt;=j</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">            for (int j = 0; j &lt;= M; j++)&#123;</span><br><span class="line">                for (int k = 0; k * V[i] &lt;= j; k++)&#123;</span><br><span class="line">                    dp[i+1][j] = fmax(dp[i+1][j], dp[i][j-k * V[i]] + k * W[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、深入分析理解-1"><a href="#3、深入分析理解-1" class="headerlink" title="3、深入分析理解"></a>3、深入分析理解</h3><p>这边也可以画出表格来一步一步填这个表格的问题，自底向上，</p><p>第一步：初始化时的表格：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti9.jpg" alt=""></p><p>第二步：在第 <strong>i</strong> 个物品的时候，我们其实可以选择上一层中的几个位置中价值最高的那一个，在这里M=10，所以只需要将两个数值进行比较，如果M大于10，那么就需要将取0个、1个和两个i2物品的情况进行比较，然后选出最大值.</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti10.jpg" alt=""></p><p>到此，分析时间复杂度为填表的时间为<strong>O(N*M)</strong>  ， 空间复杂度为<strong>O(N*M)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti11.jpg" alt=""></p><h3 id="4、优化-1"><a href="#4、优化-1" class="headerlink" title="4、优化"></a>4、优化</h3><p>优化思路和0-1背包问题一模一样的，就不在这里赘述了，直接上状态方程和代码。</p><p><strong>总结</strong>：<code>dp[i][j]</code>所依赖的值必须是已经更新的，所以前到后。（ps：0-1背包正好相反）</p><p><strong>状态转移方程为</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f[j]=max(f[j-w[i]]+c[i], f[j]);</span><br></pre></td></tr></table></figure><p><strong>代码实现</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,5,8&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax1(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化得</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = W[i]; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时时间复杂度 <strong>O(N*M）</strong>    空间复杂度<strong>O(M)</strong>   空间复杂度也是变成线性的了。</p><p><strong>注解</strong>：其实还有一个小优化</p><p>比如两件物品 ：<strong>i</strong>   <strong>j</strong>    当  <strong>i</strong> 物品的重量比 <strong>j</strong> 的重，但是 <strong>i</strong> 的价值确比 <strong>j</strong> 的低，那我们岂不是可以直接跳过 <strong>i</strong> 了，直接选择 <strong>j</strong> 物品了。道理很简单，难道这世界上会有人去买一个又贵又难吃的东西？（富豪除外）</p><h2 id="多重背包问题"><a href="#多重背包问题" class="headerlink" title="多重背包问题"></a>多重背包问题</h2><h3 id="1、题目描述-2"><a href="#1、题目描述-2" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p>有N种物品和一个容量为V的背包。第i种物品最多有n[i]件可用，每件费用是w[i]，价值是c[i]。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。</p><p><strong>分析</strong></p><p>这里既不像0-1背包每种物品只有1件，也不像完全背包那样每种物品有无数件，而是限定来了每种物品的数量，并不是你想取多少就取多少，得看看人家有没有。</p><p>经过前面两个的分析，这个多重背包问题得状态转移方程和完全背包的状态转移方程岂不是一个爹娘的样子，<strong>就是K多了一个限制</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i,j] = max(dp(i-1, j - V[i] * k) + P[i] * k); （0 &lt;= k * V[i] &lt;= j &amp;&amp; 0 &lt;= k &lt;= n[i]）</span><br></pre></td></tr></table></figure><h3 id="2、代码实现-2"><a href="#2、代码实现-2" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[],int n[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 3; //不同物品的数量</span><br><span class="line">    int M = 15; //背包的容量</span><br><span class="line">    int V[] = &#123;3,4,5&#125;; //物品的价值</span><br><span class="line">    int W[] = &#123;2,3,4&#125;; //物品的重量</span><br><span class="line">    int n[] = &#123;4, 3, 2&#125;;//每种物品的个数</span><br><span class="line">    int re = seekMax(N, M, V, W,n);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//没有优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[],int n[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">            for (int j = 0; j &lt;= M; j++)&#123;</span><br><span class="line">                //k限制了条件 不加限制就是我上面讲的完全背包问题中在我没有优化的时候提出的另外一个方程的解</span><br><span class="line">                for (int k = 0; k &lt;= n[i] &amp;&amp; k * V[i] &lt;= j; k++)&#123;  </span><br><span class="line">                    dp[i+1][j] = fmax(dp[i+1][j], dp[i][j-k * V[i]] + k * W[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分析就不再分析了，和前面的一模一样的</p><h3 id="3、优化"><a href="#3、优化" class="headerlink" title="3、优化"></a>3、优化</h3><ul><li>优化1：这边优化也可以项0-1背包和完全背包的样子，将dp二维数组改为一维的dp数组，改为滚动数组，这里不再赘述</li><li>优化2：这里有一个比较巧妙的方法，完美将多重背包问题顺利转为0-1背包的问题了。下面会详细讲解这个优化（鄙人比较pick）</li></ul><p>举个例子：比如有一种物品，她一共有8件，我们再取的时候可以取得 0 1 2 3 4 5 6 7 8 件这九种情况，但是我们在取得时候是不知道该取多少件得，这时候我们可以把这八件物品分堆，使得我们可以取得上述得九种情况的任意一种，所以分堆便是重点了，这里分堆采用2进制的方法进行分堆</p><p><strong>统一</strong>：分为 1 2 4 8 。。。总的减去前面的总和（因为最后一个并不一定是2的整数次幂）</p><script type="math/tex; mode=display">n = 2^0 + 2^1 + 2^2 + 2^3...+ 2^h+(n-2^c+1)       （其中 h=c-1）</script><p>例如八件同一件物品：分为大小为 1 2 4 1 的四个堆即可，任意组合可以得到上述选择的九种可能，此时将这四个堆想象成0-1背包问题种的不一样的物品即可。 </p><p>状态转移方程和0-1背包相同</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[],int n[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 3; //不同物品的数量</span><br><span class="line">    int M = 15; //背包的容量</span><br><span class="line">    int V[] = &#123;0,3,4,5&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,4&#125;; //物品的重量</span><br><span class="line">    int n[] = &#123;0,4,3,2&#125;;//每种物品的个数</span><br><span class="line">    int re = seekMax(N, M, V, W,n);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[],int n[])</span><br><span class="line">&#123;</span><br><span class="line">    //创建分堆后的价值和重量数组 这个大小可以根据题目给的数据范围来确定</span><br><span class="line">    int newW[N * 20];</span><br><span class="line">    int newV[M * 20]; </span><br><span class="line">    newW[0] = 0;</span><br><span class="line">    newV[0] = 0;</span><br><span class="line">    //先分堆 完善上面两个数组</span><br><span class="line">    int number = 0; //分堆后的总堆数</span><br><span class="line">    for (int i = 1; i &lt;= N;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= n[i];j *= 2)</span><br><span class="line">        &#123;</span><br><span class="line">            number++;</span><br><span class="line">            newW[number] = W[i] * j;</span><br><span class="line">            newV[number] = V[i] * j;</span><br><span class="line">            n[i] -= j;</span><br><span class="line">        &#125;</span><br><span class="line">        //最后那个</span><br><span class="line">        if(n[i]&gt;0)&#123;</span><br><span class="line">            number++;</span><br><span class="line">            newW[number] = W[i] * n[i];</span><br><span class="line">            newV[number] = V[i] * n[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= number; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = M; j &gt;= newW[i]; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - newW[i]] + newV[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此三个经典的背包问题解决啦。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti13.jpg" alt=""></p><h2 id="混合背包问题"><a href="#混合背包问题" class="headerlink" title="混合背包问题"></a>混合背包问题</h2><p>所谓混合背包的问题无非就是前面三种背包的杂糅操作，比如有的物品符合0-1背包（只能够取1件或者不取），有的物品符合完全背包问题（一件物品能够取任意件），有的物品符合多重背包问题（一种物品只能怪取限定件）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">伪代码：</span><br><span class="line">for(i = 0 ;i&lt;N;i++)&#123;</span><br><span class="line">if i属于0-1背包问题 </span><br><span class="line">采用0-1解决方法</span><br><span class="line">else if i属于完全背包问题</span><br><span class="line">采用完全解决方法</span><br><span class="line">else if i属于多重背包问题</span><br><span class="line">采用多重解决方法</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti12.jpg" alt=""></p><p>上述三种经典背包问题可谓了解动态规划的经典之作了，其实三种背包问题 都是有着异曲同工之妙呀，认真解读会让自己了解的更加深刻，舒服。其实关于背包问题的变形变异还有很多类似的题目，后续加以继续撸。。。鄙人不才，若有误望指正，本文章也是采取一些其他博客的思路，谢谢各路大神。</p><p>解决拥有子问题的问题，可以有三个方法</p><ul><li>朴素递归 （效率很低）</li><li>递归 + 记忆化 （效率较高）</li><li>递推完善dp数组（效率最高）动态规划常用</li></ul><p><strong><em>重点</em></strong>   dp数组的含义 + 状态转移方程 （具体问题具体分析）</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><p>[]: <a href="https://blog.csdn.net/woshi250hua/article/details/7636866">https://blog.csdn.net/woshi250hua/article/details/7636866</a></p><p>这位好心的博主列举了一些背包模型的例子，可以利于自己继续练习</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti21.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
            <tag> 背包问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程中无穷大常量(ox3f3f3f3f)的设定技巧</title>
      <link href="%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B%E4%B8%AD%E6%97%A0%E7%A9%B7%E5%A4%A7%E5%B8%B8%E9%87%8F-ox3f3f3f3f-%E7%9A%84%E8%AE%BE%E5%AE%9A%E6%8A%80%E5%B7%A7/"/>
      <url>%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B%E4%B8%AD%E6%97%A0%E7%A9%B7%E5%A4%A7%E5%B8%B8%E9%87%8F-ox3f3f3f3f-%E7%9A%84%E8%AE%BE%E5%AE%9A%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h4 id="编程中无穷大常量的设定技巧"><a href="#编程中无穷大常量的设定技巧" class="headerlink" title="编程中无穷大常量的设定技巧"></a>编程中无穷大常量的设定技巧</h4><p>首先，在做某一些算法的时候，会很常求最大最小值一类的问题，通常我们会设置一个初始一个answer最大int类型的最大或者最小，然后每次比较取大取小即可。</p><p>其实如果问题中各数据的范围明确，那么无穷大的设定不是问题，在不明确的情况下，很多程序员都取0x7fffffff作为无穷大，因为这是32-bit int的最大值。如果这个无穷大只用于一般的比较（比如求最小值时min变量的初值），那么0x7fffffff确实是一个完美的选择，但是在更多的情况 下，0x7fffffff并不是一个好的选择。理由如下：</p><p>很多时候我们并不只是单纯拿无穷大来作比较，而是会运算后再做比较，例如在大部分最短路径算法中都会使用的松弛操作：<br>if (d[u]+w[u][v]&lt;d[v]) d[v]=d[u]+w[u][v];<br>我们知道如果u,v之间没有边，那么w[u][v]=INF，如果我们的INF取0x7fffffff，那么d[u]+w[u][v]会溢出而变成负数， 我们的松弛操作便出错了，更一般的说，0x7fffffff不能满足“无穷大加一个有穷的数依然是无穷大”，它变成了一个很小的负数。</p><p>计算机不会表示出“无穷大”的概念，所以我们只能以一个定值来表示“最大”。那么使用什么值呢？</p><p>32-bit int举例，我们选择的最大应该满足两个条件</p><ul><li><strong>这个最大值真的很大，是和定义的最大值是同一个数量级的</strong></li><li><strong>这个最大值+这个最大值并不会溢出的，也就是无穷大嘉无穷大依然是无穷大</strong></li></ul><p>所以我们需要一个更好的家伙来顶替 0x7fffffff ，最严谨的办法当然是对无穷大进行特别处理而不是找一个很大很大的常量来代替它（或者说模拟 它），但是这样会让我们的编程过程变得很麻烦。</p><p>在我看的大佬上面，最精巧的无穷大常量取值是 0x3f3f3f3f，我不知道是谁最先开始使用这个精妙的常 量来做无穷大，自己也是学以致用，你还别说发现非常好用，而当我对这个常量做更深入的分析时，就发现它真的是非常精巧了。</p><p> 第一、0x3f3f3f3f的十进制是1061109567，也就是10^9级别的（和 0x7fffffff一个数量级），而一般场合下的数据都是小于10^9的，所以它可以作为无穷大使用而不致出现数据大于无穷大的情形。</p><p>第二、由于一般的数据都不会大于10^9，所以当我们把无穷大加上一个数据时，它并不会溢出（这就满足了“无穷大加一个有穷的数依然是无穷 大”），事实上 0x3f3f3f3f + 0x3f3f3f3f = 2122219134，这非常大但却没有超过32-bit int的表示范围，所以 0x3f3f3f3f 还满足了我们“无穷大加无穷大还是无穷大”的需求。</p><p>第三、0x3f3f3f3f还能给我们带来一个意想不到的额外好处：如果我们想要将某个数组清零，我们通常会使用 <code>memset(a,0,sizeof(a))</code>这样的代码来实现（方便而高效），但是当我们想将某个数组全部赋值为无穷大时（例如解决图论问题时邻接矩阵的 初始化），就不能使用memset函数而得自己写循环了（写这些不重要的代码真的很痛苦），我们知道这是因为 memset 是按字节操作的，它能够对数组清 零是因为0的每个字节都是0，现在好了，如果我们将无穷大设为 0x3f3f3f3f，那么奇迹就发生了，0x3f3f3f3f 的每个字节都是0x3f！所 以要把一段内存全部置为无穷大，我们只需要<code>memset(a,0x3f,sizeof(a))</code>。所以在通常的场合下，0x3f3f3f3f 真的是一个非常棒的选择。</p><p>补充：memset以字节为单位进行填充，可以全部置为0，-1，和某个int类型的值四个字节都是一样的表示的数值，别问问就是巧合！！</p>]]></content>
      
      
      <categories>
          
          <category> 算法总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小tips </tag>
            
            <tag> 常量设定 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tcp详解</title>
      <link href="%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcp%E8%AF%A6%E8%A7%A3/"/>
      <url>%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcp%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、tcp协议的特点"><a href="#1、tcp协议的特点" class="headerlink" title="1、tcp协议的特点"></a>1、tcp协议的特点</h4><p>TCP是在不可靠的IP层之上实现的可靠的数据传输协议，它主要解决传输的可靠、有序、无丢失和不重复问题。TCP 是TCP/IP 体系中非常复杂的一个协议，主要特点如下：</p><ul><li><p>TCP 是面向连接的传输层协议。</p></li><li><p>每条TCP 连接只能有两个端点，每条TCP 连接只能是点对点的（一对一）。</p></li><li><p>TCP 提供可靠的交付服务，保证传送的数据无差错、不丢失、不重复且有序。</p><ul><li><p>如何保证数据无差错、不丢失、不重复且有序的？有哪些机制来保证？</p><p>答：TCP 使用了校验、序号、确认和重传等机制来达到这一目的。</p></li></ul></li><li><p>TCP 提供全双工通信，允许通信双方的应用进程在任何时候都能发送数据，为此TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据。</p><ul><li><p>为什么需要设置缓存，缓存的作用？ </p><p>答：发送缓存用来暂时存放以下数据：1.发送应用程序传送给发送方TCP 准备发送的数据；2.TCP已发送但尚未收到确认的数据。</p><p>接收缓存用来暂时存放以下数据：1.按序到达但尚未被接收应用程序读取的数据；2.不按序到达的数据。</p></li></ul></li><li><p>TCP是面向字节流的，虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅视为一连串的无结构的字节流。</p><ul><li>一个字节占一个序号，每个报文段用第一个字节的序号来标识,例如，一报文段的序号字段值是301, 而携带的数据共有l00B, 表明本报文段的数据的最后一个字节的序号是400, 因此下一个报文段的数据序号应从401开始，也就是期望的下一个序号（确认号）。</li></ul></li></ul><h4 id="2、tcp报文段格式"><a href="#2、tcp报文段格式" class="headerlink" title="2、tcp报文段格式"></a>2、tcp报文段格式</h4><p><img src="https://www.hualigs.cn/image/60a3a18937d44.jpg" alt=""></p><p>部分字段解释：</p><p>1) 序号字段（就是seq）：序号字段的值指的是本报文段所发送的数据的第一个字节的序号。</p><p>2) 确认号字段（就是ack）：是期望收到对方的下一个报文段的数据的第一个字节的序号。若确认号为N, 则表明到序号N-1为止的所有数据都已正确收到。（累积确认）</p><p>3) 确认位ACK：只有当ACK=1时确认号字段才有效。当ACK=0时，确认号无效。TCP 规定，在连接建立后所有传送的报文段都必须把ACK置1。</p><p>4) 同步位SYN。同步SYN=1表示这是一个连接请求或连接接收报文。当SYN= 1, ACK=0 时，表明这是一个连接请求报文，对方若同意建立连接，则在响应报文中使用SYN= 1, ACK=1。即SYN=1表示这是一个连接请求或连接接收报文。</p><p>5) 终止位FIN (Finish) 。用来释放一个连接。FIN=1表明此报文段的发送方的数据已发送完毕了并要求释放传输连接。</p><h4 id="3、tcp连接管理"><a href="#3、tcp连接管理" class="headerlink" title="3、tcp连接管理"></a>3、tcp连接管理</h4><p>TCP 是面向连接的协议，因此每个TCP 连接都有三个阶段：连接建立、数据传送和连接释放。TCP 连接的管理就是使运输连接的建立和释放都能正常进行。</p><p>在TCP 连接建立的过程中，要解决以下三个问题：</p><p>1) 要使每一方都能够确知对方的存在。</p><p>2) 要允许双方协商一些参数（如最大窗口值、是否使用窗口扩大选项、时间戳选项及服务质量等）。</p><p>3) 能够对运输实体资源（如缓存大小、连接表中的项目等）进行分配。</p><p>每条TCP 连接唯一地被通信两端的两个端点（即两个套接字）确定。 端口拼接到IP地址即为套接字，tcp的连接采用的是客户机/服务器方式，主动发起连接建立的应用进程称为客户机，而被动等待连接建立的应用进程称为服务器。</p><h4 id="4、tcp连接的建立"><a href="#4、tcp连接的建立" class="headerlink" title="4、tcp连接的建立"></a>4、tcp连接的建立</h4><p><img src="https://www.hualigs.cn/image/60a3a1bfa6710.jpg" alt=""></p><p>1) 第一次握手：客户机的TCP首先向服务器的TCP发送一个连接请求报文段。这个特殊的报文段中不含应用层数据，其首部中的SYN标志位被置为1。另外，客户机会随机选择一个起始序号seq = x（连接请求报文不携带数据，但要消耗一个序号）。</p><p>2) 第二次握手：服务器的TCP 收到连接请求报文段后，如同意建立连接，就向客户机发回确认，并为该TCP连接分配TCP缓存和变量。在确认报文段中，SYN 和ACK 位都被置为1, 确认号字段的值为x+1,并且服务器随机产生起始序号seq= y( 确认报文不携带数据，但也要消耗一个序号）。确认报文段同样不包含应用层数据。</p><p>3) 第三次握手：当客户机收到确认报文段后，还要向服务器给出确认，并且也要给该连接分配缓存和变量。这个报文段的ACK 标志位被置1, 序号字段为x+1, 确认号字段ack=y+1。该报文段可以携带数据，若不携带数据则不消耗序号 http中的tcp连接的第三次握手的报文段中就捎带了客户对万维网文档的请求 。成功进行以上三步后，就建立了TCP 连接，接下来就可以传送应用层数据。TCP 提供的是全双工通信，因此通信双方的应用进程在任何时候都能发送数据。</p><p><strong>【总结】</strong></p><ul><li>1) SYN = 1,ACK = 0,seq = x;</li><li>2) SYN = 1,ACK = 1,seq = y,ack = x+1;</li><li>3) SYN = 0,ACK = 1,seq = x+1,ack=y+1。</li></ul><h4 id="5、tcp释放连接"><a href="#5、tcp释放连接" class="headerlink" title="5、tcp释放连接"></a>5、tcp释放连接</h4><p><img src="https://www.hualigs.cn/image/60a3a1dfde76f.jpg" alt=""></p><p>1) 第一次握手：客户机打算关闭连接时，向其TCP发送一个连接释放报文段，并停止发送数据，主动关闭TCP 连接，该报文段的FIN 标志位被置1, seq= u, 它等于前面已传送过的数据的最后一个字节的序号加1 (FIN 报文段即使不携带数据，也要消耗一个序号）。TCP是全双工的，即可以想象为一条TCP 连接上有两条数据通路。发送FIN 报文时，发送FIN 的一端不能再发送数据，即关闭了其中一条数据通路，但对方还可以发送数据。</p><p>2) 第二次握手：服务器收到连接释放报文段后即发出确认，确认号是ack = u + 1, 而这个报文段自己的序号是v, 等千它前面已传送过的数据的最后一个字节的序号加1 。此时，从客户机到服务器这个方向的连接就释放了，TCP连接处千半关闭状态。但服务器若发送数据，客户机仍要接收，即从服务器到客户机这个方向的连接并未关闭。</p><p>3) 第三次握手：若服务器已经没有要向客户机发送的数据，就通知TCP释放连接，此时其发出FIN=1的连接释放报文段。</p><p>4) 第四次握手：客户机收到连接释放报文段后，必须发出确认。在确认报文段中，ACK字段被置为1, 确认号ack= w + 1, 序号seq= u + 1 。此时TCP连接还未释放，必须经过时间等待计时器设置的时间2MSL（最长报文段寿命）后，A才进入连接关闭状态。</p><p><strong>【总结】</strong></p><ul><li><p>1) FIN = 1,seq = u;</p></li><li><p>2) ACK = 1,seq = v,ack = u+1;</p></li><li><p>3) FIN = 1,ACK = 1,seq = w,ack =u+1;(确认第一次的u)</p></li><li><p>4) ACK = 1,seq = u+1,ack = w+1。</p></li></ul><p><strong>question one : 什么是SYN洪泛攻击？（三次握手机制有什么问题？）</strong></p><p>答：由于服务器端的资源是在完成第二次握手时分配的，而客户端的资源是在完成第三次握手时分配的，攻击者发送TCP的SYN报文段，SYN是TCP三次握手中的第一个数据包，而当服务器返回ACK后，该攻击者就不对其进行再确认，那这个TCP连接就处于挂起状态，也就是所谓的半连接状态，服务器收不到再确认的话，还会重复发送ACK给攻击者。这样更加会浪费服务器的资源。攻击者就对服务器发送非常大量的这种TCP连接，由于每一个都没法完成三次握手，所以在服务器上，这些TCP连接会因为挂起状态而消耗CPU和内存，最后服务器可能死机，就无法为正常用户提供服务了。</p><p><strong>question two :为什么不采用“两次握手”建立连接呢？</strong></p><p>答：这主要是为了<strong>防止两次握手情况下已失效的连接请求报文段突然又传送到服务器而产生错误</strong>。考虑下面这种情况。客户A 向服务器B 发出TCP 连接请求，第一个连接请求报文在网络的某个结点长时间滞留， A 超时后认为报文丢失，于是再重传一次连接请求， B 收到后建立连接。数据传输完毕后双方断开连接。而此时，前一个滞留在网络中的连接请求到达服务器B, 而B 认为A又发来连接请求，此时若使用“三次握手”，则B 向A 返回确认报文段，由于是一个失效的请求，因此A 不予理睬，建立连接失败。若采用的是“两次握手”，则这种情况下B 认为传输连接已经建立，并一直等待A 传输数据，而A 此时并无连接请求，因此不予理睬，这样就造成了B的资源白白浪费。</p><p><strong>question three :如果已经建立了连接，但是客户端突然出现故障了怎么办?</strong></p><p>答：TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。</p><p><strong>question four :为什么连接的时候是三次握手，关闭的时候却是四次握手?</strong></p><p>答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次握手。</p><p><strong>question four :为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？</strong></p><p>答：1)虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL(Maximum SegmentLifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。2)防止出现“已失效的连接请求报文段“（和上面的为啥不用二次握手类似）。A 在发送最后一个确认报文段后，再经过2MSL可保证本连接持续的时间内所产生的所有报文段从网络中消失.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tcp协议 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="all/hello-world/"/>
      <url>all/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
