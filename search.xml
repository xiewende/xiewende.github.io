<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>常用卷积总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h4 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h4><ul><li><p>输入：$H_{in} \times W_{in} \times C_{in}$，其中 $H_{in}$ 为输入 feature map的高，$W_{in}$ 为款，$C_{in}$ 为通道数</p></li><li><p>输出：$H_{out} \times W_{out} \times C_{out}$，其中 $H_{out}$ 为输入 feature map的高，$W_{out}$ 为款，$C_{out}$ 为通道数</p></li><li><p>卷积核：$N \times K \times K \times C_k$ ，其中 N 为该卷积层的卷积核个数，$K$ 为卷积核宽与高(默认相等)，$C_k$ 为卷积核通道数</p></li></ul><h4 id="常规卷积"><a href="#常规卷积" class="headerlink" title="常规卷积"></a>常规卷积</h4><p><strong>特点：</strong></p><ul><li>卷积和通道数与输入 feature map的通道数相等，即 $C_{in} = C_k$</li><li>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</li></ul><p><strong>卷积过程：</strong></p><p>卷积核在输入 feature map 中移动，按位点乘后求和即可，通道也会求和。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/1.gif" style="zoom: 50%;" /></p><p><strong>函数语法格式，二维卷积最常用的卷积方式，先实例化再使用。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn. Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode= &#x27;zeros&#x27; )</span><br></pre></td></tr></table></figure><ul><li><p>参数解释</p><ul><li><strong>in_channels</strong> ：输入的四维张量[N, C, H, W]中的C，也就是说输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。</li><li><strong>out_channels</strong>：即是期望的输出四维张量的channels数。</li><li><strong>kernel_size</strong> ：卷积核的大小，一般我们会使用 5x5、3x3 这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个<strong>tuple</strong>，而不能写一个列表（list）。</li><li><strong>stride = 1</strong>： 卷积核在图像窗口上每次平移的间隔，即所谓的步长。跟Tensorflow框架等的意义一样。</li><li><strong>padding=0</strong>：padding也就是指图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。<strong>需要注意的是这里的填充包括图像的上下左右</strong>，以padding=1为例，若原始图像大小为<strong>32*32</strong>，那么padding后的图像大小就变成了 <strong>34*34</strong>，而不是<strong>33*33</strong>。这是Pytorch与Tensorflow在卷积层实现上最大的差别。</li><li><strong>dilation=1</strong>：这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）</li><li><strong>groups=1</strong>：决定了是否采用分组卷积，默认值为 1 .</li><li><strong>bias=True</strong> ：即是否要添加偏置参数作为可学习参数的一个，默认为True。</li><li><strong>padding_mode</strong> ：即padding的模式，默认采用零填充。</li></ul></li><li><p>输出图像的计算公式</p><ul><li><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/4.png" style="zoom:80%;" /></p></li><li><p>在大多数情况下，<strong><em>大多数情况下的</em> kernel_size、padding</strong>左右两数均相同，且不采用空洞卷积（dilation默认为1），因此只需要记住这种在深度学习课程里学过的公式就好了。</p><script type="math/tex; mode=display">O = \frac{I-K+2P}{S} + 1</script></li></ul></li></ul><h4 id="1X1卷积"><a href="#1X1卷积" class="headerlink" title="1X1卷积"></a>1X1卷积</h4><p><strong>特点、作用：</strong></p><ul><li><p>顾名思义，卷积核大小为 1ｘ1</p></li><li><p>卷积核通道数与输入 feature map 的通道数相等，即 $C_{in} = C_k$</p></li><li><p>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</p></li><li><p>不改变 feature map 的大小，目的是为了改变 channel 数，即 1ｘ1 卷积的使用场景是：不想改变输入 feature map 的宽高，但想改变它的通道数。即可以用于升维或降维。</p></li><li><p>相比 3ｘ3 等卷积，计算量及参数量都更小，计算量和参数量的计算参考另一篇文章 (22_CNN网络各种层的FLOPs和参数量paras计算)</p></li><li><p><strong>加入非线性</strong>。1*1的卷积在不同 channels 上进行线性整合，在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/2.png" style="zoom:80%;" /></p><p>1x1核的主要目的是应用非线性。在神经网络的每一层之后，我们都可以应用一个激活层。无论是ReLU、PReLU、Sigmoid还是其他，与卷积层不同，激活层是非线性的。非线性层扩展了模型的可能性，这也是通常使“深度”网络优于“宽”网络的原因。为了<strong>在不显著增加参数和计算量的情况下增加非线性层的数量，我们可以应用一个1x1内核并在它之后添加一个激活层。这有助于给网络增加一层深度</strong></p><h4 id="分组卷积（Group-Convolution）"><a href="#分组卷积（Group-Convolution）" class="headerlink" title="分组卷积（Group Convolution）"></a>分组卷积（Group Convolution）</h4><p>Group convolution 分组卷积，最早在 AlexNet 中出现，由于当时的硬件资源有限，训练 AlexNet 时卷积操作不能全部放在同一个 GPU 处理，因此作者把 feature maps 分给多个GPU分别进行处理，最后把多个 GPU 的结果进行融合。</p><p><strong>卷积过程</strong></p><p>​    将输入 feature map 分成 g 组，一个卷积核也相对应地分成 g 组，在对应的组内做卷积。（我们可以理解成分组卷积中使用的 g 组卷积核整体对应于常规卷积中的一个卷积核，只不过是将常规卷积中的一个卷积核分成了 g 组而已）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/3.png" alt=""></p><p><strong>特点、作用：</strong></p><ul><li><p>输入的 feature map 尺寸：$H_{in}<em>W_{in}</em> \frac{C_{in}}{g}$ ，共有 g 组</p></li><li><p>卷积核的规格：$N<em>K</em>K<em>\frac{C_{k}}{g}$，共有 N </em> g 组</p></li><li><p>输出 feature map 规格：$H_{out}<em>W_{out}</em>N<em>g$ ，共生成 N</em>g 个 feature map</p></li><li><p>当 g=1 时就退化成了上面讲过的常规卷积，当 $g=C_{in}$ 时就是我们下面将要讲述的深度分离卷积。</p></li><li><p>用常规卷积得到一个输出 feature map 的计算量和参数量便可以得到 g 个输出 feature map，所以分组卷积常用在轻量型高效网络中，因为它可以用少量的参数量和计算量生成大量的 feature map。</p></li><li><p>优点</p><ul><li>标准2D卷积参数量：$W \times H \times C_{in} \times C_k$</li><li><p>分组卷积参数量：$W \times H \times C_{in}/2 \times C_k/2 \times 2$</p></li><li><p>group=2,参数量减少到原来的1/2；group=4,参数量减少到原来的1/4；<strong>总结：参数量减少1/g。</strong> </p></li><li>减少运算量和参数量，相同输入输出大小的情况下，减少为原来的 1/g </li></ul></li><li><p>代码的话很简单，就是 nn.Conv2d 里面的一个参数：group。</p></li></ul><h4 id="可分离卷据（Separable-Convolution）"><a href="#可分离卷据（Separable-Convolution）" class="headerlink" title="可分离卷据（Separable Convolution）"></a>可分离卷据（Separable Convolution）</h4><h5 id="空间可分离卷积"><a href="#空间可分离卷积" class="headerlink" title="空间可分离卷积"></a>空间可分离卷积</h5><p>之所以命名为空间可分离卷积，是因为它主要处理的是卷积核的空间维度：宽度和高度。</p><p>空间可分离卷积简单地将卷积核划分为两个较小的卷积核。 最常见的情况是将3x3的卷积核划分为3x1和1x3的卷积核，如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/4.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/5.jpg" alt=""></p><ul><li><p><strong>局限性</strong>：并不是所有的卷积核都可以“分离”成两个较小的卷积核，==能够“分离”的是那些卷积核参数大小的行和列有一定倍数关系的==. 这在训练期间变得特别麻烦，因为网络可能采用所有可能的卷积核，它最终只能使用可以分成两个较小卷积核的一小部分。所以实际中用的不多</p></li><li><p><strong>参数量和计算量更少</strong>：如上图所示，不是用9次乘法进行一次卷积，而是进行两次卷积，每次3次乘法（总共6次），以达到相同的效果。 乘法较少，计算复杂性下降，网络运行速度更快。</p></li></ul><h5 id="深度可分离卷积（Depthwise-Separable-Convolution）"><a href="#深度可分离卷积（Depthwise-Separable-Convolution）" class="headerlink" title="深度可分离卷积（Depthwise Separable Convolution）"></a>深度可分离卷积（Depthwise Separable Convolution）</h5><p>深度可分离卷积的过程分为两个部分：<strong>深度卷积（depthwise convolution）和逐点卷积（pointwise convolution）</strong></p><p><strong>（1）深度卷积</strong></p><p>深度卷积意在保持输入 feature map 的通道数，即对 feature map 中的<strong>每个通道使用一个规格为 $K<em>K</em>1$ 的卷积核进行卷积，于是输入 feature map 有多少个通道就有多少个这样的卷积核</strong>，深度卷积结束后得到的输出的通道数与输入的相等。</p><p>Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积，这个过程产生的feature map通道数和输入的通道数完全一样。</p><p>这一步其实就相当于常规卷积中的一个卷积核，只不过不同通道的卷积结果不相加而已，自己体会体会。</p><p>Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。<strong>而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/6.png" style="zoom:80%;" /></p><p><strong>（2） 逐点卷积</strong></p><p>在上一步的基础上，运用 1ｘ1 卷积进行逐点卷积。</p><p>使用一个 1ｘ1 卷积核就可以得到输出 feature map 一维的结果。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/7.png" style="zoom:80%;" /></p><p>如果你要输出 feature map 有 256 维，那么就使用 256 个 1ｘ1 卷积核即可。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/8.png" style="zoom:80%;" /></p><ul><li><strong>可以理解成常规的卷积分成了两步执行，但是分成两步后参数量和计算量大大减少，网络运行更快</strong></li><li>深度分离卷积几乎是构造轻量高效模型的必用结构，如Xception, MobileNet, MobileNet V2, ShuffleNet, ShuffleNet V2, CondenseNet等轻量型网络结构中的必用结构。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/9.png" style="zoom:80%;" /></p><h4 id="转置卷积（Transposed-Convolution）"><a href="#转置卷积（Transposed-Convolution）" class="headerlink" title="转置卷积（Transposed Convolution）"></a>转置卷积（Transposed Convolution）</h4><p><strong>转置卷积（Transposed Convolution）</strong> 在语义分割或者对抗神经网络（GAN）中比较常见，<strong>其主要作用就是做上采样。</strong></p><ul><li><strong>转置卷积不是卷积的逆运算、不是逆运算、不是逆运算（重要的事情说三遍）</strong></li><li>转置卷积也是卷积</li></ul><p>函数语法格式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, bias=True)</span><br></pre></td></tr></table></figure><p>概述：就是<strong>反卷积</strong>，该函数是用来进行转置卷积的，它主要做了这几件事：<strong>首先</strong>，对输入的feature map进行padding操作，得到新的feature map；<strong>然后</strong>，随机初始化一定尺寸的卷积核；<strong>最后</strong>，用随机初始化的一定尺寸的卷积核在新的feature map上进行卷积操作。卷积核确实是随机初始的，但是后续可以对卷积核进行单独的修改</p><p>主要作用就是起到上采样的作用。但转置卷积不是卷积的逆运算（一般卷积操作是不可逆的），它只能恢复到原来的大小（shape），数值与原来不同。转置卷积的运算步骤可以归为以下几步：</p><ul><li>1、在输入特征图元素间填充 <strong>stride-1 行列 0</strong>（其中 stride 表示转置卷积的步距）</li><li><p>2、在输入特征图四周填充 <strong>k-p-1</strong> <strong>行列0</strong>（其中k表示转置卷积的kernel_size大小，p为转置卷积的padding，注意这里的padding和卷积操作中有些不同）</p></li><li><p>3、将卷积核参数上下、左右翻转 </p></li><li>4、做正常卷积运算（填充0，步距1）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/12.png" style="zoom:80%;" /></p><p>下图展示了转置卷积中不同 stride 和 padding 的情况：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/16.png" style="zoom: 67%;" /></p><p><strong>输出尺寸计算：</strong></p><script type="math/tex; mode=display">output = (input-1) * stride + output\_padding - 2*padding + kernel_size</script><p>不过时常 output_padding=0,</p><script type="math/tex; mode=display">output = (input-1) * stride - 2*padding + kernel_size</script><h4 id="空洞卷积（Dilated-Convolution）"><a href="#空洞卷积（Dilated-Convolution）" class="headerlink" title="空洞卷积（Dilated Convolution）"></a>空洞卷积（Dilated Convolution）</h4><p><strong>空洞卷积也叫扩张卷积或者膨胀卷积，简单来说就是在卷积核元素之间加入一些空格(零)来扩大卷积核的过程。</strong></p><p>空洞卷积诞生在图像分割领域，在一般的卷积结构中因为存在 pooling 操作，目的是增大感受野也增加非线性等，但是 pooling 之后特征图的大小减半，而图像分割是 pixel-wise 的，因此后续需要 upsamplng 将变小的特征图恢复到原始大小，这里的 <strong>upsampling 主要是通过转置卷积完成</strong>，但是经过这么多的操作之后会将很多细节丢失，<strong>那么空洞卷积就是来解决这个的，既扩大了感受野，又不用 pooling</strong> 。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/10.png" style="zoom:50%;" /></p><p>假设以一个变量a来衡量空洞卷积的扩张系数，则加入空洞之后的实际卷积核尺寸与原始卷积核尺寸之间的关系：<strong>K = K + (k-1)(a-1)</strong></p><p>其中<strong>k为原始卷积核大小，a为卷积扩张率(dilation rate)</strong>，K为经过扩展后实际卷积核大小。除此之外，空洞卷积的卷积方式跟常规卷积一样。我们用一个扩展率a来表示卷积核扩张的程度。比如说<strong>a=1,2,4</strong>的时候卷积核核感受野如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230807/11.jpg" alt=""></p><p>在这张图像中，3×3 的红点表示经过卷积后，输出图像是 3×3 像素。尽管所有这三个扩张卷积的输出都是同一尺寸，但模型观察到的感受野有很大的不同。当a=1，原始卷积核size为3 <em> 3，就是常规卷积。a=2时，加入空洞之后的卷积核：size=3+(3-1) </em> (2-1)=5，对应的感受野可计算为：(2 ^(a+2))-1=7。a=3时，卷积核size可以变化到3+(3-1)(4-1)=9，感受野则增长到 (2 ^(a+2))-1=15。<strong>有趣的是，与这些操作相关的参数的数量是相等的。我们「观察」更大的感受野不会有额外的成本</strong>。因此，扩张卷积可用于廉价地增大输出单元的感受野，而不会增大其核大小，这在多个扩张卷积彼此堆叠时尤其有效。</p><ul><li><strong>扩大感受野</strong>：一般来说，在深度神经网络中增加感受野并且减少计算量的方法是下采样。但是下采样牺牲了空间分辨率和一些输入的信息。空洞卷积一方面增大了感受野可以检测分割大目标，另一方面相较于下采样增大了分辨率可以精确定位目标。</li><li><p><strong>捕获多尺度上下文信息</strong>：当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。</p></li><li><p>代码实现就是控制  nn.Conv2d 里面的一个参数：dilation 。</p></li></ul><h4 id="可变形卷积（Deformable-Convolution）"><a href="#可变形卷积（Deformable-Convolution）" class="headerlink" title="可变形卷积（Deformable Convolution）"></a>可变形卷积（Deformable Convolution）</h4><p><strong>要解决的问题：</strong>传统卷积，只能是死板的正方形感受野，不能定义任意形状的感受野，但感受野的形状不限制更能提取有效信息</p><p><strong>目的：使得卷积的感受野</strong>通过训练<strong>可以自适应调整</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 卷积 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Loss_in_Deep_Learning</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Loss-in-Deep-Learning/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Loss-in-Deep-Learning/</url>
      
        <content type="html"><![CDATA[<h3 id="Loss-in-Deep-Learning"><a href="#Loss-in-Deep-Learning" class="headerlink" title="Loss in Deep Learning"></a>Loss in Deep Learning</h3><p>监督学习主要分为两类</p><ul><li><p>回归问题：目标输出变量是连续结果，如预测西瓜的含糖率（0.00~1.00）</p></li><li><p>分类问题：目标输出变量是离散结果，如判断一个西瓜是好瓜还是坏瓜，那么目标变量只能是1（好瓜）,0（坏瓜）</p><ul><li>二分类问题：只有两个类别</li><li>多分类问题：多个类别。</li></ul></li></ul><p>损失函数严格上可分为两类：<strong>分类损失</strong>和<strong>回归损失</strong>，其中<strong>分类损失</strong>根据类别数量又可分为<strong>二分类损失</strong>和<strong>多分类损失</strong>。在使用的时候需要注意的是：<strong>回归函数预测数量，分类函数预测标签</strong>。</p><h4 id="回归损失函数"><a href="#回归损失函数" class="headerlink" title="回归损失函数"></a>回归损失函数</h4><h5 id="平均绝对误差损失-MAE"><a href="#平均绝对误差损失-MAE" class="headerlink" title="平均绝对误差损失(MAE)"></a>平均绝对误差损失(MAE)</h5><p>平均绝对误差 Mean Absolute Error(MAE) 是常用的损失函数，也称为 L1 Loss，其基本公式为：</p><script type="math/tex; mode=display">L_{MAE} = \frac{1}{N} \sum_{i=1}^{N}\left | y_{i} - \hat{y}_i \right |</script><p>简单说就是预测值和标签值差值的绝对值平均值，其可视化图如下图，MAE损失的最小值为0（当预测值等于真实值），最大值为无穷大，可以看出随着预测值与真实值绝对误差 $\left | y_{i} - \hat{y}_i \right |$ 的增加，<strong>MAE损失呈线性增长</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/1.jpg" style="zoom:80%;" /></p><h5 id="均方差损失-MSE"><a href="#均方差损失-MSE" class="headerlink" title="均方差损失(MSE)"></a>均方差损失(MSE)</h5><p>均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下：</p><script type="math/tex; mode=display">L_{MSE} = \frac{1}{N} \sum_{i=1}^{N}\left ( y_{i} - \hat{y}_i \right )^2</script><p>损失函数的最小值为 0（当预测等于真实值时），最大值为无穷大。下图是对于真实值 ，不同的预测值 [-1.5, 1.5] 的均方差损失的变化图。横轴是不同的预测值，纵轴是均方差损失，可以看到随着预测与真实值绝对误差  $\left | y_{i} - \hat{y}_i \right |$ 的增加，<strong>均方差损失呈二次方地增长</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/2.jpg" style="zoom:80%;" />/&gt;</p><h5 id="MAE和MSE的区别"><a href="#MAE和MSE的区别" class="headerlink" title="MAE和MSE的区别"></a>MAE和MSE的区别</h5><p><strong>MAE 和 MSE 作为损失函数的主要区别是：MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier 更加健壮，即更加不易受到 outlier 影响。</strong></p><p>MSE 通常比 MAE 可以更快地收敛。当使用梯度下降算法时，MSE 损失的梯度为 $-\hat{y}_i$ ，而 MAE 损失的梯度为  $\mp 1$ ，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的。当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了 MSE 在大部分时候比 MAE 收敛地更快。这个也是 MSE 更为流行的原因.</p><p>MAE 对于 outlier 更加 robust，MAE 和 MSE 损失函数可视化中，由于MAE 损失与绝对误差之间是<strong>线性关系</strong>，MSE 损失与误差是<strong>平方关系</strong>，<strong>当误差非常大的时候，MSE 损失会远远大于 MAE 损失</strong>。因此当数据中出现一个误差非常大的 outlier 时，MSE 会产生一个非常大的损失，对模型的训练会产生较大的影响。</p><h5 id="Huber-Loss"><a href="#Huber-Loss" class="headerlink" title="Huber Loss"></a>Huber Loss</h5><p>MSE损失收敛快但是容易收到 outlier 的影响，MAE对 outlier 更加健壮但是收敛满，Huber Loss则是一种将 MSE 和 MAE结合起来，取二者优点的损失函数，也称为 Smooth Absolute Error Loss，其原理就是在误差接近0的时候使用 MSE，误差较大时使用 MAE，公式为：</p><script type="math/tex; mode=display">L_{h u b e r}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right| \leq \delta} \frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}+\mathbb{I}_{\left|y_{i}-\hat{y}_{i}\right|>\delta}\left(\delta\left|y_{i}-\hat{y}_{i}\right|-\frac{1}{2} \delta^{2}\right)</script><p>其中 $\delta$  是 Huber Loss 的一个超参数， $\delta$ 的值是在 MSE 和 MAE 两个损失连接的位置，上述等式第一项是 MSE 部分。第二项是 MAE 部分，在MAE部分公式为 $\delta\left|y_{i}-\hat{y}_{i}\right|-\frac{1}{2} \delta^{2}$ 是为了保证误差   $\left | y_{i} - \hat{y}_i \right | = \mp \delta$ 时，MAE和MSE的取值是一致的，进而保证 Huber Loss损失连续可导。下图是 $\delta = 1.0$ 时的 Huber Loss，可以看到在区间 $[-\delta,\delta]$ 就是MSE损失，在区间 $(-\infty,\delta)$ 和 $(\delta, +\infty)$ 就是MAE损失：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230422/3.jpg" style="zoom:80%;" /></p><p>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使<strong>损失函数可导并且梯度更加稳定</strong>；在误差较大时使用 MAE 可以降低 outlier 的影响，使<strong>训练对 outlier 更加健壮</strong>。缺点是需要额外地设置一个 $\delta$ 超参数。</p><h4 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h4><h5 id="交叉熵损失-CrossEntropy-Loss"><a href="#交叉熵损失-CrossEntropy-Loss" class="headerlink" title="交叉熵损失(CrossEntropy Loss)"></a>交叉熵损失(CrossEntropy Loss)</h5><p>交叉熵损失函数是分类中最常用的损失函数，交叉熵是用来度量<strong>两个概率分布的差异性</strong>的，用来衡量模型<strong>学习到的分布和真实分布的差异</strong>。</p><p><strong>在二分类中</strong>，通常使用 Sigmoid 函数将模型的输出压缩到 【0，1】区间内，$\hat{y}_i \in (0,1)$, 用来代表给定输入 $x_i$ ，模型判断为为正类的概率。因此也得到负类的概率，</p><script type="math/tex; mode=display">p(y_i=1 | x) = \hat{y}_i \quad p(y_i=0 | x) = 1-\hat{y}_i</script><script type="math/tex; mode=display">p(y_i | x_i) = (\hat{y})^{y_i}(1-\hat{y}_i)^(1-y_i)</script><p><strong>二分类交叉熵损失（binary entropy loss）</strong>的一般形式：P为正类概率，y为标签。</p><script type="math/tex; mode=display">Loss = -(y\,log\,P + (1-y)log\,(1-P))</script><p><strong>交叉熵公式的原理</strong>：</p><ul><li><p>信息量，信息量表示一条信息消除不确定性的程度，<strong>信息量的大小和事件发生的概率成反比</strong></p><script type="math/tex; mode=display">-log\,p</script></li><li><p>信息熵，信息熵则是在结果出来之前对可能产生的信息量的期望，期望可以理解为所有可能结果的概率乘以该对应的结果。</p><script type="math/tex; mode=display">H(X) = - \sum_{i=1}^n P(X=x_i)\,log\,P(X=x_i)</script><p><strong>信息熵是用来衡量事物不确定性的。信息熵越大（信息量越大，P越小），事物越具不确定性，事物越复杂。</strong></p></li></ul><ul><li><p>相对熵（KL散度），相对熵又称互熵 设 P(x) 和 Q(x) 是取值的两个概率分布，相对熵用来表示两个<strong>概率分布的差异</strong>，当两个随机分布相同时，他们的相对熵为0，当两个随机分布的差别增大时，他们的相对熵也会增大：</p><script type="math/tex; mode=display">D_{K L}(P \| Q)=-\sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)-\left(-\sum_{i=1}^{n} P\left(x_{i}\right) \log P\left(x_{i}\right)\right)=\sum_{i=1}^{n} P\left(x_{i}\right) \log \frac{P\left(x_{i}\right)}{Q\left(x_{i}\right)}</script><ul><li>KL散度不是一个对称量，$D_{K L}(P | Q) \neq D_{K L}(Q | P)$。</li><li>KL散度的值始终大于&gt;=0,当且仅当 $P(X)=Q(x)$ 等号成立。</li></ul></li></ul><p><strong>交叉熵</strong>， KL散度换种写法：</p><script type="math/tex; mode=display">D_{K L}(P \| Q)=-\sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)-\left(-\sum_{i=1}^{n} P\left(x_{i}\right) \log P\left(x_{i}\right)\right) = -H(P(X)) - \sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)</script><p><strong>交叉熵 $H(P,Q)$ 即等于 信息熵+KL散度</strong>：</p><script type="math/tex; mode=display">H(P,Q) = H(P) + D_KL((P \| Q)) = - \sum_{i=1}^{n} P\left(x_{i}\right) \log Q\left(x_{i}\right)</script><p>把 P 看作随机变量的真实分布的话，KL散度左半部分 $ -H(P(X))$ 的信息熵其实是一个固定值，KL散度的大小变化其实是由右半部分交叉熵来决定的，因为右半部分含有近似分布 Q，我们可以把它看作网络或模型的实时输出，把KL散度或者交叉熵看做<strong>真实标签与网络预测结果的差异</strong>，所以<strong>神经网络的目的就是通过训练使近似分布逼近真实分布</strong>。从理论上讲，<strong>优化KL散度与优化交叉熵的效果应该是一样的</strong>。所以我认为，在深度学习中选择优化交叉熵而非KL散度的原因可能是为了减少一些计算量，交叉熵毕竟比KL散度少一项。</p><p><strong>多分类交叉熵损失</strong>，多分类和二分类类似，二分类的标签为1和0，而多分类可以用one-hot编码来表示，公式为：</p><script type="math/tex; mode=display">Loss = -\frac{1}{N} \sum_{i=0}^{N-1}\sum_{k=0}^{K-1}\,y_{i,k}\,log\, p_{i,k}</script><p>其中，$y_{i,k}$ 表示第 i 个样本的真实标签为 k，共有 K 个标签值 N 个样本，$p_{i,k}$ 表示第 i 个样本预测为第 k 个标签值得概率，通过对该损失函数得拟合，也在一定程度上增大了类间距离，在 torch中，<code>torch.nn.functional.cross_entropy</code> 实现了将输出整合到【0，1】概率区间，不需要 softmax 操作。</p><h5 id="Weight-Loss"><a href="#Weight-Loss" class="headerlink" title="Weight Loss"></a>Weight Loss</h5><p>交叉熵Loss可以用在大多数语义分割场景中，但它有一个明显的缺点，那就是对于只用分割前景和背景的时候，当前景像素的数量远远小于背景像素的数量时，即 y=0 的数量远大于 y=1 的数量，损失函数中 y=0 的成分就会占据主导，<strong>使得模型严重偏向背景，导致效果不好。</strong></p><p>由于交叉熵损失会分别评估每个像素的类别预测，然后对所有像素的损失进行平均，因此我们实质上是在对图像中的每个像素进行平等地学习。<strong>如果多个类在图像中的分布不均衡，那么这可能导致训练过程由像素数量多的类所主导，即模型会主要学习数量多的类别样本的特征，并且学习出来的模型会更偏向将像素预测为该类别。</strong></p><p>FCN论文和U-Net论文中针对这个问题，对输出概率分布向量中的每个值进行加权，即希望模型更加关注数量较少的样本，以缓解图像中存在的类别不均衡问题。</p><p>比如对于二分类，正负样本比例为1: 99，此时模型将所有样本都预测为负样本，那么准确率仍有99%这么高，但其实该模型没有任何使用价值。</p><p>为了平衡这个差距，就对正样本和负样本的损失赋予不同的权重，<strong>带权重的二分类损失函数公式</strong>如下：</p><script type="math/tex; mode=display">Loss = -(pos\_weight \times y\,log\,P + (1-y)log\,(1-P))</script><script type="math/tex; mode=display">pos\_weight = \frac{num\_neg}{num\_pos}</script><p>要减少假阴性样本的数量，可以增大 pos_weight；要减少假阳性样本的数量，可以减小 pos_weight。</p><p><strong>带权重的交叉熵Loss</strong>，公式为：</p><script type="math/tex; mode=display">Loss = -\sum_{c=1}^M w_c\,y_c\,log(p_c)</script><p>可以看到只是在交叉熵Loss的基础上为每一个类别添加了一个权重参数，其中 $w_c$ 的计算公式为： $w_c = \frac{N-N_c}{N}$ 其中 N 表示总的像素个数，而 $N_c$  表示 GT 类别为 c 的像素个数。这样相比于原始的交叉熵Loss，在样本数量不均衡的情况下可以获得更好的效果。</p><h5 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h5><p><strong>重点：</strong>需要解决不同类别的<strong>像素数量不均衡</strong>问题，但有时还需要将<strong>像素分为难学习和容易学习这两种样本</strong>。</p><p>容易学习的样本模型可以很轻松地将其预测正确，模型只要将大量容易学习的样本分类正确，loss就可以减小很多，从而导致模型不怎么顾及难学习的样本，所以我们要想办法让模型更加关注难学习的样本。</p><p>何凯明团队在RetinaNet论文中引入了Focal Loss来解决<strong>难易样本数量不平衡</strong>，我们来回顾一下。 我们在计算分类的时候常用的损失——二分类交叉熵的公式如下：</p><script type="math/tex; mode=display">Loss_\mathrm{CE}(p, y)=\left\{\begin{array}{ll}-\log (p) & \text { if } y=1 \\-\log (1-p) & \text { otherwise }\end{array}\right.</script><p>为了解决<strong>正负样本数量不平衡</strong>的问题，我们经常在二元交叉熵损失前面加一个参数 $\alpha$，即：</p><script type="math/tex; mode=display">Loss_\mathrm{CE}(p, y)=\left\{\begin{array}{ll}- \alpha \log (p) & \text { if } y=1 \\- (1-\alpha) \log (1-p) & \text { otherwise }\end{array}\right.</script><p><strong>虽然参数 $\alpha$ 平衡了正负样本的数量，上式针对不同类别的像素数量不均衡提出了改进方法</strong>。但实际上，有时候候选目标都是<strong>易分样本</strong>。这些样本的损失很低，但是由于数量极不平衡，易<strong>分样本的数量相对来讲太多，最终主导了总的损失。</strong></p><p>因此，这篇论文认为<strong>易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本</strong> 。所以Focal Loss横空出世了。一个简单的想法就是只要我们将高置信度样本的损失降低一些就好了吧？ 也即是下面的公式：</p><script type="math/tex; mode=display">Focal\_Loss(p, y)=\left\{\begin{array}{ll}- (1-p)^{\gamma } \log (p) & \text { if } y=1 \\- p^{\gamma } \log (1-p) & \text { otherwise }\end{array}\right.</script><p>其中的 $\gamma$ 通常设置为2。</p><p>举个例子，预测一个正样本，如果预测结果为0.95，这是一个容易学习的样本，有 $(1−0.95)^2=0.0025$ ，损失直接减少为原来的1/400。</p><p>而如果预测结果为0.5，这是一个难学习的样本，有 $(1−0.5)^2=0.25$ ，损失减小为原来的1/4，虽然也在减小，但是相对来说，减小的程度小得多。</p><p><strong>所以通过这种修改，就可以使模型更加专注于学习难学习的样本</strong>。</p><p><strong>而将这个修改和对正负样本不均衡的修改合并在一起，就是大名鼎鼎的 focal loss：</strong>带两个超参数：$\alpha$ 和 $\gamma$</p><script type="math/tex; mode=display">Focal\_Loss(p, y)=\left\{\begin{array}{ll}- \alpha(1-p)^{\gamma } \log (p) & \text { if } y=1 \\- (1-\alpha)p^{\gamma } \log (1-p) & \text { otherwise }\end{array}\right.</script><p><strong>Focal Loss代码：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class FocalLoss(nn.Module):</span><br><span class="line">    def __init__(self, gamma=0, alpha=None, size_average=True):</span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])</span><br><span class="line">        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)</span><br><span class="line">        self.size_average = size_average</span><br><span class="line"></span><br><span class="line">    def forward(self, input, target):</span><br><span class="line">        if input.dim()&gt;2:</span><br><span class="line">            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W =&gt; N,C,H*W</span><br><span class="line">            input = input.transpose(1,2)    # N,C,H*W =&gt; N,H*W,C</span><br><span class="line">            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C =&gt; N*H*W,C</span><br><span class="line">        target = target.view(-1,1)</span><br><span class="line"></span><br><span class="line">        logpt = F.log_softmax(input)</span><br><span class="line">        logpt = logpt.gather(1,target)</span><br><span class="line">        logpt = logpt.view(-1)</span><br><span class="line">        pt = Variable(logpt.data.exp())</span><br><span class="line"></span><br><span class="line">        if self.alpha is not None:</span><br><span class="line">            if self.alpha.type()!=input.data.type():</span><br><span class="line">                self.alpha = self.alpha.type_as(input.data)</span><br><span class="line">            at = self.alpha.gather(0,target.data.view(-1))</span><br><span class="line">            logpt = logpt * Variable(at)</span><br><span class="line"></span><br><span class="line">        loss = -1 * (1-pt)**self.gamma * logpt</span><br><span class="line">        if self.size_average: return loss.mean()</span><br><span class="line">        else: return loss.sum()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Regularization_Summary</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Regularization-Summary/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Regularization-Summary/</url>
      
        <content type="html"><![CDATA[<h3 id="正则化总结"><a href="#正则化总结" class="headerlink" title="正则化总结"></a>正则化总结</h3><h4 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h4><ul><li><p>欠拟合是指模型不能在训练集上获得足够低的误差。</p></li><li><p>过拟合是指训练误差和测试误差之间的差距太大，神经网络对训练数据进行很好的建模但在看到来自同一问题域的新数据时失败的现象。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230329/1.png" style="zoom:50%;" /></p><p><strong>模型容量低，而训练数据复杂度高</strong>时，此时模型由于学习能力不足，无法学习到数据集中的“一般规律”，因而导致训练误差高，表现为欠拟合。</p><p><strong>模型容量高，而训练数据简单</strong>时，此时模型由于学习能力太强，记住所有的训练数据，却没有理解数据背后的规律，对于训练集以外的数据泛化能力差，表现为过拟合。</p></li></ul><h5 id="解决欠拟合"><a href="#解决欠拟合" class="headerlink" title="解决欠拟合"></a>解决欠拟合</h5><ul><li>在模型容量和训练数据复杂度不匹配时，发生了欠拟合现象，常见解决方法有：</li><li><strong>增加新特征</strong>：可以考虑加入特征组合、高次特征等，来增大假设空间。</li><li><strong>增大模型容量</strong>：容量低的模型可能很难拟合训练集。</li><li><strong>减少正则化参数</strong>：正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数。</li></ul><h5 id="解决过拟合"><a href="#解决过拟合" class="headerlink" title="解决过拟合"></a>解决过拟合</h5><p><strong>引起过拟合</strong>的主演原因如下：</p><ul><li><strong>训练数据集问题</strong>。如训练样本不均衡，训练集中正样本偏多，那么去预测负样本肯定不准；训练样本数据少，尤其是当比模型参数数量还少时，更容易发生过拟合；训练数据噪声干扰过大，模型会学习很多的噪声特征等。</li><li><strong>模型过于复杂</strong>。模型参数数量太多，参数取值范围太大，模型已经能够“死记硬背”记下了所有训练数据的信息（记住了不适合于测试集的训练集特性）；模型假设的合理性不存在，也就是假设成立的条件实际并不成立。</li><li><strong>模型训练迭代次数太多</strong>。对数据反复地训练也可能会让模型拟合了训练样本中没有代表性的特征</li></ul><p><strong>解决过拟合问题</strong>，最重要的减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。通常解决过拟合的方法有：</p><ul><li>数据集增强</li><li>控制模型容量大小</li><li>正则化（Regularization）</li><li>丢弃法（Dropout）</li><li>提前终止训练（Early Stopping）</li></ul><h4 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化(Regularization)"></a>正则化(Regularization)</h4><p><strong>正则化其实就是在原来的目标函数的基础上又加了一项非负项</strong>， 并且这个非负项是 w 的函数。 这样的话target不变的基础上得让这个loss变得小一点， 相当于对其产生了一种约束。也可以认为<strong>正则化是减小权值方差的一种策略，减小权值的取值范围，从而使得模型求解空间变小，不会拟合出复杂的曲线（函数）</strong>.</p><p><strong>正则化主要用于避免过拟合的产生和减少网络误差，通过将L1范数或者L2范数加入到损失函数中，在损失函数中增加一个惩罚项，在进行反向传播]时就相当于在原来的权重上乘上了一个小于1的系数，使得权重减小，从而使得模型的权重的取值范围减小，即模型的求解空间变小，不会拟合出复杂的曲线（函数），从而达到抑制过拟合的目的，从而限制模型参数的数量以及参数的大小。</strong></p><ul><li>通常使用 L2 正则，称为权重衰减（weight decay），一般在优化器中使用，取值一般为0.01,0.001,0.0001;</li><li>L2 正则：容易计算， 可导， 适合基于梯度的方法，对异常值非常敏感；<strong>执行 L2 正则化鼓励权重值趋向于零</strong>（但不完全为零）。</li><li>L1 正则：<strong>L1模型可以将 一些权值缩小到零</strong>（稀疏，由于它可以提供稀疏的解决方案， 因此通常是建模特征数量巨大时的首选模型）;<strong>执行 L1 正则化鼓励权重值为0.</strong></li><li>直观地说，较小的权重会减少隐藏神经元的影响。在那种情况下，那些隐藏的神经元变得可以忽略不计，神经网络的整体复杂性得到降低。不太复杂的模型通常会避免数据中的建模噪声，因此不会出现过度拟合。</li></ul><h5 id="正则化之L1和L2"><a href="#正则化之L1和L2" class="headerlink" title="正则化之L1和L2"></a>正则化之L1和L2</h5><p>误差 = 偏差 + 方差 + 噪声</p><ul><li><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度， 即刻画了学习算法本身的拟合能力</li><li><strong>方差</strong>度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li><li><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界。</li></ul><p><strong>正则化就是在目标损失函数上面添加一个正则项：</strong></p><script type="math/tex; mode=display">L1 \;  Regularization：\quad \mathcal{L}_{L1} = \mathcal{L} + \alpha ||w||_1 = \mathcal{L} + \frac{\alpha}{n}\sum_{i=1}^{n}|w_i|</script><script type="math/tex; mode=display">L2 \;  Regularization：\quad \mathcal{L}_{L2} = \mathcal{L} + \alpha ||w||_2^2 = \mathcal{L} + \frac{\alpha}{n}\sum_{i=1}^{n}w_i^2</script><p>其中 $\alpha$ 是正则化参数，决定了我们对模型进行正则化的程度（惩罚力度），$w$ 是权重。加上正则项，就是希望我们的<strong>代价函数小</strong>，同时也希望我们这里的 <strong>$w_i$ 小，这就是说明每个样本的权重都很小</strong>，这样模型就不会太多的关注某种类型的样本， 模型参数也不会太复杂，有利于缓解过拟合现象。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230329/2.png" style="zoom:67%;" /></p><p>等高线图中：</p><ul><li>彩色的圆圈就是损失函数值 $\mathcal{L}$ 等高线，也就是说同一条线上的损失函数值是相等的；</li><li>左边菱形代表L1的等高线；右边圆形代表L2的等高线;</li><li>假设二维权重下的示意图 $(w_1,w_2)$;</li></ul><p>分析：</p><ul><li><p>损失函数值 $\mathcal{L}$ 等高线和正则项等高线，分别代表了两个优化问题。对原始的损失函数中添加了正则项之后，优化问题就变成了两个子优化问题的博弈。</p></li><li><p>左边是涉及 L1 正则项的，其中 A,B,C点产生的损失值是相等的，黑色的菱形(坐标中心) 表示L1正则项的一个等高线，假设$w_1 + w_2 = r,假设r=1$ ，那么菱形框任意一个点产生的正则项值都是1。我们现在考虑A, B,C三点的目标函数，他们的损失值 $\mathcal{L}$ 是相等的，那么这三个点哪个点的Regularization最小呢？ C点的正则项是1， 而我们会发现A点和B点的正则项都比C大（其实A,B,C这一条损失值 $\mathcal{L}$ 等高线上就是C的正则项最小）， 所以C点的目标函数最小。所以我们如果在L1的情况下找个最优解，既能损失 $\mathcal{L}$ 最小，又能权重最小，那么往往这个最优解就发生在坐标轴上，也就是上面的C点。 这样一个神奇的现象发生了，w1竟然等于0， 就说明参数解有0 的出现了， $w_1$ 就消失了。所以L1正则项一般会产生稀疏的解，也就是有项权重解为 0 。这是因为加上L1之后，我们参数的解往往会发生在坐标轴上导致某些参数的值为0;</p></li><li>右边是涉及 L2 正则项的，其中A’, B’, C’点产生的损失值也是相等的，黑色的圆形(坐标中心) 表示L2正则项的一个等高线，假设$w_1^2 + w_2^2 = r,假设r=1$ 。和上面的分析一样，，如果我们在A’, B’, C’点确定最优解的话，依然是C’点， 因为它在损失值  $\mathcal{L}$ 相等的情况下正则最小。但是我们发现L2正则下不过出现某个参数为0的情况，而是w1和w2都比较小。所以L2正则项的最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化 $|w|$ 时，就会使每一项趋近于0。</li><li>对于 L1 正则化中，在正则项等高线和损失值等高线相切/相交始终都是在坐标轴上的；最优点会保持在 L1 等高线的断点处，依然在坐标轴上，故某个权重值会变为0。</li><li>而 L2 的相切点则只能无限接近坐标轴，惩罚力度再大，都到不了0。</li><li>惩罚力度 $\alpha$ 相当于 L1中的菱形在坐标中心处大小 $w_1 + w_2 = \alpha$ ；相当于 L2 中圆形在坐标中心处的大小  $w_1^2 + w_2^2 = \alpha$ 。当损失值  $\mathcal{L}$ 和正则项之和最小时，上述的博弈取得平衡。而此时平衡点一定是相切点/端点。相切点的具体位置，取决于正则项的惩罚力度，也就是公式里的α。每一个平衡点，对应着一个α的设置。<strong>当惩罚力度大时，相切/相交点往靠近坐标轴的方向移动</strong>，而<strong>惩罚力度小时，相切/相交往远离坐标轴的方向移动</strong>。</li></ul><p><strong>L1正则化的特点：</strong></p><ul><li>不容易计算， 在零点连续但不可导， 需要分段求导</li><li>L1模型可以将 一些权值缩小到零（稀疏）</li><li>执行隐式变量选择。 这意味着一些变量值对结果的影响降为0， 就像删除它们一样</li><li>其中一些预测因子对应较大的权值， 而其余的（几乎归零）</li><li>由于它可以提供稀疏的解决方案， 因此通常是建模特征数量巨大时的首选模型</li><li>它任意选择高度相关特征中的任何一个， 并将其余特征对应的系数减少到0</li><li>L1范数对于异常值更具提抗力</li></ul><p><strong>L2正则化的特点：</strong></p><ul><li>容易计算， 可导， 适合基于梯度的方法</li><li>将一些权值缩小到接近0</li><li>相关的预测特征对应的系数值相似</li><li>当特征数量巨大时， 计算量会比较大</li><li>对于有相关特征存在的情况， <strong>它会包含所有这些相关的特征</strong>， 但是相关特征的权值分布取决于相关性。</li><li><strong>对异常值非常敏感</strong></li><li>相对于L1正则会更加准确</li></ul><p>为什么增加<strong>正则项后会使得权重变小，缩小了模型的权重的取值范围</strong>？在 pytorch 中 L2正则项 称为 <strong>权值衰减(weight decay)</strong>，为啥叫衰减？怎么实现衰减的？</p><ul><li><p>模型参数的更新公式：</p><script type="math/tex; mode=display">w_{i+1} = w_i - \varepsilon  \frac{ \partial Loss}{ \partial w_i}</script></li><li><p>在损失函数上添加了一个 L2 正则项后 $Loss_{new} = Loss + \frac{\alpha}{2}\sum_{i=1}^{n}w_i^2$ ,那么参数的更新变为：</p><script type="math/tex; mode=display">w_{i+1} = w_i - \varepsilon  \frac{ \partial Loss_{new}}{ \partial w_i} = w_i - \varepsilon (\frac{ \partial Loss}{ \partial w_i} + \alpha * w_1 ) = w_i - \varepsilon \frac{ \partial Loss}{ \partial w_i} - \varepsilon * \alpha * w_i</script><p>唯一的区别是，通过添加正则化项，我们从当前权重（等式中的第一项）中引入了额外的减法。</p><p>换句话说，与损失函数的梯度无关，每次执行更新时，我们都会使权重变小一点。也可以看成参数 $w_i$ 本身发生一个衰减即使移项 $w_i(1-\varepsilon <em> \alpha)，其中 \varepsilon </em> \alpha \in(0,1)$。</p></li><li><p>pytorch 中使用 L2 就是在优化器中（例：<code>torch.optim.SGD</code>）指定 weight_decay 这个参数即可。 </p></li></ul><h5 id="正则化之Dropout"><a href="#正则化之Dropout" class="headerlink" title="正则化之Dropout"></a>正则化之Dropout</h5><p><strong>Dropout叫做随机失活。就是给出一个概率(随机)，让某个神经元的权重为0(失活)，dropout 意味着在以某种概率P进行训练期间，神经网络的神经元在训练期间被关闭</strong>。一般是应用到<strong>全连接层（nn.Linear）</strong>，而且只在训练过程中使用，在推理预测是不使用的。所以一般在pytorch中设置 <code>model.train,model.eval</code> 来限制Dropout和BN。</p><p><strong>为什么dropout可以抑制过拟合呢</strong>？dropout通过随机失活神经元，使得模型参数减少，网络变得简单，不会拟合出较为复杂的曲线，事实上也就相当于正则。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230329/3.jpg" alt=""></p><p>就是每一层，让某些神经元不起作用，这样就就相当于把网络进行简化了(左边和右边可以对比），我们有时候之所以会出现过拟合现象，就是因为我们的网络太复杂了，参数太多了，并且我们后面层的网络也可能太过于依赖前层的某个神经元，加入Dropout之后， 首先网络会变得简单，减少一些参数，并且由于不知道浅层的哪些神经元会失活，导致后面的网络不敢放太多的权重在前层的某个神经元，这样就减轻了一个过渡依赖的现象， 对特征少了依赖， 从而有利于缓解过拟合。</p><p>关于Dropout还有一个注意的问题，就是<strong>数据的尺度变化</strong>。 这个是什么意思呢？ 我们用Dropout的时候是这样用的： <strong>只在训练的时候开启Dropout，而测试的时候是不用Dropout的，也就是说模型训练的时候会随机失活一部分神经元， 而测试的时候我们用所有的神经元，</strong>那么这时候就会出现这个数据尺度的问题， <strong>所以测试的时候，所有权重都乘以1-drop_prob</strong>， 以保证训练和测试时尺度变化一致，drop_prob是我们的随机失活概率。</p><p>pytorch 中的 <code>torch.nn.Dropout(p=0.5, inplace=False)</code>：p就是随机失活概率。第一点就是Dropout加的时候注意放置的位置，第二点就是由于Dropout操作；模型训练和测试是不一样的，上面我们说了，训练的时候采用Dropout而测试的时候不用Dropout， 那么我们在迭代的时候，就得告诉网络目前是什么状态，如果要测试，就得先用<code>.eval()</code>函数告诉网络一下子，训练的时候就用<code>.train()</code>函数告诉网络一下子。<strong>Pytorch在实现Dropout的时候， 是权重乘以 $\frac{1}{1-p}$  的，也就是除以1-p, 这样就不用再测试的时候权重乘以1-p了， 也没有改变原来数据的尺度.</strong></p><p>Reference:  <a href="https://blog.csdn.net/wuzhongqiang/article/details/105612578">正则化</a> ; <a href="https://zhuanlan.zhihu.com/p/376000306">L1和L2的正确姿势</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Regularization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metric_in_Semantic_Segmentation</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Metric-in-Semantic-Segmentation/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Metric-in-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<h3 id="Metric-in-Semantic-Segmentation"><a href="#Metric-in-Semantic-Segmentation" class="headerlink" title="Metric in Semantic Segmentation"></a>Metric in Semantic Segmentation</h3><h4 id="Dice-Coefficient"><a href="#Dice-Coefficient" class="headerlink" title="Dice Coefficient"></a>Dice Coefficient</h4><ul><li><p>定义：Dice系数，是一种集合相似度度量函数，通常用于计算两个样本点的相似度（值范围为[0, 1]）。用于分割问题，分割最好时为1，最差为0。（可解决样本不均衡问题）</p></li><li><p>计算公式：</p><script type="math/tex; mode=display">Dice = \frac{2 \times \left |  X \cap Y \right |}{\left | X \right | + \left | Y \right | } =            \frac{2 \times 预测正确的结果 }{ 真实结果 + 预测结果 } \qquad\qquad X是标签；Y是预测值</script><p>其中 $\left |  X \cap Y \right |$ 是表示 X 和 Y 的交集<strong>（逐像素相乘后相加）</strong>，$\left | X \right | $ 和 $ \left | Y \right |$ 表示其元素的个数<strong>（逐像素（Or平方）相加）</strong>。在计算的时候一般会加一个smooth，防止分母出现0</p></li><li><p>Dice loss = 1 - Dice</p></li><li><p>代码实现1（简单）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># H*W，只针对二维，多类多batch分开计算</span><br><span class="line">def dice_coeff(pred, target):</span><br><span class="line">    smooth = 1.</span><br><span class="line">    num = pred.size(0)</span><br><span class="line">    m1 = pred.view(num, -1)  # Flatten </span><br><span class="line">    m2 = target.view(num, -1)  # Flatten</span><br><span class="line">    intersection = (m1 * m2).sum() # 计算交集</span><br><span class="line">    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)</span><br></pre></td></tr></table></figure></li><li><p>代码实现2（标准）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># H*W</span><br><span class="line">def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):</span><br><span class="line">    # Average of Dice coefficient for all batches, or for a single mask</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    if input.dim() == 2 and reduce_batch_first:</span><br><span class="line">        raise ValueError(f&#x27;Dice: asked to reduce batch </span><br><span class="line">        but got tensor without batch dimension (shape &#123;input.shape&#125;)&#x27;)</span><br><span class="line"></span><br><span class="line">    if input.dim() == 2 or reduce_batch_first:</span><br><span class="line">    # torch.dot 点乘，对应元素相乘后相加，一个值，分子交集</span><br><span class="line">        inter = torch.dot(input.reshape(-1), target.reshape(-1))</span><br><span class="line">        # 分母，并集</span><br><span class="line">        sets_sum = torch.sum(input) + torch.sum(target)</span><br><span class="line">        if sets_sum.item() == 0:</span><br><span class="line">            sets_sum = 2 * inter</span><br><span class="line">        return (2 * inter + epsilon) / (sets_sum + epsilon)</span><br><span class="line">    else:</span><br><span class="line">        # compute and average metric for each batch element</span><br><span class="line">        dice = 0</span><br><span class="line">        for i in range(input.shape[0]):</span><br><span class="line">            dice += dice_coeff(input[i, ...], target[i, ...])</span><br><span class="line">        return dice / input.shape[0]</span><br><span class="line"></span><br><span class="line">def multiclass_dice_coeff(input: Tensor, target: Tensor,</span><br><span class="line">  reduce_batch_first: bool = False, epsilon=1e-6):</span><br><span class="line">    # Average of Dice coefficient for all classes</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    dice = 0</span><br><span class="line">    for channel in range(input.shape[1]):</span><br><span class="line">        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], </span><br><span class="line">           reduce_batch_first, epsilon)</span><br><span class="line"></span><br><span class="line">    return dice / input.shape[1]</span><br><span class="line"></span><br><span class="line">def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):</span><br><span class="line"># 在调用的时候，groud-truth若是多类别，需要进行one-hot编码</span><br><span class="line"># 【B,C,H,W】target and input</span><br><span class="line">    # Dice loss (objective to minimize) between 0 and 1</span><br><span class="line">    assert input.size() == target.size()</span><br><span class="line">    fn = multiclass_dice_coeff if multiclass else dice_coeff</span><br><span class="line">  return 1 - fn(input, target, reduce_batch_first=True)</span><br></pre></td></tr></table></figure></li></ul><h4 id="Mean-Intersection-over-Union"><a href="#Mean-Intersection-over-Union" class="headerlink" title="Mean Intersection over Union"></a>Mean Intersection over Union</h4><ul><li><p>mIoU：Mean Intersection over Union，均交并比，为语义分割的标准度量。其计算所有<strong>类别交集和并集之比</strong>的平均值.</p></li><li><p>先验提示：</p><ul><li>TP(真正): 预测正确, 预测结果是正类, 真实是正类</li><li>FP(假正): 预测错误, 预测结果是正类, 真实是负类</li><li>FN(假负): 预测错误, 预测结果是负类, 真实是正类</li><li>TN(真负): 预测正确, 预测结果是负类, 真实是负类  # 跟该类别无关,所以不包含在并集中</li></ul></li><li><p>mIoU的计算：直观理解，计算两圆交集（橙色部分）与两圆并集（红色+橙色+黄色）之间的比例，理想情况下两圆重合，比例为1</p><script type="math/tex; mode=display">mIoU = \frac{1}{k+1} \sum_{i=0}^{k} \frac{TP}{FN+FP+TP}</script><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230326/1.png" style="zoom: 50%;" /></p></li><li><p>计算：</p><ul><li><p>先求混淆矩阵：K 分类问题就会生成 K * K 的混淆矩阵。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230707/1.jpg" style="zoom: 50%;" /></p><p>假设有150个样本数据，预测类别1，2，3各有50 个，分类结束的混淆矩阵为上：</p><p>每一行之和表示该类别的真实样本数量，每一列之和表示被预测为该类别的样本数量</p></li></ul><p>第一行说明有43个属于第一类别的样本被正确预测为了第一类，有两个属于第一类别的样本被错误预测成为了第二类。</p><ul><li><p>再求 mIoU：</p><p><strong>mIoU = 混淆矩阵对角线的值  / (混淆矩阵的每一行再加上每一列，最后减去对角线上的值)</strong></p><p>混淆矩阵:  对角线上的值的和代表分类正确的像素点个数(preb与target一致),对角线之外的其他值的和代表所有分类错误的像素的个数。</p><p>混淆矩阵矩阵中 (x, y) 位置的元素代表该张图片中真实类别为 x ,被预测为 y 的像素个数。</p></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 1先求混淆矩阵</span><br><span class="line">def _fast_hist(self, label_pred, label_true):</span><br><span class="line">        # 找出标签中需要计算的类别,去掉了背景</span><br><span class="line">        mask = (label_true &gt;= 0) &amp; (label_true &lt; self.num_classes)</span><br><span class="line">        # np.bincount计算了从0到n**2-1这n**2个数中每个数出现的次数，返回值形状(n, n)</span><br><span class="line">        hist = np.bincount(self.num_classes * label_true[mask].astype(int) +</span><br><span class="line">                            label_pred[mask], minlength=self.num_classes ** 2)</span><br><span class="line">                            .reshape(self.num_classes,self.num_classes)</span><br><span class="line">        return hist</span><br><span class="line"># 2根据混淆矩阵求mIoU</span><br><span class="line"># 输入：预测值和真实值 [batch_size, H, W] </span><br><span class="line"># 语义分割的任务是为每个像素点分配一个label</span><br><span class="line">def ev aluate(self, predictions, gts):</span><br><span class="line">    for lp, lt in zip(predictions, gts):</span><br><span class="line">    assert len(lp.flatten()) == len(lt.flatten())</span><br><span class="line">    self.hist += self._fast_hist(lp.flatten(), lt.flatten())</span><br><span class="line">    # miou</span><br><span class="line">    # 每个类别 iou</span><br><span class="line">    iou = np.diag(self.hist) / (self.hist.sum(axis=1) + self.hist.sum(axis=0) np.diag(self.hist))</span><br><span class="line">    # 取平均值</span><br><span class="line">    miou = np.nanmean(iou) </span><br></pre></td></tr></table></figure></li></ul></li><li><p>常用求mIoU代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># 输入 pred，target 【B,H,W】</span><br><span class="line"># 第一种方式 比较合适我理解</span><br><span class="line">def iou_mean(pred, target, n_classes = 1):</span><br><span class="line">    # n_classes ：the number of classes in your dataset,not including background</span><br><span class="line">    # for mask and ground-truth label, not probability map</span><br><span class="line">    ious = [] #每个类别的 IoU</span><br><span class="line">    iousSum = 0</span><br><span class="line">    pred = pred.view(-1)</span><br><span class="line">    target = target.view(-1)</span><br><span class="line">    # Ignore IoU for background class (&quot;0&quot;)</span><br><span class="line">    for cls in range(1, n_classes+1):  </span><br><span class="line">      pred_inds = pred == cls</span><br><span class="line">        target_inds = target == cls</span><br><span class="line">        # Cast to long to prevent overflows</span><br><span class="line">        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()  </span><br><span class="line">        union = pred_inds.long().sum().data.cpu().item() </span><br><span class="line">        + target_inds.long().sum().data.cpu().item() - intersection</span><br><span class="line">        if union == 0:</span><br><span class="line">          ious.append(float(&#x27;nan&#x27;))  # If there is no ground truth, do not include in evaluation</span><br><span class="line">        else:</span><br><span class="line">          ious.append (float(intersection) / float(max(union, 1)))</span><br><span class="line">          iousSum += float(intersection) / float(max(union, 1))</span><br><span class="line">       </span><br><span class="line">      return iousSum/n_classes  # mIoU</span><br><span class="line">      </span><br><span class="line"># 第二种方式</span><br><span class="line"># &#x27;K&#x27; classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.</span><br><span class="line">def intersectionAndUnion(output, target, K, ignore_index=255):</span><br><span class="line">    assert output.ndim in [1, 2, 3]</span><br><span class="line">    assert output.shape == target.shape</span><br><span class="line">    output = output.reshape(output.size).copy()</span><br><span class="line">    target = target.reshape(target.size)</span><br><span class="line">    output[np.where(target == ignore_index)[0]] = ignore_index</span><br><span class="line">    intersection = output[np.where(output == target)[0]]</span><br><span class="line">    area_intersection, _ = np.histogram(intersection, bins=np.arange(K + 1))</span><br><span class="line">    area_output, _ = np.histogram(output, bins=np.arange(K + 1))</span><br><span class="line">    area_target, _ = np.histogram(target, bins=np.arange(K + 1))</span><br><span class="line">    area_union = area_output + area_target - area_intersection</span><br><span class="line">    </span><br><span class="line">    ious = area_intersection / area_union+epsilon  # 是一个array，代表每个类别的IoU</span><br><span class="line">    mIoU = np.nanmean(ious)  # mIoU</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Dice和IoU的联系：</p><script type="math/tex; mode=display">IoU = \frac{TP}{FN+FP+TP}</script><script type="math/tex; mode=display">Dice = \frac{2 \times \left |  X \cap Y \right |}{\left | X \right | + \left | Y \right | }</script><p>其中在 Dice 中 $\left |  X \cap Y \right |$ 就是 TP，$\left | X \right |$ 假设是ground-truth的话就是 FN+TP，$\left | Y \right |$ 假设是预测的 mask的话就是 TP+FP：</p><script type="math/tex; mode=display">Dice = \frac{2 \times TP}{TP+FN+TP+FP}</script><p>得到：</p><script type="math/tex; mode=display">IoU = \frac{Dice}{2-Dice}</script><p>根据在【0，1】值域中的函数图像，可以发现：</p><ul><li>IoU和Dice同时为0，同时为1；这很好理解，就是全预测正确和全部预测错误</li><li>在相同的预测情况下，可以发现Dice给出的评价会比IoU高一些</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Metric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Normalized_Summary</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Normalized-Summary/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Normalized-Summary/</url>
      
        <content type="html"><![CDATA[<h3 id="归一化总结-Normalization"><a href="#归一化总结-Normalization" class="headerlink" title="归一化总结(Normalization)"></a>归一化总结(Normalization)</h3><p>神经网络的学习的本质就是学习数据的分布，使模型收敛，得到学习数据的特性。如果没有对数据进行<strong>归一化处理</strong>，那么<strong>每一批次训练的数据的分布就有可能不一样</strong>。从大的方面来讲，神经网络需要在多个分布中找到一个合适的平衡点；从小的方面来说，由于每层网络的输入数据在不断的变化，这会导致不容易找到合适的平衡点，最终使得构建的神经网络模型不容易收敛。当然，如果只是对输入数据做归一化，这样只能保证数据在输入层是一致的，并不能保证每层网络的输入数据分布是一致的，所以在神经网络模型的<strong>中间层也需要加入归一化处理</strong>。利用随机梯度下降更新参数时，每次参数更新都会导致网络中间每一层的输入的分布发生改变。<strong>越深的层，其输入分布会改变的越明显</strong>。</p><p>内部协变量偏移(<strong>Internal Covariate Shift</strong>):也就是<strong>在训练过程中，隐层的输入分布老是变来变去</strong>， 每一层的参数在更新过程中，会改变下一层输入的分布，神经网络层数越多，变现的越明显。为了解决内部协变量偏移问题，这就要使得每一个神经网络层的输入的分布在训练过程保持一致。</p><h4 id="批量归一化-Batch-Normalization-BN"><a href="#批量归一化-Batch-Normalization-BN" class="headerlink" title="批量归一化(Batch Normalization, BN)"></a>批量归一化(Batch Normalization, BN)</h4><h5 id="为什么需要BN"><a href="#为什么需要BN" class="headerlink" title="为什么需要BN?"></a>为什么需要BN?</h5><p>在深层网络中的训练中，由于反向传播算法，<strong>模型的参数</strong>在发生指数型变化(因为是<strong>链式传播</strong>)，从而导致每一层的输入分布会发生剧烈变化，这就会引起两个问题：</p><ul><li><p>网络需要不断调整来适应输入数据分布的变化，导致网络学习速度的降低</p></li><li><p>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度，训练不稳定。</p></li><li><p>什么是梯度饱和区？</p><p>当我们在神经网络中采用饱和激活函数（saturated activation function）时，例如sigmoid，tanh激活函数，很容易使得模型训练陷入梯度饱和区，此时梯度会变得很小接近于0，从而导致网络收敛很慢。有两种解决方法：</p><ol><li><p>线性整流函数ReLU可以在一定程度上解决训练进入梯度饱和区的问题。</p></li><li><p>我们可以让激活函数的输入分布保持在一个稳定的状态来尽可能避免他们陷入梯度饱和区，这就是BN的想法。</p></li></ol></li></ul><h5 id="解决上述问题"><a href="#解决上述问题" class="headerlink" title="解决上述问题"></a>解决上述问题</h5><ul><li>固定网络每一层<strong>输入值的分布</strong>来缓解这两个问题。比如归一化到标准正态分布。</li></ul><h5 id="BN基本原理和公式"><a href="#BN基本原理和公式" class="headerlink" title="BN基本原理和公式"></a>BN基本原理和公式</h5><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/1.png" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/2.png" style="zoom:80%;" /></p><ul><li>$\gamma $ 和 $\beta $ : 是两个可训练参数，主要是在一定程度上恢复数据本身的表达能力，对规范化后的数据进行线性处理。</li></ul><h5 id="BN的计算"><a href="#BN的计算" class="headerlink" title="BN的计算"></a>BN的计算</h5><p>选择 torch.nn.BatchNorm2d为例子，给定特征图 【N, H, W, C】，其中N是batch_size, HW特征图的宽高，C是通道数，那么上面公式的B就是下图【N,C,HW】中的蓝色部分：保留通道 C 计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/3.jpg" style="zoom:80%;" /></p><ul><li><p>也就是说 <strong>BN是对不同样本里面的同一个特征通道进行归一化处理，逐特征维度归一化</strong>，可训练参数 $\gamma $ 和 $\beta $  的维度是 C。保留通道C计算均值和方差，故有2C个可训练参数（C个 $\gamma $ 和C个 $\beta $  ），2C个不可训练参数（均值mean和方差var）。</p></li><li><p><strong>BN训练时的均值和方差</strong>：<strong>该批次</strong>内数据相应维度的均值与方差</p></li><li><p><strong>BN测试时的均值和方差</strong>：<strong>基于所有批次</strong>的期望(无偏估计)计算所得，训练阶段根据 mini-batch 的数据计算均值和方差，并使用滑动平均法计算<strong>全局均值和方差</strong>；推理阶段使用训练阶段计算的全局均值和方差参与计算。</p></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">def BatchNorm(x, gamma, beta, eps=1e-5): </span><br><span class="line">    # x: input shape [N, C, H, W] </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    mean = torch.mean(input=x, dim=[0,2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[0,2,3], keepdim=True) </span><br><span class="line">    # mean, var shape : [1, C, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"></span><br><span class="line"># numpy版本 更对(两个可学习的参数)</span><br><span class="line">def batch_normalize2d(x, alpha=1.0, beta=0.0, eps=1e-5):</span><br><span class="line"># x: input shape [N, C, H, W] </span><br><span class="line">    mean = np.mean(x, axis=(0, 2, 3), keepdims=True)  # [1, C, 1, 1]</span><br><span class="line">    variance = np.var(x, axis=(0, 2, 3), keepdims=True)  # [1, C, 1, 1]</span><br><span class="line">    alpha = np.zeros_like(mean) + alpha #[1,C,1,1]</span><br><span class="line">    beta = np.zeros_like(mean) + beta #[1,C,1,1]</span><br><span class="line">    x = (data - mean) / np.sqrt(variance + eps) </span><br><span class="line">    return x * alpha + beta</span><br></pre></td></tr></table></figure></li></ul><h5 id="BN的优点缺点"><a href="#BN的优点缺点" class="headerlink" title="BN的优点缺点"></a>BN的优点缺点</h5><ul><li><p>优点：加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失，同时起到一定的正则化作用。</p><ul><li>BN将每层的数据进行标准化，并通过可学习参数 $\gamma $ 和 $\beta $  两个学习参数来调整这种分布。使得网络中<strong>每层输入数据的分布相对稳定，加速模型的训练速度</strong>，解决“internal covariate shift”的问题。</li><li>允许网络使用饱和性激活函数(sigmoid,tanh)，<strong>缓解了梯度消失的问题</strong>，<strong>避免了梯度弥散和爆炸</strong>。BN可以控制数据的分布范围，在遇到sigmoid或者tanh等激活函数时，不会使数据落在饱和区域导致梯度弥散。并且BN可以避免ReLU激活函数数据死亡的问题。</li><li>降低权重初始化的困难，在深度网络中，网络的最终训练效果也受到初始化的影响，初始化决定了神经网络最终会收敛到哪个局部最小值中，具有随机性。通过BN来标准化分布，<strong>可以降低初始化权重的影响</strong>。</li><li>因为不同的mini_batch均值和方差都有所不同，这就<strong>为网络的学习过程增加了随机噪音</strong>，与dropout随机关闭神经元给网络带来的噪音类似，一定程度上起到了正则化的作用。<strong>在正则化方面，一般全连接层用dropout，卷积层用BN。</strong></li><li>BN中的  $\gamma $ 和 $\beta $ 的作用：<ul><li>保证了模型的capacity，意思就是，γ和β作为调整参数可以调整被BN刻意改变过后的输入，即能够保证能够还原成原始的输入分布。BN对每一层输入分布的调整有可能改变某层原来的输入，当然有了这两个参数，经过调整也可以不发生改变，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。</li><li>适应激活函数，如果是sigmoid函数，那么BN后的分布在0-1之间，由于sigmoid在接近0的地方趋于线性，非线性表达能力可能会降低，因此通过γ和β可以自动调整输入分布，使得非线性表达能力增强。</li><li>如果激活函数为ReLU，那么意味着将有一半的激活函数无法使用，那么通过β可以进行调整参与激活的数据的比例，防止dead-Relu问题。</li></ul></li></ul></li><li><p>缺点</p><ul><li><p>BN特别依赖于大的batch_size，而由于显卡等硬件限制，我们大多数batch_size都设置的较小，性能会急剧下降。</p></li><li><p>对于序列化数据的网络不太适用，尤其是序列样本长度不同时。如RNN，LSTM。</p></li></ul></li></ul><h4 id="层归一化-Layer-Normalization"><a href="#层归一化-Layer-Normalization" class="headerlink" title="层归一化(Layer Normalization)"></a>层归一化(Layer Normalization)</h4><p>如果一个神经元的净输入分布在神经网络中是动态变化的，比如循环神经网络，那么无法应用批归一化操作。</p><p>层归一化和批归一化不同的是，<strong>层归一化是对一个中间层的所有神经元进行归一化</strong>。</p><p><strong>注意</strong>：LayerNorm的均值和方差是根据单个数据计算的，所以<strong>不需要计算全局均值和全局方差</strong>。类似BN的计算公式，只不过是在计算均值和方差是在不同的维度上进行的。</p><h5 id="LN基本原理和公式"><a href="#LN基本原理和公式" class="headerlink" title="LN基本原理和公式"></a>LN基本原理和公式</h5><p>基本原理和公式与 BN的差不多，只不过是计算均值和方差在不一样的维度罢了。</p><h5 id="LN的计算"><a href="#LN的计算" class="headerlink" title="LN的计算"></a>LN的计算</h5><p>选择 torch.nn.LayerNorm为例（<em>normalized_shape=(C,H,W)</em>），对于【N, C, H, W】的特征图（下图可以理解为【N ,C, H*W】，保留通道 N 计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/4.jpg" style="zoom:80%;" /></p><ul><li><p>注意与BatchNorm2d的不同：</p><ul><li>当torch.nn.LayerNorm的elementwise_affine为True时， $\gamma $ 和 $\beta $ 的参数数量分别时 C*H*W 。</li><li>当torch.nn.LayerNorm的elementwise_affine为False时，没有 $\gamma $ 和 $\beta $ 参数</li></ul></li><li><p><strong>训练和推理阶段</strong>：均值和方差根据输入数据计算，不需要在训练集上用滑动平均方法计算，所以不保存均值和方差。</p></li><li><p>代码：实现的时候γ和β参数的维度和一开始想的不一样（一开始以为和batch_norm2d一样，那应该是2N个可学习参数，实际上是根据元素数量来的），看了源码才发现没有均值和方差这2个参数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">def LayerNorm(x, gamma, beta, eps=1e-5): </span><br><span class="line">    # x: input shape [N, C, H, W] </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    mean = torch.mean(input=x, dim=[1,2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[1,2,3], keepdim=True) </span><br><span class="line">    # mean, var shape: [N, 1, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"></span><br><span class="line"># numpy版本 更对(两个可学习的参数)</span><br><span class="line">def layer_normalize2d(x, alpha=1.0, beta=0.0, eps=1e-5):</span><br><span class="line"># x: input shape [N, C, H, W] </span><br><span class="line">    mean = np.mean(x, axis=(1, 2, 3), keepdims=True)  # [N, 1, 1, 1]</span><br><span class="line">    variance = np.var(x, axis=(1, 2, 3), keepdims=True)  # [N, 1, 1, 1]</span><br><span class="line">    alpha = np.zeros(shape=(1,) + x.shape[1:]) + alpha #[1,C,H,W]</span><br><span class="line">    beta = np.zeros(shape=(1,) + x.shape[1:]) + beta #[1,C,H,W]</span><br><span class="line">    x = (x- mean) / np.sqrt(variance + eps)</span><br><span class="line">    return x * alpha + beta</span><br></pre></td></tr></table></figure><p>在这种方法中，batch(N) 中的每个示例都在 [C, H, W] 维度上进行了归一化。 与 BN 一样，它可以加速和稳定训练，并且不受批次的限制。 此方法可用于批量为 1 的在线学习任务。</p></li></ul><h4 id="实例归一化-Instance-Normalization"><a href="#实例归一化-Instance-Normalization" class="headerlink" title="实例归一化(Instance Normalization)"></a>实例归一化(Instance Normalization)</h4><p>IN是针对图像像素做归一化处理，适用于生成模型中，例如图像的风格化迁移等。</p><h5 id="IN基本原理和公式"><a href="#IN基本原理和公式" class="headerlink" title="IN基本原理和公式"></a>IN基本原理和公式</h5><p>基本原理和公式与 BN的差不多，只不过是计算均值和方差在不一样的维度罢了。</p><h5 id="IN的计算"><a href="#IN的计算" class="headerlink" title="IN的计算"></a>IN的计算</h5><p>以 torch.nn.InstanceNorm2d（track_running_stats=True, affine=True）为例，对于【N, C, H, W】的特征图（下图可以理解为[N ,C, H*W]），保留通道N和C计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/5.jpg" style="zoom:80%;" /></p><ul><li><p>相信有很多朋友都是根据这张图来理解Instance Norm，一眼看上去均值mean和方差variance的维度应该是[N, C]，但是打印出runing_mean和runing_var维度一看，竟然是[C]！What?</p><p>好家伙，经过多次测试，终于搞明白了，原来在归一化计算（一、归一化  中的公式）的时候，均值和方差维度是[N, C]，但是因为batch_size可能会变化，所以running_mean和running_var保存的时候把[N,C]的均值和方差取了个均值，所以runing_mean和runing_var维度是[C]。</p></li><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#torch版本</span><br><span class="line">def InstanceNorm(x, gamma, beta, eps=1e-5): </span><br><span class="line">    # x: input shape [N, C, H, W] </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    mean = torch.mean(input=x, dim=[2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[2,3], keepdim=True) </span><br><span class="line">    # mean, var  shape: [N, C, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"></span><br><span class="line">#numpy版本</span><br><span class="line">def instance_normalize2d(x, alpha=1.0, beta=0.0, eps=1e-5):</span><br><span class="line">    mean = np.mean(x, axis=(2, 3), keepdims=True)  # [N, C, 1, 1]</span><br><span class="line">    variance = np.var(x, axis=(2, 3), keepdims=True)  # [N, C, 1, 1]</span><br><span class="line">    alpha = np.zeros_like(mean) + alpha</span><br><span class="line">    beta = np.zeros_like(mean) + beta</span><br><span class="line">    # print(mean[:, :, 0, 0].mean(axis=0), variance[:, :, 0, 0].mean(axis=0))</span><br><span class="line">    x = (x - u) / np.sqrt(variance + eps) </span><br><span class="line">return x * alpha + beta</span><br></pre></td></tr></table></figure></li></ul><h4 id="组归一化-Group-Normalization"><a href="#组归一化-Group-Normalization" class="headerlink" title="组归一化(Group Normalization)"></a>组归一化(Group Normalization)</h4><p>InstanceNorm就是GroupNorm的特例，当Group Norm分组和channels相同时，就是instance norm，当分组为1时，就是LayerNorm。 GN 将通道分成组并在它们之间进行标准化。 该方案使计算独立于批量大小。</p><h5 id="GN基本原理和公式"><a href="#GN基本原理和公式" class="headerlink" title="GN基本原理和公式"></a>GN基本原理和公式</h5><p>基本原理和公式与 BN的差不多，只不过是计算均值和方差在不一样的维度罢了。</p><h5 id="GN计算"><a href="#GN计算" class="headerlink" title="GN计算"></a>GN计算</h5><p>以 torch.nn.GroupNorm为例，对于【N, C, H, W】的特征图（下图可以理解为[N ,C, H*W]），需要先将通道C维度划分为G个组得到新的特征图【N, G,C/G H, W】，然后保留通道N和G计算均值和方差。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/6.jpg" style="zoom:80%;" /></p><ul><li><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def GroupNorm(x, gamma, beta, G, eps=1e-5): </span><br><span class="line">    # x: input features with shape [N, C, H, W] </span><br><span class="line">    # G : number of groups </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, G, C // G, H, W]) </span><br><span class="line">    mean = torch.mean(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    # mean, var shape : [N, G, 1, 1, 1] </span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, C, H, W]) </span><br><span class="line">    return x * gamma + beta</span><br><span class="line"># 可学习参数的不同(更好)    </span><br><span class="line">def GroupNorm(x, G, gamma=1.0, beta=0.0, eps=1e-5): </span><br><span class="line">    # x: input features with shape [N, C, H, W] </span><br><span class="line">    # G : number of groups </span><br><span class="line">    N, C, H, W = x.shape </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, G, C // G, H, W]) </span><br><span class="line">    mean = torch.mean(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    var = torch.var(input=x, dim=[2,3,4], keepdim=True) </span><br><span class="line">    # mean, var shape : [N, G, 1, 1, 1] </span><br><span class="line">    gamma = np.zeros_like(mean) + gamma</span><br><span class="line">    beta = np.zeros_like(mean) + beta</span><br><span class="line">    x = (x - mean) / torch.sqrt(var + eps) </span><br><span class="line">    x = torch.reshape(input=x, shape=[N, C, H, W]) </span><br><span class="line">    return x * gamma + beta</span><br></pre></td></tr></table></figure></li></ul><h4 id="可切换归一化-Switchable-Normalization"><a href="#可切换归一化-Switchable-Normalization" class="headerlink" title="可切换归一化(Switchable Normalization)"></a>可切换归一化(Switchable Normalization)</h4><p>BN、LN、IN这些归一化方法往往能提升模型性能，但当你接收一个任务时，具体选择哪个归一化方法仍然需要人工选择，这往往需要大量的对照实验或者开发者本身优秀的经验才能选出最合适的归一化方法，因此SN出场了。</p><p>它的算法核心在于提出了一个可微的归一化层，可以让模型根据数据来学习到每一层该选择的归一化方法，或是三个归一化方法的加权和。所以SN是一个任务无关的归一化方法，在分类，检测，分割，IST，LSTM等各个方向的任务中，均取得了非常好的效果。</p><p>SN算法是为三组不同的γ和β分别学习共六个标量值$(W_k,W_k^{‘})$，得到他们的加权和：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/9.png" alt=""></p><p>其中=$W={IN,LN,BN}$。这样仅仅通过增加六个参数，由三种不同统计方法的共同优化，得到更好的归一化结果，但训练过程比较复杂。因此其比其他的归一化方法存在更高的鲁棒性、通用性和多样性。</p><h4 id="权重归一化-Weight-Normalization"><a href="#权重归一化-Weight-Normalization" class="headerlink" title="权重归一化(Weight Normalization)"></a>权重归一化(Weight Normalization)</h4><p>权重归一化(Weight Normalization)是对神经网络的连接权重进行归一化，通过再参数化(Reparameterization)方法，将连接权重分解为长度和方向两种参数。</p><p>已经对输入和层输出进行了标准化，唯一剩下的就是权重。因为它们可以在没有任何控制的情况下变大，尤其是当我们无论如何都要标准化输出时。 通过标准化权重，我们实现了更平滑的损失和更稳定的训练。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/7.jpg" style="zoom:80%;" /></p><ul><li><p>代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def WeightStand(w, eps=1e-5): </span><br><span class="line">    # w: input features shape [Cin, Cout, kernel_size, kernel_size] </span><br><span class="line">    mean = torch.mean(input=w, dim=[0,2,3], keepdim=True) </span><br><span class="line">    var = torch.var(input=w, dim=[0,2,3], keepdim=True) </span><br><span class="line">    # mean, var shape : [1, Cout, 1, 1] </span><br><span class="line">    w = (w - mean) / torch.sqrt(var + eps) </span><br><span class="line">    return w</span><br></pre></td></tr></table></figure></li></ul><h4 id="归一化对比"><a href="#归一化对比" class="headerlink" title="归一化对比"></a>归一化对比</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20230325/8.jpg" alt=""></p><ul><li><p>BN：<strong>取整个Batch-size,多个样本做归一化</strong>。</p></li><li><p>LN：<strong>取同一个样本的不同通道做归一化，逐样本归一化</strong>。是由Hinton及其学生提出，可以很好的用在序列型网络如RNN中，同时LN在训练和预测时均值方差都由当前样本确定，这与BN不同。可以不进行批训练。</p></li><li><p>IN：<strong>仅仅对每一个样本的每一个通道做归一化</strong>。主要用于生成模型中， feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格，如图片风格迁移，图片生成结果主要依赖于某个图像实例，所以BN不行</p></li><li><p>GN：<strong>介于LN和IN之间的一种方法，对每个样本的多个通道进行归一化</strong>。用由何凯明团队提出，优化了BN在batch_size较小时的劣势，适用于占用显存较大的任务，如图像分割，一般为16个通道为一组(经验)</p></li></ul><p><strong>目前BN使用最为广泛，能加快模型收敛速度，提高网络泛化性</strong>，但是存在小batch时错误率的问题。LN多用于RNN中，且不需要批训练，但在输入的特征区别较大时不建议使用。</p><p>IN用于图像的风格化迁移方面，不会受到通道数和batch size的影响，但特征通道之间的存在相关性时，则不建议使用。GN避免了BN的问题，训练时与batch size大小无关，但验证效果比BN差一些。<strong>SN是让模型根据数据来学习到每一层该选择的归一化方法或是BN、IN、GN归一化方法的加权和，但计算复杂</strong>。WN在噪声较大时能取得更好的效果，不受限于batch，但是对初始参数较为敏感，目前使用极少。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data_Enhancemenct</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Data-Enhancemence/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/Data-Enhancemence/</url>
      
        <content type="html"><![CDATA[<h2 id="Data-Enhancement"><a href="#Data-Enhancement" class="headerlink" title="Data Enhancement"></a>Data Enhancement</h2><ul><li>数据增强可以提高泛化能力，但这一过程依赖于数据集，而且需要专门知识。其次，数据增强假定领域内样本都是同一类，且没有对不同类不同样本之间领域关系进行建模。</li><li>避免过拟合。当数据集具有某种明显的特征,例如数据集中图片基本在同一个场景中拍摄,使用Cutout方法和风格迁移变化等相关方法可避免模型学到跟目标无关的信息。</li><li>提升模型鲁棒性,降低模型对图像的敏感度。当训练数据都属于比较理想的状态,碰到一些特殊情况,如遮挡,亮度,模糊等情况容易识别错误,对训练数据加上噪声,掩码等方法可提升模型鲁棒性。</li><li>增加训练数据,提高模型泛化能力。</li><li>避免样本不均衡。在工业缺陷检测方面,医疗疾病识别方面,容易出现正负样本极度不平衡的情况,通过对少样本进行一些数据增强方法,降低样本不均衡比例</li></ul><p><strong>常规弱增强</strong></p><ul><li>随机缩放，随机按 [0.5, 2.0] 调整图像大小。</li><li>随机翻转，以 0.5 的概率水平翻转图像。</li><li>随机裁剪，从图像中随机裁剪一个区域（513 × 513，769 × 769）</li></ul><p><strong>常规强增强</strong></p><ul><li>Identity，返回原始图像。</li><li>反相，将图像像素反相。</li><li>自动对比度，最大化（正常化）图像对比度。</li><li>均衡，均衡图像直方图。</li><li>高斯模糊，使用高斯核对图像进行模糊处理。</li><li>对比度，以 [0.05, 0.95] 调整图像对比度。</li><li>锐度，以 [0.05, 0.95] 的幅度调整图像的锐度。</li><li>色彩，通过 [0.05, 0.95] 增强图像的色彩平衡</li><li>亮度，以 [0.05, 0.95] 调节图像亮度</li><li>色调，以 [0.0, 0.5] 的幅度抖动图像的色调</li><li>Posterize，将每个像素降低到 [4,8] 位。</li><li>日晒化，将图像中所有高于阈值 [1,256] 的像素反相。</li><li>CutMix，</li></ul><p><strong>空间几何变换</strong></p><ul><li><p>翻转，翻转包括水平翻转和垂直翻转。</p></li><li><p>crop，裁剪图片的感兴趣区域（ROI），通常在训练的时候，会采用随机裁剪的方法，下图为随机裁剪4次的效果。</p></li><li><p>旋转，对图像做一定角度对旋转操作，看看效果。</p></li><li><p>缩放变形，随机选取图像的一部分，然后将其缩放到原图像尺度。</p></li><li><p>仿射变换，同时对图片做裁剪、旋转、转换、模式调整等多重操作。</p></li><li><p>视觉变换，对图像应用一个随机的四点透视变换。</p></li><li>分段仿射（PiecewiseAffine），分段仿射在图像上放置一个规则的点网格，根据正态分布的样本数量移动这些点及周围的图像区域。</li></ul><p><strong>噪声类</strong></p><p>随机噪声是在原来的图片的基础上，随机叠加一些噪声。</p><ul><li>高斯噪声</li><li><p>CoarseDropout，在面积大小可选定、位置随机的矩形区域上丢失信息实现转换，所有通道的信息丢失产生黑色矩形块，部分通道的信息丢失产生彩色噪声。</p></li><li><p>SimplexNoiseAlpha，产生连续单一噪声的掩模后，将掩模与原图像混合。</p></li><li><p>FrequencyNoiseAlpha。在频域中用随机指数对噪声映射进行加权，再转换到空间域。在不同图像中，随着指数值逐渐增大，依次出现平滑的大斑点、多云模式、重复出现的小斑块。</p></li></ul><p><strong>模糊类</strong></p><p>减少各像素点值的差异实现图片模糊，实现像素的平滑化。</p><ul><li>高斯模糊</li><li><p>ElasticTransformation，根据扭曲场的平滑度与强度逐一地移动局部像素点实现模糊效果。</p></li><li><p>HSV对比度变换，通过向HSV空间中的每个像素添加或减少V值，修改色调和饱和度实现对比度转换。</p></li><li><p>RGB颜色扰动，将图片从RGB颜色空间转换到另一颜色空间，增加或减少颜色参数后返回RGB颜色空间。</p></li><li><p>随机擦除法，对图片上随机选取一块区域，随机地擦除图像信息。</p></li><li><p>超像素法（Superpixels），在最大分辨率处生成图像的若干个超像素，并将其调整到原始大小，再将原始图像中所有超像素区域按一定比例替换为超像素，其他区域不改变。</p></li><li><p>转换法（invert），按给定的概率值将部分或全部通道的像素值从v设置为255-v。</p></li><li><p>边界检测（EdgeDetect），检测图像中的所有边缘，将它们标记为黑白图像，再将结果与原始图像叠加。</p></li><li><p>GrayScale，将图像从RGB颜色空间转换为灰度空间，通过某一通道与原图像混合。</p></li><li><p>锐化（sharpen）与浮雕（emboss），对图像执行某一程度的锐化或浮雕操作，通过某一通道将结果与图像融合。</p></li></ul><h3 id="Mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION"><a href="#Mixup-BEYOND-EMPIRICAL-RISK-MINIMIZATION" class="headerlink" title="Mixup: BEYOND EMPIRICAL RISK MINIMIZATION"></a>Mixup: BEYOND EMPIRICAL RISK MINIMIZATION</h3><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/1.png" alt=""></p><ul><li>代码：<a href="https://github.com/facebookresearch/mixup-cifar10">GitHub - facebookresearch/mixup-cifar10: mixup: Beyond Empirical Risk Minimization</a></li><li>代码1：<a href="https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch/blob/master/Residual-Attention-Network/train_mixup.py">https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch/blob/master/Residual-Attention-Network/train_mixup.py</a></li><li>论文：<a href="https://arxiv.org/pdf/1710.09412.pdf">https://arxiv.org/pdf/1710.09412.pdf</a></li><li>参考链接：<a href="https://www.zhihu.com/question/308572298，https://blog.csdn.net/u013841196/article/details/81049968，https://blog.csdn.net/sinat_36618660/article/details/101633504?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;utm_relevant_index=1">https://www.zhihu.com/question/308572298，https://blog.csdn.net/u013841196/article/details/81049968，https://blog.csdn.net/sinat_36618660/article/details/101633504?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;utm_relevant_index=1</a></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/2.jpg" alt=""></p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/3.jpg" alt=""></p><ul><li><p>对于输入的一个batch的待测图片images，我们将其和随机抽取的图片进行融合，融合比例为lam，得到混合张量inputs；</p></li><li><p>第1步中图片融合的比例lam是[0,1]之间的随机实数，符合beta分布，相加时两张图对应的每个像素值直接相加，即 inputs = lam <em> images + (1-lam) </em> images_random</p></li><li><p>将上面得到的混合张量inputs传递给model得到输出张量outpus，随后计算损失函数时，我们针对两个图片的标签分别计算损失函数，然后按照比例lam进行损失函数的加权求和，即loss = lam <em> criterion(outputs, targets_a) + (1 - lam) </em> criterion(outputs, targets_b)；</p></li><li><p><strong>Mixup核心思想：两张图片采用比例混合，label也需要按照比例混合</strong></p></li><li><p>计算损失函数有两个视角：首先应用于目标检测</p><ul><li>对相应的lable进行线性加权，由于lable采用one-hot编码可以理解为对k个类别的每个类给出样本属于该类的概率。加权以后就变成了”two-hot”，也就是认为样本同时属于混合前的两个类别</li><li>不混合label，而是用加权的输入在两个label上分别计算<strong>cross-entropy loss</strong>，最后把两个loss加权作为最终的loss。由于cross-entropy loss的性质，这种做法和把label线性加权是等价的（代码采用这种方式）</li></ul></li><li><p>代码实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">for i,(images,target) in enumerate(train_loader):</span><br><span class="line">    # 1.input output</span><br><span class="line">    images = images.cuda(non_blocking=True)</span><br><span class="line">    target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)</span><br><span class="line">    # 2.mixup  </span><br><span class="line">    # 一般情况下，config.alpha=1，是个超参数</span><br><span class="line">    alpha=config.alpha</span><br><span class="line">    lam = np.random.beta(alpha,alpha)</span><br><span class="line">    index = torch.randperm(images.size(0)).cuda()</span><br><span class="line">    inputs = lam*images + (1-lam)*images[index,:]</span><br><span class="line">    targets_a, targets_b = target, target[index]</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)</span><br><span class="line">    # 3.backward</span><br><span class="line">    optimizer.zero_grad()   # reset gradient</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()        # update parameters of net</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/4.jpg" alt=""></p><h3 id="Cutout-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutou"><a href="#Cutout-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutou" class="headerlink" title="Cutout:Improved Regularization of Convolutional Neural Networks with Cutou"></a>Cutout:Improved Regularization of Convolutional Neural Networks with Cutou</h3><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/5.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/uoguelph-mlrg/Cutout">https://github.com/uoguelph-mlrg/Cutout</a></li><li>论文：<a href="https://arxiv.org/pdf/1708.04552.pdf">https://arxiv.org/pdf/1708.04552.pdf</a></li></ul><h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>这篇文章的出发点除了解决遮挡问题外，还有从dropout上得到启发（所以也称为Cutout）。众所周知，Dropout随机隐藏一些神经元，最后的网络模型相当于多个模型的集成。类似于dropout的思路，这篇文章将drop用在了输入图片上，并且drop掉连续的区域——即矩形区域。通过<strong>patch的遮盖</strong>让网络学习到遮挡的特征。cutout不仅能够让模型学习到如何辨别他们，同时还能更好地结合上下文从而关注一些局部次要的特征。作为一个正则化方法，防止CNN过拟合。cutcout方法很简单，就是在训练的时候，在随机位置应用一个方形矩阵。作者认为这种技术鼓励网络去利用整个图片的信息，而不是依赖于小部分特定的视觉特征。</p><p>Cutout 出发点和随机擦除一样，也是模拟遮挡，目的是提高泛化能力，实现上比<strong>Random Erasing(类似的数据增强方法)</strong>简单，随机选择一个固定大小的正方形区域，然后采用全0填充就OK了，当然为了避免填充0值对训练的影响，应该要对数据进行中心归一化操作，norm到0。</p><h4 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h4><p>需要注意的是作者发现cutout区域的大小比形状重要，所以cutout只要是正方形就行，非常简单。具体操作是利用固定大小的矩形对图像进行遮挡，在矩形范围内，所有的值都被设置为0，或者其他纯色值。而且擦除矩形区域存在一定概率不完全在原图像中的（论文设置50%）</p><ul><li><p>代码实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class Cutout(object):</span><br><span class="line">    &quot;&quot;&quot;Randomly mask out one or more patches from an image.</span><br><span class="line">    Args:</span><br><span class="line">        n_holes (int): Number of patches to cut out of each image. default 1</span><br><span class="line">        length (int): The length (in pixels) of each square patch.  default 16</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, n_holes, length):</span><br><span class="line">        self.n_holes = n_holes</span><br><span class="line">        self.length = length</span><br><span class="line">    def __call__(self, img):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            img (Tensor): Tensor image of size (C, H, W).</span><br><span class="line">        Returns:</span><br><span class="line">            Tensor: Image with n_holes of dimension length x length cut out of it.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        h = img.size(1)</span><br><span class="line">        w = img.size(2)</span><br><span class="line">        mask = np.ones((h, w), np.float32)</span><br><span class="line">        for n in range(self.n_holes):</span><br><span class="line">            y = np.random.randint(h)</span><br><span class="line">            x = np.random.randint(w)</span><br><span class="line">            y1 = np.clip(y - self.length // 2, 0, h)</span><br><span class="line">            y2 = np.clip(y + self.length // 2, 0, h)</span><br><span class="line">            x1 = np.clip(x - self.length // 2, 0, w)</span><br><span class="line">            x2 = np.clip(x + self.length // 2, 0, w)</span><br><span class="line">            mask[y1: y2, x1: x2] = 0.</span><br><span class="line">        mask = torch.from_numpy(mask)</span><br><span class="line">        mask = mask.expand_as(img)</span><br><span class="line">        img = img * mask</span><br><span class="line">        return img</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/6.jpg" alt=""></p><h3 id="CutMix-Regularization-Strategy-to-Train-Strong-Classifiers-with-Localizable-Features"><a href="#CutMix-Regularization-Strategy-to-Train-Strong-Classifiers-with-Localizable-Features" class="headerlink" title="CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features"></a>CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</h3><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/7.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/clovaai/CutMix-PyTorch">https://github.com/clovaai/CutMix-PyTorch</a></li><li>论文：<a href="https://arxiv.org/pdf/1905.04899v2.pdf">https://arxiv.org/pdf/1905.04899v2.pdf</a></li></ul><h4 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h4><ul><li><p><strong>区域的丢弃策略（Reginal dropout strategies）</strong>能够增强卷积神经网络分类器的性能。</p><ul><li>优点： 该策略能够使得模型更有效的关注目标的明显部分，有好的泛化和目标定位能力。</li><li>缺点： 利用黑色像素或者随机噪声填充移除区域，这样的操作在训练过程中容易导致信息的缺失和无效性。</li><li><strong>解决方法： 提出了CutMix——使用训练集中的图像填补移除区域</strong></li></ul></li><li><p><strong>CutMix最大程度的利用了同一张图像上的两种不同图像信息。具有更好的分类性能和目标定位功能。CutMix在填充了训练集中的其他照片的同时，label也进行了相同比例转换。</strong></p></li><li><strong>CutMix采用了cutout的局部融合思想，并不是采用全0的mask填充，而是采用另外一张图片的相同大小区域填充，就是混合两张图的局部区域，并且采用了mixup的混合label策略，看起来比较make sense</strong></li></ul><h4 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/8.jpg" alt=""></p><ul><li><p>代码</p><ul><li><p>整体算法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">for i, (input, target) in enumerate(train_loader):</span><br><span class="line">   # measure data loading time</span><br><span class="line">   data_time.update(time.time() - end)</span><br><span class="line">   input = input.cuda()</span><br><span class="line">   target = target.cuda()</span><br><span class="line">   r = np.random.rand(1)</span><br><span class="line">   if args.beta &gt; 0 and r &lt; args.cutmix_prob:</span><br><span class="line">      # 1.设定lambda的值，服从beta分布</span><br><span class="line">      lam = np.random.beta(args.beta, args.beta)  # args.beta 超参数 default 1</span><br><span class="line">      # 2.找到两个随机样本</span><br><span class="line">      rand_index = torch.randperm(input.size()[0]).cuda()</span><br><span class="line">      target_a = target</span><br><span class="line">      target_b = target[rand_index]</span><br><span class="line">      # 3.生成裁剪区域B</span><br><span class="line">      bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)</span><br><span class="line">      # 4.将原有的样本A中的B区域，替换成样本B中的B区域</span><br><span class="line">      input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]</span><br><span class="line">      # 5.根据裁剪区域坐标框的值调整lamda的值</span><br><span class="line">      lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))</span><br><span class="line">      # 6.将生成的新的训练样本丢到模型中进行训练</span><br><span class="line">      output = model(input)</span><br><span class="line">      # 7.按lamda值分配权重</span><br><span class="line">      loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>裁剪区域B的坐标值函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def rand_bbox(size, lam):</span><br><span class="line">    W = size[2]</span><br><span class="line">    H = size[3]</span><br><span class="line">    # 论文里的公式2：求出B的rw,rh</span><br><span class="line">    cut_rat = np.sqrt(1. - lam)</span><br><span class="line">    cut_w = np.int(W * cut_rat)</span><br><span class="line">    cut_h = np.int(H * cut_rat)</span><br><span class="line">    # 论文里的公式2：求出B的rx,ry</span><br><span class="line">    cx = np.random.randint(W)</span><br><span class="line">    cy = np.random.randint(H)</span><br><span class="line">    # 限制坐标区域不超过样本大小</span><br><span class="line">    bbx1 = np.clip(cx - cut_w // 2, 0, W)</span><br><span class="line">    bby1 = np.clip(cy - cut_h // 2, 0, H)</span><br><span class="line">    bbx2 = np.clip(cx + cut_w // 2, 0, W)</span><br><span class="line">    bby2 = np.clip(cy + cut_h // 2, 0, H)</span><br><span class="line">    # 返回裁剪B区域的坐标值</span><br><span class="line">    return bbx1, bby1, bbx2, bby2</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/9.jpg" alt=""></p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20220320/10.jpg" alt=""></p><ul><li>热力图：<strong>CutMix</strong>的操作使得模型能够从一幅图像上的局部视图上识别出两个目标，提高训练的效率。由图可以看出，<strong>Cutout</strong>能够使得模型专注于目标较难区分的区域（腹部），但是有一部分区域是没有任何信息的，会影响训练效率；<strong>Mixup</strong>的话会充分利用所有的像素信息，但是会引入一些非常不自然的伪像素信息。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Mixup:将随机的两张样本按比例混合，分类的结果按比例分配；</li><li>Cutout:随机的将样本中的部分区域cut掉，并且填充0像素值，分类的结果不变；</li><li><p>CutMix:就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值，分类结果按一定的比例分配</p></li><li><p>上述三种数据增强的区别：cutout 和 cutmix 就是填充区域像素值的区别；mixup和cutmix是混合两种样本方式上的区别：mixup是将两张图按比例进行插值来混合样本，cutmix是采用cut部分区域再补丁的形式去混合图像，不会有图像混合后不自然的情形</p></li><li><p>CutMix优点：</p><ul><li>在训练过程中不会出现非信息像素，从而能够提高训练效率；</li><li>保留了regional dropout的优势，能够关注目标的non-discriminative parts；</li><li>通过要求模型从局部视图识别对象，对cut区域中添加其他样本的信息，能够进一步增强模型的定位能力；</li><li>不会有图像混合后不自然的情形，能够提升模型分类的表现；</li><li>训练和推理代价保持不变。</li></ul></li><li><p><strong>more data enhancement</strong></p><ul><li><a href="https://cloud.tencent.com/developer/article/1904288">https://cloud.tencent.com/developer/article/1904288</a></li><li><a href="https://www.zhihu.com/question/319291048">https://www.zhihu.com/question/319291048</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Data_Enhancemenct </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers"><a href="#Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers" class="headerlink" title="Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with  Transformers"></a>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with  Transformers</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/fudan-zvg/SETR">https://github.com/fudan-zvg/SETR</a></li><li>论文：<a href="https://arxiv.org/abs/2012.15840">https://arxiv.org/abs/2012.15840</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>最新的语义分割方法采用具有编码器-解码器体系结构的全卷积网络（FCN）。编码器逐渐降低空间分辨率，并通过更大的感受野学习更多的抽象/语义视觉概念。由于上下文建模对于分割至关重要，因此最新的工作集中在通过扩张/空洞卷积或插入注意力模块来增加感受野。但是，基于编码器-解码器的FCN体系结构保持不变。</p><p>在本文中，我们旨在通过将语义分割视为序列到序列的预测任务来提供替代视角。具体来说，我们部署一个纯 transformer（即，不进行卷积和分辨率降低）将图像编码为一系列patch。通过在 transformer的每一层中建模全局上下文，此编码器可以与简单的解码器组合以提供功能强大的分割模型，称为SEgmentation TRansformer（SETR）。</p><p><strong>一个标准的FCN分割模型有一个编码器-解码器结构：编码器用于特征表示学习，而解码器用于编码器产生的特征表示的像素级分</strong>类。编码器由堆叠的卷积层组成，特征图的分辨率逐渐降低，编码器能够以逐渐增加的感受野学习更多的抽象/语义视觉概念。<br><strong>优点</strong>：translation equivariance：尊重了成像过程的本质，支持了模型对看不见的图像数据的泛化能力<br><strong>局部性</strong>：通过跨空间共享参数来控制模型的复杂性。<br><strong>缺点</strong>：感受野有限，难以学习无约束场景图像中的语义分割的长期依赖信息。<br><strong>解决方法：</strong><br>直接操作卷积运算：大内核尺寸（large kernel sizes），atrous卷积和图像/特征金字塔。<br>注意力模块集成到FCN架构中：对特征图中所有像素的全局交互进行建模。当应用于语义分割时，通常是是将注意力模块与位于顶部的注意力层结合到FCN架构中。不改变FCN模型结构的本质：<strong>编码器下采样输入的空间分辨率，用于区分语义类别的低分辨率特征映射；解码器将特征表示上采样为全分辨率分割映射</strong>。<br>本文中，我们用纯transformer 取代空间分辨率逐渐降低的基于堆叠卷积层的编码器，这种编码器将输入图像视为由学习到的面片嵌入表示的图像面片序列，并使用全局自关注建模对该序列进行转换，以进行有区别的特征表示学习。</p><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><ul><li><p>问题</p><p>典型的语义分割Encoder-Decoder结构以多次下采样损失空间分辨率为代价来抽取局部/全局特征。网络Layer一旦固定,每一层的感受野是受限的,因此要获得更大范围的语义信息,理论上需要更大的感受野即更深的网络结构。</p><p>如何既能够抽取<strong>全局的语义信息,</strong>又能尽量<strong>不损失分辨率,</strong>一直是语义分割的<strong>难点</strong></p></li><li><p>解决方法</p><ul><li>用常用于NLP领域的transformer作为Encoder来抽取全局的语义信息(整个过程不损失image分辨率),代替传统FCN的编码部分,从序列-序列学习的角度,为语义分割问题提供了一种新的视角；</li><li>将图像序列化处理,利用Transformer框架、完全用注意力机制来实现Encoder的功能；</li><li>提出三种复杂度不同的Decoder结构</li></ul></li><li><p>整体网络架构</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/2.jpg" alt=""></p></li><li><p><strong>part 1：图像序列化处理</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/5.jpg" alt=""></p></li></ul><p>image to sequenc，因为NLP中transformation的输入是一维序列,所以需要把图像(H <em> W </em> C)转换成 1D序列。<br>1）： 按pixel-wise进行flatten。考虑到计算量问题所以此方法不通。<br>2）： 按patch-wise进行flatten。本文采用此方法。</p><p>1、输入图像的大小 H <em> W </em> 3（256 <em> 256 </em> 3）的大小，patch_size = 16 <em> 16，因此图像序列化为 256/16 </em> 256 /16 = 256个 （16 <em> 16 </em> 3）的图片大小</p><p>2、向量化后的patch<code>p_i</code>经过<code>Linear Projection</code>function得到向量<code>e_i</code> ，旁边注释<code>e_i</code>是patch embedding,<code>p_i</code>是position embedding。</p><ul><li><strong>part 2：Transformer</strong></li></ul><p>这边采用的是纯 Transformer 的encoder结构，只不过中间重复叠用了24次，具体的使用可以查看 PIT， PVT，Swin Transformer 的总结文档。</p><ul><li><p><strong>part3 Decoder</strong></p><p>本文就提出了三种不一样的 decoder 的设计，分别如下</p><ul><li><p>Navite Upsampling（Naive）</p><p>2-layer：（1 <em> 1）conv  +  sync batch norm（w/ReLU）+ （1 </em> 1）conv</p><p>将Transformer输出的特征维度降到分类类别数后经过双线性上采样恢复原分辨率</p></li><li><p>Progressive UPsampling (PUP）</p><p>交替使用卷积层和两倍上采样操作，为了从<code>H/16 × W/16 × 1024</code> 恢复到<code>H × W × 19</code>(19是cityscape的类别数) 需要4次操作,以恢复到原分辨率。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/6.jpg" alt=""></p></li><li><p>Multi-Level feature Aggregation (MLA)</p><p>首先将Transformer的输出<code>&#123;Z1,Z2,Z3…ZLe&#125;</code>均匀分成M等份,每份取一个特征向量。如下图,24个transformer的输出均分成4份,每份取最后一个,即<code>&#123;Z6,Z12,Z18,Z24&#125;</code> .后面的Decoder只处理这些取出的向量。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/7.jpg" alt=""></p></li></ul><p>具体步骤：</p><p>1.2D —&gt; 3D。将<code>ZL</code> 从2D <code>(H × W)/256 × C</code>恢复到3D <code>H/16 × W/16 × C</code></p><p>2.经过3-layer的卷积<code>1 × 1, 3 × 3, and 3 × 3</code></p><p>3.双线性上采样<code>4×</code></p><p>4.自上而下的融合。以增强<code>Zl</code> 之间的相互联系,如上图最后一个<code>Zl</code>理论上拥有全部上面三个feature的信息,融合即cat</p><p>5.<code>3 × 3</code></p><p>6.双线性插值<code>4×</code> 恢复至原分辨率。</p></li><li><p><strong>损失函数设计</strong></p><p>totalloss = auxiliary loss + main loss</p><p>其中main loss为<code>CrossEntropyLoss</code> ,auxiliary loss在17年CVPR有提及</p></li></ul><h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h3><ul><li>We reformulate the image semantic segmentation problem from a sequence-to-sequence learning perspective, offering an alternative to the dominating encoder-decoder FCN model design.</li><li>As an instantiation, we exploit the transformer framework to implement our fully attentive feature representation encoder by sequentializing images.</li><li>To extensively examine the self-attentive feature presentations, we further introduce three different decoder designs with varying complexities. Extensive experiments show that our SETR models can learn superior feature representations as compared to different FCNs with and without attention modules, yielding new state of the art on ADE20K (50.28%), Pascal Context (55.83%) and competitive results on Cityscapes. Particularly, our entry is ranked the 1stplace in the highly competitive ADE20K test server leaderboard.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>在Cityscapes/ADE20K/PASCAL Context三个数据集上进行了实验。实验结果优于用传统FCN(with &amp; without attention module)抽特征的方法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/4.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/3.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><ul><li>卷积操作的感受野有限是传统FCN体系结构的一个内在限制。为突破该限制，逐渐提出两类方法<ul><li>改变卷积：包括增大卷积核kernel_size、Non-local(跟本文有点像，每次抽取的都是全局特征)和特征金字塔。例如DeepLab引入空洞卷积/SPP/ASPP。</li><li>将注意力模块集成到FCN体系结构中：一次对所有像素的全局信息抽取特征。例如PSANet提出点向空间注意模块、DANet嵌入channel attention和spatial attention。</li></ul></li><li>有人提到，本文是把ViT模型原封不动迁移过来了，替换了encoder，虽带来了精度的提升但模型的计算量和参数量都非常大。</li></ul><blockquote><p>ViT(Vision Transformer)首次证明了纯基于transformer的图像分类模型可以达到sota。</p></blockquote><ul><li>CNN是通过不断地堆积卷积层来完成对图像从局部信息到全局信息的提取,不断堆积的卷积层慢慢地扩大了感受野直至覆盖整个图像;但是transformer并不假定从局部信息开始,而且一开始就可以拿到全局信息,学习难度更大一些,但transformer学习长依赖的能力更强。</li><li>CNN结构更适合底层特征,Transformer更匹配高层语义。二者无绝对差别,就是看问题的尺度差异,本质都是消息传递。</li><li>现在对transformer理解还不充分,为啥Transformer之后没有改变分辨率,还要用Decoder来恢复原image的分辨率(得看看transformer那篇论文,或者评论区有好心人解答一下嘛)。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(CVPR) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(CVPR) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransFuse:Fusing_Transformers_and_CNNs_for_Medical_Image_Segmentation</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation"><a href="#TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation" class="headerlink" title="TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation"></a>TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/Rayicer/TransFuse">https://github.com/Rayicer/TransFuse</a></li><li>论文：<a href="https://arxiv.org/abs/2102.08005">https://arxiv.org/abs/2102.08005</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>传统CNN网络很难捕获长距离的依赖关系，而且一味的加深网络的深度会带来大量的计算冗余。</p><p>文章提出了一种并行分支的TransFuse网络，<strong>结合transformer和CNN两种网络架构，能同时捕获全局依赖关系和低水平的空间细节</strong>，文中还提出了一种BiFusion module用来混合两个分支所提取的图像特征。使用TransFuse，可以以较浅的方式有效地捕获全局依赖性和low-level空间细节</p><ul><li>transformer：good at modeling global context，<strong>but</strong> lack of spatial inductive-bias in modelling local information and limitations in capturing fine-grained details</li><li>CNN：low-level spatial details can be efficiently captured <strong>but</strong> lack of efficiency in capturing global context information</li></ul><p>TransFuse在多个医学分割任务中达到SOTA，并在降低参数和提高推理速度方面得到很大的提升。</p><ul><li><strong>advantage：</strong>firstly, by leveraging the merits of CNNs and Transformers, we argue that  TransFuse can capture global information without building very deep nets while  preserving sensitivity on low-level context; secondly, our proposed BiFusion  module may simultaneously exploit different characteristics of CNNs and  Transformers during feature extraction, thus making the fused representation  powerful and compact.</li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><ul><li><p><strong>整体网络架构</strong>，TransFuse包含两个分支，左边是transformer分支，右边是CNN分支，模型通过BiFusion层整合两个分支的特征，然偶经过上采样和attention-gated skip-connection输出分割结果</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/2.jpg" alt=""></p></li><li><p><strong>Transformer Branch</strong><br>Transformer Branch是一个完整的编解码结构，编码器部分使用的是transformer结构，解码器部分使用的是SERT中提到的渐进上采样（PUP）结构。</p></li><li><p><strong>CNN Branch</strong><br>CNN Branch使用ResNet的第四层，第三层和第二层的输出作为这一分支的输出，由于transformer可以捕获全局的上下文信息，故而CNN Branch并不需要设计的很深 。</p></li><li><p><strong>BiFusion Module</strong><br>BiFusion Module主要由通道注意力和空间注意力组成，对Transformer Branch做通道注意力，对CNN Branch做空间注意力。然后经过卷积，相乘，拼接，残差操作，实现两个分支的特征融合。</p><ul><li><p>通道注意力</p><p>特征的每一个通道都代表着一个专门的检测器，因此，通道注意力是关注什么样的特征是有意义的。为了汇总空间特征，作者采用了全局平均池化和最大池化两种方式来分别利用不同的信息。<strong>通道注意力聚焦在“什么”是有意义的输入图像</strong></p></li><li><p>空间注意力</p><p>空间注意力模块来关注哪里的特征是有意义的，<strong>空间注意力聚焦在“哪里”是最具信息量的部分</strong>，这是对通道注意力的补充。为了计算空间注意力，沿着通道轴应用平均池化和最大池操作，然后将它们连接起来生成一个有效的特征描述符</p></li><li><p>通道注意力和空间注意力详细请看 代表性论文 <strong>CBAM: Convolutional Block Attention Module</strong></p></li></ul></li><li><p>最后通过上采样和attention-gated skip-connection输出分割结果。</p></li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li><p>本文采用四个数据集进行验证，分别是，Polyp Segmentation，Skin Lesion Segmentation， Hip Segmentation，Prostate Segmentation</p></li><li><p>定量结果</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/4.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/5.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/3.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>文章使用transformer结构捕捉图像的全局上下文信息，并利用这一优点减小CNN结构的层数，只是用很少的卷积层提取局部空间信息作为transformer的补充，并通过BiFusion进行特征融合，最后通过Attention-gate，上采样输出分割结果。文中一共出现四种注意力机制。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer"><a href="#Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer" class="headerlink" title="Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer"></a>Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/zhiheLu/CWT-for-FSS">https://github.com/zhiheLu/CWT-for-FSS</a></li><li>论文：<a href="https://arxiv.org/abs/2108.03032">https://arxiv.org/abs/2108.03032</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>得益于大型的标签数据和深度学习算法的发展，语义分割方法近几年取得了很大的进步。但这些方法有两个局限：</p><p><strong>1）过度依赖带标签数据，而这些数据的获得通常消耗大量人力物力；</strong></p><p><strong>2）训练好的模型并不能处理训练过程中未见的新类别。</strong></p><p>面对这些局限，小样本语义分割被提出来，它的目的是通过对少量样本的学习来分割新类别。一般来说，小样本语义分割方法是通过用训练数据模拟测试环境进行元学习使得训练的模型有很好的泛化能力，从而在测试时可以仅仅利用几个样本的信息来迭代模型完成对新类别的分割。具体地，小样本分割模型是在大量的模拟任务上进行训练，每个模拟任务有两个数据组：Support set and Query set。Support set 是有标签的K-shot样本，而Query set只在训练的时候有标签。这样的模拟任务可以有效地模拟测试环境</p><ul><li>针对元学习的概念，可以参考：<a href="https://zhuanlan.zhihu.com/p/289043310">https://zhuanlan.zhihu.com/p/289043310</a></li><li>针对小样本学习的一些概念，可以参考：<a href="https://zhuanlan.zhihu.com/p/84290146">https://zhuanlan.zhihu.com/p/84290146</a></li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><p>一个<strong>小样本分类系统</strong>一般由三部分构成：<strong>编码器，解码器和分类器</strong>。其中，前两个模块模型比较复杂，最后一个分类器结构简单。我们发现现存的小样本分类方法通常在<strong>元学习</strong>的过程中更新所有模块或者除编码器外的模块，而所利用更新模块的数据仅仅有几个样本。在这样的情况下，我们认为模型更新的参数量相比于数据提供的信息量过多，从而不足以优化模型参数。基于此分析，我们提出了一个全新的元学习训练范式，即只对分类器进行元学习。为了方便读者更好的理解，论文给出了两种方法的对比，如下图，图(a)为传统方法，要训练三个模块，图(b)为本文方法，编码器解码器在经过大量有标记数据训练后便冻结，只调整分类器权重</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/2.jpg" alt=""></p><p>具体地，我们采用常规的分割方法对编码器和解码器进行训练，训练后在元学习的过程中不在更新。这是基于我们的假设：在大量标签数据训练下的模型可以提取有辨别性的特征，对一些新类别也有效，这也可以解释很多方法直接使用ImageNet预训练的模型进行特征提取。在分析数据的时候，我们发现Support set和Query set的数据有时有较大的类内差异，如下图。同样的类别，不同的角度即可产生很大的区别。这就使得利用Support set迭代的模型不能很好地作用在Query set上。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/3.jpg" alt=""></p><p>为了解决这个问题，我们提出了<strong>Classifier Weight Transformer</strong>来动态地利用Query set的特征信息来进一步更新分类器模块，从而提升分割任务性能。一个少镜头分割模型通常由三个模块组成：编码器、解码器和分类器。为了学习适应新类别，现有方法通常在编码器在 ImageNet 上进行预训练后对整个模型进行元学习 [23]。在情节训练阶段，模型的所有三个部分都是元学习的。训练后，给定一个带有带注释的支持集图像和用于测试的查询图像的新类，该模型有望使所有三个部分都适应新类。由于只有很少的带注释的支持集图像和复杂且相互关联的三部分，这种适应通常是次优的。为了克服这些限制，我们提出了一个分两个阶段的简单而有效的训练范式。在第一阶段，我们通过监督学习对编码器和解码器进行预训练，以获得更强的特征表示。在第二阶段，连同冻结的编码器和解码器，我们仅对分类器进行元训练。这是因为我们认为预训练的特征表示部分（即编码器和解码器）足以泛化到任何看不见的类别；因此，少样本语义分割的关键在于调整二元分类器（分离前景和背景像素），而不是从少样本样本中调整整个模型。我们方法的概述如下图所示。具体的算法参考原文。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/4.jpg" alt=""></p><p>者假设，一个学习了大量图片和信息的传统分割网络已经能够从任何一张图片中捕捉到充分的，有利于区分背景和前景的信息，无论训练时是否遇到了同类的图。那么面对少样本的新类时，只要对分类器进行元学习即可。 对于分类器的学习，作者提出了一种分类器权重转移方法CWT，根据每一张查询集图象，临时调整分类器参数。借助Transformer的思想，作者将分类器权重转化为Query，将图象提取出来的特征转化为Key和Value，然后根据这三个值调整分类器权重，最后通过残差连接，与原分类器参数求和     </p><p>本文提出的元学习名为episodic training。一般来说，本文提出的元学习针对是小样本的类进行学习。元学习分两步</p><ul><li>第一步是内循环，和预训练一样，根据支持集上的图片和mask进行训练，不过只修改分类器参数。文中指出，当新类样本数够大时，只使用外循环，即只更新分类器，就能匹敌SOTA，但是当面对小样本时，表现就不尽如人意。</li><li>第二步是外循环，根据每一副查询图片，微调分类器参数，而且微调后的参数只针对这一张查询图片，不能用于其他查询图象，也不覆盖修改原分类器参数。</li></ul><h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h3><ul><li>We propose a novel model training paradigm for few-shot semantic  segmentation. Instead of meta-learning the whole, complex segmentation model, we  focus on the simplest classifier part to make new-class adaptation more  tractable. </li><li>We introduce a novel meta-learning algorithm that leverages a  Classifier Weight Transformer (CWT) for adapting dynamically the classifier  weights to every query sample.</li><li>Extensive experiments with two popular  backbones (ResNet-50 and ResNet-101) show that the proposed methodyieldsa  newstate-of-the-artperformance, often surpassing existing alternatives,  especially on 5-shot case, by a large margin. Further, under a more challenging  yet practical cross-domain setting, the margin becomes even bigger.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>在两个标准小样本分割数据集PASCAL和COCO上，我们的方法在大多数情况下取得了最优的结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/5.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/6.jpg" alt=""></p><ul><li>跨数据集的情景下测试了我们模型的性能，可以看出我们的方法展示了很好的鲁棒性</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/7.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/7.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>提出了一种新的元学习训练范式来解决小样本语义分割问题。相比于现有的方法，这种方法更加简洁有效，只对分类器进行元学习。为了解决类内差异问题，我们提出<strong>Classifier Weight Transformer</strong>来利用Query特征信息来迭代分类器，从而获得更加鲁棒的分割效果。通过大量的实验，我们证明了方法的有效性。针对我们现在的创新点，找到存在的问题，采取简便的方法提升性能未必不是一个新的思路，就拿现在这个而言，并没有对编码器-解码-分类器三个进行元学习，而是只针对其中最简单的分类器进行创新，也采纳了transformer，也算是transformer的一个应用把。果然现在在cv领域transformer大放异彩，针对这种思想，之前也看过类似输入网络的图像大小对性能的影响，也看过动态预测调整输入图片大小的预测，也是一种思路，所以咋创新方面不用单单停留在网络的结构上，而是应该放在整体的架构的，因为任何一处的改变都可能对最后的分割性能产生不一样的影响。道阻且长，望君继续努力。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(ICCV) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(ICCV) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SOTR-Segmenting-Objects-with-Transformers</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="SOTR-Segmenting-Objects-with-Transformers"><a href="#SOTR-Segmenting-Objects-with-Transformers" class="headerlink" title="SOTR: Segmenting Objects with Transformers"></a>SOTR: Segmenting Objects with Transformers</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/first.png" alt=""></p><ul><li><p>代码：<a href="https://github.com/easton-cau/SOTR">https://github.com/easton-cau/SOTR</a></p></li><li><p>论文：<a href="https://arxiv.org/abs/2108.06747">https://arxiv.org/abs/2108.06747</a> </p></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><ul><li><p>首先作者研究了实例分割的发展历程，以及各个阶段方法的大概总结，提出个各个阶段的优点与不足的，在实例分割这块，主要的方法就是包括</p><p>Top-down instance segmentation 和 Bottom-up instance segmentation</p><ul><li><strong>Top-down instance segmentation</strong><ul><li><strong>proposal-based</strong>方法：基于目标检测，在得到目标检测框之后再在框内做语义分割分割前景背景，由于这种方法需要借助目标检测中的区域提议，因此该方法称为<strong>proposal-based</strong>方法.，这种方法就是遵循先检测后分割的范式的。缺点如下 例如 Mak-RCNN<ul><li>1）由于有限的感受野，CNN在高级视觉语义信息中相对缺乏 特征的连贯性来关联实例 , 导致对大对象的次优结果；</li><li>2）分割质量和推理速度都严重依赖对象检测 器，在复杂场景中性能较差。</li></ul></li></ul></li><li><p><strong>Bottom-up instance segmentation</strong></p><ul><li><strong>proposal-free方法</strong>：在语义分割图的基础上，将像素聚集到不同的实例上。这类方法的基本思想是利用CNN学到每个像素实例级的特征，接着用一种聚合方法将像素聚合成实例。这种方法通常分两步，一个是分割，一个是聚合。语义分割图获得之后，将像素一步步的聚合到不同的实例中。学习每像素嵌入(per-pixel embedding)和实例感知 特征(instance-aware features)，然后使用后处理技术依次分组，根据嵌入特征(embedding characteristics)将它们转换为实例，只要缺点如下<ul><li>不稳定的聚类（例如碎片化和联合掩码）以及对不同场景数据集的泛化能力差，当场景非常复杂并且一张图像中存在密集的物体时，背景像素上不可避免地 会损失大量的计算和时间</li></ul></li></ul></li><li><p><strong>STOR</strong>：就是一种 (Bottom-up) 自底向上的model模型，也就是基于<strong>proposal-free方法</strong>，有效地学习了位置敏感特征(position-sensitive features )，动态生成实例掩码 (dynamically generates instance masks )，无需后处理分组以及边界框位置和尺度的界限，<strong>SOTR</strong> 以图像为输入，结合 CNN 和 Transformer 模块来 提取特征，并直接对类概率和实例掩码进行预测。</p></li></ul></li><li><p><strong>Transformer</strong></p><ul><li><p>近几年来，由于 Transformer 的崛起，其在cv领域的重要性可想而知，在cv领域上，很多人试图完全 替代 卷积运算 或 将类 CNN 架构与transformer结合用于视觉任务中的特征提取，它可以轻松捕 获全局范围特征(global-range characteristics)并自然地对长距离语义依赖项进行建模</p><ul><li>self-attention，作为transformers 的关键机制，广泛地聚 合了来自整个输入域的特征和位置信息。因此基于transformer的模型可以更好地区分具有相同语义类 别的重叠实例，这使得它们比CNN更适合高级视觉任务。但是他也有不足：<ul><li>（1）典型的attention在提取<strong>(low-level features)</strong>方低级特征方面表现不佳，导致对小对象的错误预测</li><li>（2）由于广泛的特征图(feature map)，需要大量的内存和时间，尤其是在训练阶段</li><li><strong>（1）的问题可以通过结合 CNN 主干得到有效解决</strong></li></ul></li></ul></li><li><p>为了降低传统 self-attention 的内存和计算复杂度，我们提出了 <strong>twin attention</strong>（双注意力），一种替代的自注意 力自回归块，通过将全局空间 attention (注意力) 分解为独立的垂直和水平 attention (注意力) 来显着 减少计算和内存。与原始 Transformer 相比，这种精心设计的架构在计算和内存方面具有显着的节省，尤其是在用于密集预测（如实例分割）的大输入上。</p></li></ul></li><li><p>我们提出了一种创新的自底向上模型(bottom-up model)，称为 <strong>SOTR</strong>，巧妙地结 合了 CNN 和 Transformer 的优点。具体地说，我们采用Transformer模 型来获 取全局依赖关系并提取高级特征(<strong>high-level features</strong>) 以用于后续的函数头部（<strong>Functional heads</strong>）的预 测。其简化了分割管道，建 立在附加了两个并行子任务的替代 CNN 主干上：（1）通过transformer预测每个实例的类别和（2） 动态生成具有多个的(segmentation mask)分割掩码级上采样模块。 SOTR 可以分别通过特征金字塔 网络 (FPN) 和<strong>twin Transformer</strong>有效地提取较低级别的特征表示并捕获远程上下文依赖关系</p></li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><p><strong>SOTR 是一种 CNN-transformer 混合实例分割模型</strong>，可以同时学习 2D 平面信息表示并轻松捕获远程信息。 它遵循直接分割范式，首先将输入特征图划分为     <strong>patches</strong> （补丁），然后在动态分割每个实例的同时预测每个<strong>patches</strong> （补丁）的类别。 具体来说，我们的模型主要由三部分组成：1）从输入图像中提取图像特征的主干，尤其是低级和局部特征，2）一个用于建模全局和语义依赖关系的 <strong>Transformer</strong>  ，它附加了功能头以 分别预测每个<strong>patches</strong> （补丁）的类别和卷积核，以及 3) 一个多级上采样模块，通过在生成特征图和相应的卷积核之间执行动态卷积操作来生成最终的分割掩码(segmentation mask)。 总体框架如<strong>图 2</strong> 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-2.jpg" alt=""></p><p>SOTR 可以分别通过特征金字塔网络 (FPN) 和双Transformer有效地提取较低级别的特征表示并捕获远程上下文依赖关系。同时，与原始Transformer相比，所提出的双Transformer具有时间和资源效率，因为只涉及一行和一列注意力来编码像素。</p><ul><li><strong>twin attention（双注意力）</strong>机制，用稀疏表示来简化 <strong>attention(注意力)</strong> 矩阵。 我们的策略主要将感受野限制为固定步幅的设计块模式。 它首先计算每列中的 <strong>attention(注意力)</strong>，同时保持不同列中的元素独立。 该策略可以在水平尺度上聚合元素之间的上下文信息（见图 3（1））。 然后，在每一行内执行类似的 <strong>attention(注意力) </strong>以充分利用垂直尺度上的特征交互（如图 3（2）所示）。 两个尺度中的注意力依次连接为最后一个尺度，具有全局感受野，涵盖了两个维度的信息</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-3.png" alt=""></p><ul><li><strong>作者称之为 Twins</strong>，代码写的很复杂，但其实就是提前将行维度和列维度分别整合到前一个维度再输入 Attention 模块。这样做可以将时间复杂度降低为：</li></ul><script type="math/tex; mode=display">O((H \times W)^2) \quad to  \quad O(H \times W^2 + W \times H^2)^1</script><ul><li><strong>Functional heads</strong></li></ul><p>来自 <strong>Transformer</strong> 模块的特征图（feature maps）被输入到不同的<strong>函数头（functional heads）</strong>以进行后续预测。 <strong>类头(class head)</strong> 包括 单个线性层(linear)以输出 $N ×N ×M $ 的分类结果，<strong>其中 M 是类的数量</strong>。 由于每个patch(补丁) 只为一个中心落入patch(补丁) 的单个对象分配一个类别，如 YOLO [32]，我们利用多级预测并在不同特征级别共享头部，以进一步提高不同尺度对象的模型性能和效率 . <strong>核头(kernel head)</strong> 也由一个线性层(linear)组成，与 <strong>类头(class head)</strong>并行输出一个 $N×N×D$ 张量用于后续的 掩码(mask) 生成，其中张量表示具有D个参数的 $ N×N $ 卷积核。 在训练期间，<strong>Focal Loss [26]</strong> 被应用于分类，而这些卷积核的所有监督都来自最终的掩码(mask) 损失。</p><ul><li><strong>Mask</strong></li></ul><p>为了构建实例感知和位置敏感分割的掩码特征表示，一种直接的方法是对不同尺度的每个特征图进行预测（[36, 12] 等）。 但是，它会增加时间和资源。 受 <strong>Panoptic FPN [22]</strong> 的启发，我们设计了<strong>多级上采样模块(multi-level upsampling module)</strong>，将来自每个 <strong>FPN 级</strong>和 <strong>transformer</strong> 的特征合并为一个统一的掩码特征。 首先，从 <strong>transformer</strong>模块获得带有位置信息的相对低分辨率特征图P5，并结合<strong>FPN中的P2-P4</strong>执行融合。 对于每个尺度的特征图，操作了 3×3 Conv、Group Norm [39] 和 ReLU 的几个阶段。 然后P3-P5被双线性上采样 2×、4×、8×，分别为  $(\frac{H}{4},\frac{W}{4})$分辨率。 最后，将处理后的 P2-P5 加在一起后，执行逐点卷积和上采样以创建最终统一的 $ H×W$ 特征图。</p><p>例如掩码(mask)预测，SOTR 通过对上述统一特征图执行动态卷积运算为每个 patch(补丁) 生成掩码(mask)。 给定来自内核头部(kernel head)的预测卷积核  $K \quad \epsilon \quad K^{N \times N \times D} $  ，每个内核负责相应 patch(补丁)中实例的掩码(mask)生成。 具体操作可以表示如下：</p><script type="math/tex; mode=display">Z^{H \times W \times N^2} = F^{H \times W \times C} * K^{N \times N  \times D}</script><p>其中 <strong>*</strong> 表示卷积操作，Z是最终生成的掩码，维度为 $ H×W×N^2 $。 需要注意的是，D 的取值取决于卷积核的形状，即D等于$λ^2C$，其中 <strong>λ</strong> 为核大小。 最终的实例分割掩码可以由 <strong>Matrix NMS [37]</strong> 生成，每个掩码都由 <strong>Dice Loss [30]</strong> 独立监督。，</p><ul><li><p>代码的具体实现可以查看github上提供的源代码。参考知乎老哥的讲解：</p><p><a href="https://zhuanlan.zhihu.com/p/424036708">https://zhuanlan.zhihu.com/p/424036708</a> </p></li></ul><h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul><li>We introduce an innovative CNN-transformer-hybrid instance segmentation  framework, termed SOTR. It can effectively model local connectivity and  longrange dependencies leveraging CNN backbone and transformer encoder in the  input domain to make them highly expressive. What’s more, SOTR considerably  streamlines the overall pipeline by directly segmenting object instances without  relying on box detection. </li><li>We devise the twin attention, a new  position-sensitive self-attention mechanism, which is tailored for our  transformer. This well-designed architecture enjoys a significant saving in  computation and memory compared with original transformer, especially on large  inputs for a dense prediction like instance segmentation. </li><li>Apart from pure  transformer based models, the proposed SOTR does not need to be pre-trained on  large datasets to generalize inductive biases well. Thus, SOTR is easier applied  to insufficient amounts of data. </li><li>The performance of SOTR achieves 40.2% of AP  with the ResNet-101-FPN backbone on the MS COCO benchmark, outperforming most of  state-of-the-art approaches in accuracy. Furthermore, SOTR demonstrates  significantly better performance on medium (59.0%) and large objects (73.0%),  thanks to the extraction of global information by twin transformer.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>定量的结果：SOTR 在 MS COCO 数据集上表现良好，并超越了最先进的实例分割方法。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-7.jpg" alt=""></p><ul><li>定性结果：比起其他的分割方法，SOTR具有较好的分割性能</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-4.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>rethingking的话，我觉得，现在transformer那么火爆的时刻，本文其实相对简单，但是其的创新点很突出，很新颖。面对某些具体的任务，了解现在主流的方法并且找出他们各自的优点与不足，然后主要针对这些不足提出新的解决方案。在transformer广泛应用于cv领域后，找到transformer在cv领域后的不足之处，就比如提取low-level farture map特征和计算量内存上面存在不足，所以本文也是比较针对这个方面进行了研究，所以最后才可以提出一种创新的分割框架SOTR</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(ICCV) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(ICCV) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析UNet</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>研究一个深度学习算法，可以先看网络结构，看懂网络结构后，再Loss计算方法、训练方法等。本文主要针对UNet的网络结构进行讲解</p><p>卷积神经网络被大规模的应用在分类任务中，输出的结果是整个图像的类标签。但是UNet是像素级分类，输出的则是每个像素点的类别，且不同类别的像素会显示不同颜色，UNet常常用在生物医学图像上，而该任务中图片数据往往较少。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。</p><ul><li><strong>优点</strong><ul><li>输出结果可以定位出目标类别的位置；</li><li>由于输入的训练数据是patches，这样就相当于进行了数据增强，从而解决了生物医学图像数量少的问题，数据增强有利于模型的训练</li></ul></li><li><strong>缺点</strong><ul><li>训练过程较慢，网络必须训练每个patches，由于每个patches具有较多的重叠部分，这样持续训练patches，就会导致相当多的图片特征被多次训练，造成资源的浪费，导致训练时间加长且效率会低下。但是也会认为网络对这个特征进行多次训练，会对这个特征影响十分深刻，从而准确率得到改进。但是这里你拿一张图片复制100次去训练，很可能会出现过拟合的现象，对于这张图片确实十分敏感，但是拿另外一张图片来就可能识别不出了啦</li><li>定位准确性和获取上下文信息不可兼得，大的patches需要更多的max-pooling，这样会减少定位准确性，因为最大池化会丢失目标像素和周围像素之间的空间关系，而小patches只能看到很小的局部信息，包含的背景信息不够。</li></ul></li></ul><h4 id="网络结构原理"><a href="#网络结构原理" class="headerlink" title="网络结构原理"></a>网络结构原理</h4><p>UNet网络结构，最主要的两个特点是：U型网络结构和Skip Connection跳层连接。</p><p>UNet网络结构分为三个部分，原理图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/1.jpg" alt=""></p><ul><li><p>第一部分是<strong>主干特征提取部分</strong>，我们可以利用<strong>主干部分</strong>获得一个又一个的<strong>特征层</strong>，Unet的主干特征提取部分与VGG相似，为卷积和最大池化的堆叠。<strong>利用主干特征提取部分我们可以获得五个初步有效特征层</strong>，在第二步中，我们会利用这五个有效特征层可以进行特征融合。</p><ul><li><p>下采样</p></li><li><p>左边特征提取网络：使用conv和pooling，就是每次向下采样之前都会进行两次的卷积操作，然后向下采样，然后再进行两次卷积操作，以此往复，向下连续采样五次</p></li></ul></li><li><p>第二部分是<strong>加强特征提取部分</strong>，我们可以利用主干部分获取到的<strong>五个初步有效特征层</strong>进行上采样，并且进行特征融合，获得一个最终的，融合了<strong>所有特征的有效特征层</strong>。</p><ul><li><p>上采样</p></li><li><p>右边网络为特征融合网络：使用上采样产生的特征图与左侧特征图进行concatenate操作</p></li><li><p>Skip Connection中间四条灰色的平行线，Skip Connection就是在上采样的过程中，融合下采样过过程中的feature map。Skip Connection用到的融合的操作也很简单，就是将feature map的通道进行叠加，俗称Concat。</p></li><li><p>Concat操作也很好理解，举个例子：一本大小为10cm<em>10cm，厚度为3cm的书A，和一本大小为10cm</em>10cm，厚度为4cm的书B。将书A和书B，边缘对齐地摞在一起。这样就得到了，大小为10cm*10cm厚度为7cm的一摞书（就是直接把书叠起来的意思）</p></li><li><p>对于feature map，一个大小为<strong>256 <em> 256 </em> 64</strong>的feature map，即feature map的w（宽）为256，h（高）为256，c（通道数）为64。和一个大小为<strong>256 <em> 256 </em> 32</strong>的feature map进行Concat融合，就会得到一个大小为<strong>256 <em> 256 </em> 96</strong>的feature map。</p><p>在实际使用中，Concat融合的两个feature map的大小不一定相同，例如<strong>256 <em> 256 </em> 64</strong>的feature map和<strong>240 <em> 240 </em> 32</strong>的feature map进行Concat。</p><p>这种时候，就有两种办法：</p><ul><li><p>第一种：将大<strong>256 <em> 256 </em> 64</strong>的feature map进行裁剪，裁剪为<strong>240 <em> 240 </em> 64</strong>的feature map，比如上下左右，各舍弃8 pixel，裁剪后再进行Concat，得到<strong>240 <em> 240 </em> 96</strong>的feature map。</p></li><li><p>第二种：将小<strong>240 <em> 240 </em> 32</strong>的feature map进行padding操作，padding为<strong>256 <em> 256 </em> 32</strong>的feature map，比如上下左右，各补8 pixel，padding后再进行Concat，得到<strong>256 <em> 256 </em> 96</strong>的feature map。</p></li></ul><p>UNet采用的Concat方案就是第二种，将小的feature map进行padding，padding的方式是补0，一种常规的常量填充。</p></li></ul></li><li><p>第三部分是<strong>预测部分</strong>，我们会利用<strong>最终获得的最后一个有效特征层</strong>对每一个特征点进行分类，相当于对每一个像素点进行分类。<strong>（将最后特征层调整通道数，也就是我们要分类个数）</strong></p><ul><li>最后再经过两次卷积操作，生成特征图，再用两个卷积核大小为<strong>1*1</strong>的卷积做分类得到最后的两张heatmap，例如第一张表示第一类的得分，第二张表示第二类的得分heatmap，然后作为softmax函数的输入，算出概率比较大的softmax，然后再进行loss，反向传播计算。</li></ul></li></ul><h4 id="网络代码实现"><a href="#网络代码实现" class="headerlink" title="网络代码实现"></a>网络代码实现</h4><p>按照UNet的网络结构分parts去实现Unet结构，<strong>采取一种搭积木的方式，先定义各个独立的模块，最后组合拼接就可以！</strong></p><h5 id="DoubleConv模块"><a href="#DoubleConv模块" class="headerlink" title="DoubleConv模块"></a>DoubleConv模块</h5><p>如下图所示模块，连续的两个卷积的操作，在整个UNet网络中，主干特征提取网络和加强特征网络中各自使用了五次，每一层都会采取这个操作，故可以提取出来：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/2.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class DoubleConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.double_conv(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>nn.Sequential 是一个时许的容器，会将里面的 modle 逐一执行，执行顺序为：<strong>卷积-&gt;BN-&gt;ReLU-&gt;卷积-&gt;BN-&gt;ReLU。</strong></p></li><li><p>in_channels, out_channels，输入输出通道定义为参数，增强扩展使用</p></li><li><p>卷积 nn.Conv2d 的输出：</p><ul><li><p><strong>nn. Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,dilation=1, groups=1, bias=True, padding_mode= ‘zeros’ )</strong></p><ul><li>in_channels:输入的四维张量[N, C, H, W]中的C，也就是说输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。</li><li>out_channels:也很好理解，即期望的四维输出张量的channels数，不再多说。</li><li>kernel_size:卷积核的大小，一般我们会使用5x5、3x3这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）。<br>stride = 1:卷积核在图像窗口上每次平移的间隔，即所谓的步长。这个概念和Tensorflow等其他框架没什么区别，不再多言。</li><li>padding:这是Pytorch与Tensorflow在卷积层实现上最大的差别，padding也就是指图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。需要注意的是这里的填充包括图像的上下左右，以padding=1为例，若原始图像大小为32 <em> 32，那么padding后的图像大小就变成了34 </em> 34，而不是33*33。<br>Pytorch不同于Tensorflow的地方在于，Tensorflow提供的是padding的模式，比如same、valid，且不同模式对应了不同的输出图像尺寸计算公式。而Pytorch则需要手动输入padding的数量，当然，Pytorch这种实现好处就在于输出图像尺寸计算公式是唯一的，</li><li>dilation:这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）。更形象和直观的图示可以观察Github上的Dilated convolution animations，展示了dilation=2的情况。</li><li>groups:决定了是否采用分组卷积，groups参数可以参考groups参数详解</li><li>bias:即是否要添加偏置参数作为可学习参数的一个，默认为True。</li><li>padding_mode:即padding的模式，默认采用零填充。</li></ul></li><li><p>输出通道就是 out_channels</p></li><li><p>输出的 <strong>X * X</strong> 计算公式：</p><script type="math/tex; mode=display">O = （I - K + 2P）/ S +1</script></li></ul></li></ul><pre><code>- I 为输入feature map的大小，O为输出feature map的大小，K为卷积核的大小，P为padding的大小，S为步长</code></pre><h5 id="Down（下采样模块）"><a href="#Down（下采样模块）" class="headerlink" title="Down（下采样模块）"></a>Down（下采样模块）</h5><p>UNet的下采样模块有着4次的下采样过程，过程如下</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/3.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Down(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.maxpool_conv(x)</span><br></pre></td></tr></table></figure><ul><li>代码很简单，就是一个maxpool池化层，进行下采样，然后接一个DoubleConv模块。</li><li>到这里，左边的网络完成！！</li></ul><h5 id="Up（上采样模块）"><a href="#Up（上采样模块）" class="headerlink" title="Up（上采样模块）"></a>Up（上采样模块）</h5><p>上采样模块就是出来常规的上采样操作以外，还需要进行特征融合，</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/4.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Up(nn.Module):</span><br><span class="line">   </span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)</span><br><span class="line">        self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    def forward(self, x1, x2):</span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        # input is CHW</span><br><span class="line">        diffY = x2.size()[2] - x1.size()[2]</span><br><span class="line">        diffX = x2.size()[3] - x1.size()[3]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,</span><br><span class="line">                        diffY // 2, diffY - diffY // 2])</span><br><span class="line">     </span><br><span class="line">        x = torch.cat([x2, x1], dim=1)</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><ul><li><p>初始化函数里定义的上采样方法（反卷积）以及卷积采用DoubleConv</p></li><li><p>反卷积，顾名思义，就是反着卷积。卷积是让featuer map越来越小，反卷积就是让feature map越来越大，</p><p>下面蓝色为原始图片，周围白色的虚线方块为padding结果，通常为0，上面绿色为卷积后的图片。</p><p>这个示意图，就是一个从 <strong>2 * 2</strong>的feature map  —-&gt;  <strong>4 * 4 </strong>的feature map过程。</p><p>在forward前向传播函数中，x1接收的是<strong>上采样</strong>的数据，x2接收的是<strong>特征融合</strong>的数据。特征融合方法就是，上文提到的，先对小的feature map进行padding，再进行concat。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/6.gif" alt=""></p></li></ul><h5 id="OutConv模块"><a href="#OutConv模块" class="headerlink" title="OutConv模块"></a><strong>OutConv模块</strong></h5><p>用上述的DoubleConv模块、Down模块、Up模块就可以拼出UNet的主体网络结构了。UNet网络的输出需要根据分割数量，整合输出通道。</p><p>利用前面的模块，我们可以获取输入进来的图片的特征，此时，我们需要利用特征获得预测结果</p><p>利用特征获得预测结果的过程为：</p><ul><li><strong>利用一个1x1卷积进行通道调整，将最终特征层的通道数调整成num_classes。</strong>  <strong>（即对每一个像素点进行分类）</strong></li></ul><p>这个过程简单，顺便也包装一下吧</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class OutConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><p>到这里，所有的积木已经完成了，接下来就是搭建的过程了。</p><h5 id="UNet模块"><a href="#UNet模块" class="headerlink" title="UNet模块"></a>UNet模块</h5><p>到这里，按照UNet网络结构，设置每个模块的输入输出通道个数以及调用顺序，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from nets.net_of_me.unet_parts import *</span><br><span class="line">class UNet(nn.Module):</span><br><span class="line">    def __init__(self, n_channels, n_classes, bilinear=False):</span><br><span class="line">        super(UNet, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, 64)</span><br><span class="line">        self.down1 = Down(64, 128)</span><br><span class="line">        self.down2 = Down(128, 256)</span><br><span class="line">        self.down3 = Down(256, 512)</span><br><span class="line">        self.down4 = Down(512, 1024)</span><br><span class="line">        self.up1 = Up(1024, 512, bilinear)</span><br><span class="line">        self.up2 = Up(512, 256, bilinear)</span><br><span class="line">        self.up3 = Up(256, 128, bilinear)</span><br><span class="line">        self.up4 = Up(128, 64, bilinear)</span><br><span class="line">        self.outc = OutConv(64, n_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        return logits</span><br></pre></td></tr></table></figure><h4 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练网络的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.backends.cudnn as cudnn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">from nets.unet_training import CE_Loss, Dice_loss, LossHistory</span><br><span class="line">from utils.dataloader import DeeplabDataset, deeplab_dataset_collate</span><br><span class="line">from utils.metrics import f_score</span><br><span class="line"></span><br><span class="line">def get_lr(optimizer):</span><br><span class="line">    for param_group in optimizer.param_groups:</span><br><span class="line">        return param_group[&#x27;lr&#x27;]</span><br><span class="line"></span><br><span class="line">def fit_one_epoch(net,epoch,epoch_size,epoch_size_val,gen,genval,Epoch,cuda):</span><br><span class="line">    net = net.train()</span><br><span class="line">    total_loss = 0</span><br><span class="line">    total_f_score = 0</span><br><span class="line"></span><br><span class="line">    val_toal_loss = 0</span><br><span class="line">    val_total_f_score = 0</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(gen):</span><br><span class="line">        if iteration &gt;= epoch_size:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        #进行训练</span><br><span class="line">        outputs = net(imgs)</span><br><span class="line">        loss    = CE_Loss(outputs, pngs, num_classes = NUM_CLASSES)</span><br><span class="line">        if dice_loss:</span><br><span class="line">            main_dice = Dice_loss(outputs, labels)</span><br><span class="line">            loss      = loss + main_dice</span><br><span class="line"></span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">        waste_time = time.time() - start_time #训练epoch需要的时间</span><br><span class="line">        start_time = time.time()</span><br><span class="line"></span><br><span class="line">        if (iteration % 50 == 0):</span><br><span class="line">            print(&quot;epoch = &#123;&#125; and loss = &#123;&#125; and waste_time = &#123;&#125;&quot;.format(epoch,loss.item(),waste_time))</span><br><span class="line">            #写入日志文件</span><br><span class="line">            with open(&quot;log/train_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch,loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Training&#x27;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Validation&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(genval):</span><br><span class="line">        if iteration &gt;= epoch_size_val:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line"># 开始训练</span><br><span class="line">            outputs = net(imgs)</span><br><span class="line">            #计算损失函数</span><br><span class="line">            val_loss = CE_Loss(outputs, pngs, num_classes=NUM_CLASSES)</span><br><span class="line">            if dice_loss:</span><br><span class="line">                main_dice = Dice_loss(outputs, labels)</span><br><span class="line">                val_loss = val_loss + main_dice</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">            val_toal_loss += val_loss.item()</span><br><span class="line">            val_total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">            if (iteration % 50 == 0):</span><br><span class="line">                print(&quot;epoch = &#123;&#125; and val_loss = &#123;&#125; &quot;.format(epoch, val_loss.item()))</span><br><span class="line">                # 写入日志文件</span><br><span class="line">                with open(&quot;log/val_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                    f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch, val_loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Validation&#x27;)</span><br><span class="line">    print(&#x27;Epoch:&#x27; + str(epoch + 1) + &#x27;/&#x27; + str(Epoch))</span><br><span class="line">    print(&#x27;Total Loss: %.4f || Val Loss: %.4f &#x27; % (total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line">    print(&#x27;Saving state, iter:&#x27;, str(epoch + 1))</span><br><span class="line">    torch.save(model.state_dict(), &#x27;model/Epoch%d-Total_Loss%.4f-%.4f.pth&#x27; % ((epoch + 1), total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    #------------------------------#</span><br><span class="line">    #   输入图片的大小</span><br><span class="line">    #------------------------------#</span><br><span class="line">    inputs_size = [512,512,3]</span><br><span class="line">    #---------------------#</span><br><span class="line">    #   分类个数+1</span><br><span class="line">    #   2+1</span><br><span class="line">    #---------------------#</span><br><span class="line">    NUM_CLASSES = 21</span><br><span class="line">    #   Cuda的使用</span><br><span class="line">    #-------------------------------#</span><br><span class="line">    Cuda = True</span><br><span class="line">    #linux服务器</span><br><span class="line">    dataset_path = &quot;/data/xwd/pro_datas/VOCdevkit/VOC2007&quot;</span><br><span class="line"></span><br><span class="line">    #网络</span><br><span class="line">    model = UNet(n_channels=inputs_size[-1], n_classes=NUM_CLASSES).train()</span><br><span class="line"></span><br><span class="line">    if Cuda:</span><br><span class="line">        net = torch.nn.DataParallel(model)</span><br><span class="line">        cudnn.benchmark = True</span><br><span class="line">        net = net.cuda()</span><br><span class="line"></span><br><span class="line">    # 打开训练数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/train.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        train_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    # 打开验证数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/val.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        val_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    if True:</span><br><span class="line">        lr = 1e-4</span><br><span class="line">        Init_Epoch = 0</span><br><span class="line">        Interval_Epoch = 5</span><br><span class="line">        Batch_size = 4</span><br><span class="line"></span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr)  #优化器</span><br><span class="line">        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.92) #学习率的调整</span><br><span class="line">        </span><br><span class="line">        #封装数据</span><br><span class="line">        train_dataset = DeeplabDataset(train_lines, inputs_size, NUM_CLASSES, True, dataset_path)</span><br><span class="line">        val_dataset = DeeplabDataset(val_lines, inputs_size, NUM_CLASSES, False, dataset_path)</span><br><span class="line">        gen = DataLoader(train_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                         drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line">        gen_val = DataLoader(val_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                             drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line"></span><br><span class="line">        epoch_size = len(train_lines) // Batch_size</span><br><span class="line">        epoch_size_val = len(val_lines) // Batch_size</span><br><span class="line"></span><br><span class="line">        if epoch_size == 0 or epoch_size_val == 0:</span><br><span class="line">            raise ValueError(&quot;数据集过小，无法进行训练，请扩充数据集。&quot;)</span><br><span class="line"></span><br><span class="line">        for epoch in range(Init_Epoch, Interval_Epoch):</span><br><span class="line">            fit_one_epoch(model, epoch, epoch_size, epoch_size_val, gen, gen_val, Interval_Epoch, Cuda)</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>封装数据集</strong>的办法主要采用：自定义类继承Dataset，下面展示的是他的伪代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># ================================================================== #</span><br><span class="line">#                Input pipeline for custom dataset                 #</span><br><span class="line"># ================================================================== </span><br><span class="line"># You should build your custom dataset as below.</span><br><span class="line">class CustomDataset(torch.utils.data.Dataset):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Initialize file paths or a list of file names. </span><br><span class="line">        pass</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span><br><span class="line">        # 2. Preprocess the data (e.g. torchvision.Transform).</span><br><span class="line">        # 3. Return a data pair (e.g. image and label).</span><br><span class="line">        pass</span><br><span class="line">    def __len__(self):</span><br><span class="line">        # You should change 0 to the total size of your dataset.</span><br><span class="line">        return 0 </span><br><span class="line"># You can then use the prebuilt data loader. </span><br><span class="line">custom_dataset = CustomDataset()</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,</span><br><span class="line">                                           batch_size=64, </span><br><span class="line">                                           shuffle=True)</span><br></pre></td></tr></table></figure><ul><li><strong>init</strong>函数是这个类的初始化函数，根据指定的图片路径，读取所有图片数据，</li><li><strong>len</strong>函数可以返回数据的多少，这个类实例化后，通过len()函数调用。</li><li><strong>getitem</strong>函数是数据获取函数，在这个函数里你可以写数据怎么读，怎么处理，并且可以一些数据预处理、数据增强都可以在这里进行</li></ul><p>下面的是自定义的这个方法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">class DeeplabDataset(Dataset):</span><br><span class="line">    def __init__(self,train_lines,image_size,num_classes,random_data,dataset_path):</span><br><span class="line">        super(DeeplabDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.train_lines    = train_lines</span><br><span class="line">        self.train_batches  = len(train_lines)</span><br><span class="line">        self.image_size     = image_size</span><br><span class="line">        self.num_classes    = num_classes</span><br><span class="line">        self.random_data    = random_data</span><br><span class="line">        self.dataset_path   = dataset_path</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.train_batches</span><br><span class="line"></span><br><span class="line">    def rand(self, a=0, b=1):</span><br><span class="line">        return np.random.rand() * (b - a) + a</span><br><span class="line"></span><br><span class="line">    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=1.5, val=1.5):</span><br><span class="line">        label = Image.fromarray(np.array(label))</span><br><span class="line"></span><br><span class="line">        h, w = input_shape</span><br><span class="line">        # resize image</span><br><span class="line">        rand_jit1 = rand(1-jitter,1+jitter)</span><br><span class="line">        rand_jit2 = rand(1-jitter,1+jitter)</span><br><span class="line">        new_ar = w/h * rand_jit1/rand_jit2</span><br><span class="line"></span><br><span class="line">        scale = rand(0.5,1.5)</span><br><span class="line">        if new_ar &lt; 1:</span><br><span class="line">            nh = int(scale*h)</span><br><span class="line">            nw = int(nh*new_ar)</span><br><span class="line">        else:</span><br><span class="line">            nw = int(scale*w)</span><br><span class="line">            nh = int(nw/new_ar)</span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        label = label.resize((nw,nh), Image.NEAREST)</span><br><span class="line">        label = label.convert(&quot;L&quot;)</span><br><span class="line">        </span><br><span class="line">        # flip image or not</span><br><span class="line">        flip = rand()&lt;.5</span><br><span class="line">        if flip: </span><br><span class="line">            image = image.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">            label = label.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">        </span><br><span class="line">        # place image</span><br><span class="line">        dx = int(rand(0, w-nw))</span><br><span class="line">        dy = int(rand(0, h-nh))</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, (w,h), (128,128,128))</span><br><span class="line">        new_label = Image.new(&#x27;L&#x27;, (w,h), (0))</span><br><span class="line">        new_image.paste(image, (dx, dy))</span><br><span class="line">        new_label.paste(label, (dx, dy))</span><br><span class="line">        image = new_image</span><br><span class="line">        label = new_label</span><br><span class="line"></span><br><span class="line">        # distort image</span><br><span class="line">        hue = rand(-hue, hue)</span><br><span class="line">        sat = rand(1, sat) if rand()&lt;.5 else 1/rand(1, sat)</span><br><span class="line">        val = rand(1, val) if rand()&lt;.5 else 1/rand(1, val)</span><br><span class="line">        x = cv2.cvtColor(np.array(image,np.float32)/255, cv2.COLOR_RGB2HSV)</span><br><span class="line">        x[..., 0] += hue*360</span><br><span class="line">        x[..., 0][x[..., 0]&gt;1] -= 1</span><br><span class="line">        x[..., 0][x[..., 0]&lt;0] += 1</span><br><span class="line">        x[..., 1] *= sat</span><br><span class="line">        x[..., 2] *= val</span><br><span class="line">        x[x[:,:, 0]&gt;360, 0] = 360</span><br><span class="line">        x[:, :, 1:][x[:, :, 1:]&gt;1] = 1</span><br><span class="line">        x[x&lt;0] = 0</span><br><span class="line">        image_data = cv2.cvtColor(x, cv2.COLOR_HSV2RGB)*255</span><br><span class="line">        return image_data,label</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        if index == 0:</span><br><span class="line">            shuffle(self.train_lines)  </span><br><span class="line">        annotation_line = self.train_lines[index]</span><br><span class="line">        name = annotation_line.split()[0]</span><br><span class="line">        # 从文件中读取图像</span><br><span class="line">        jpg = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;JPEGImages&quot;), name + &quot;.jpg&quot;))</span><br><span class="line">        png = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;SegmentationClass&quot;), name + &quot;.png&quot;))</span><br><span class="line"></span><br><span class="line">        if self.random_data:</span><br><span class="line">            jpg, png = self.get_random_data(jpg,png,(int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        else:</span><br><span class="line">            jpg, png = letterbox_image(jpg, png, (int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        png = np.array(png)</span><br><span class="line">        png[png &gt;= self.num_classes] = self.num_classes</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        #   转化成one_hot的形式</span><br><span class="line">        #   在这里需要+1是因为voc数据集有些标签具有白边部分</span><br><span class="line">        #   我们需要将白边部分进行忽略，+1的目的是方便忽略。</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        seg_labels = np.eye(self.num_classes+1)[png.reshape([-1])]</span><br><span class="line">        seg_labels = seg_labels.reshape((int(self.image_size[1]),int(self.image_size[0]),self.num_classes+1))</span><br><span class="line">        jpg = np.transpose(np.array(jpg),[2,0,1])/255</span><br><span class="line">        return jpg, png, seg_labels</span><br></pre></td></tr></table></figure><p>我这边设置的epoch并不算很大，采用3090的显卡也是运行了一段时间是时间，可以看到网络，loss实在逐渐在收敛的：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/7.jpg" alt=""></p><p>采用训练好的模型进行预测，看看结果如何：</p><p>这边采用的是在网络上copy的图片预处理和后续处理的代码，本人目前对图片处理还是比较菜，把别人的代码贴在这里，最后给出自己的预测结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line">import colorsys</span><br><span class="line">import copy</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from PIL import Image</span><br><span class="line">from torch import nn</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">#-------------------------------------------#</span><br><span class="line">#   使用自己训练好的模型预测需要修改2个参数</span><br><span class="line">#   model_path和num_classes都需要修改！</span><br><span class="line">#   如果出现shape不匹配</span><br><span class="line">#   一定要注意训练时的model_path和num_classes数的修改</span><br><span class="line">#--------------------------------------------#</span><br><span class="line">class Unet(object):</span><br><span class="line">    _defaults = &#123;</span><br><span class="line">        &quot;model_path&quot;        : &#x27;model\Epoch2-Total_Loss1.0039-0.8573.pth&#x27;, #保存的训练模型的路径</span><br><span class="line">        &quot;model_image_size&quot;  : (512, 512, 3), #输入图片的大小</span><br><span class="line">        &quot;num_classes&quot;       : 21, </span><br><span class="line">        &quot;cuda&quot;              : True,</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        #   blend参数用于控制是否</span><br><span class="line">        #   让识别结果和原图混合</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        &quot;blend&quot;             : True</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   初始化UNET</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        self.__dict__.update(self._defaults)</span><br><span class="line">        self.device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">        self.generate()</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   获得所有的分类</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def generate(self):</span><br><span class="line">        self.net = UNet(n_channels=self.model_image_size[-1],n_classes=self.num_classes).eval()</span><br><span class="line"></span><br><span class="line">        # 加载本地的模型参数</span><br><span class="line">        state_dict = torch.load(self.model_path,self.device)</span><br><span class="line">        self.net.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line">        if self.cuda:</span><br><span class="line">            self.net = nn.DataParallel(self.net) #可以调用多个GPU，帮助加速训练</span><br><span class="line">            self.net = self.net.to(self.device)</span><br><span class="line"></span><br><span class="line">        print(&#x27;&#123;&#125; model loaded.&#x27;.format(self.model_path))</span><br><span class="line"></span><br><span class="line">        if self.num_classes &lt;= 21:</span><br><span class="line">            self.colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),                              (0, 128, 128),  (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), </span><br><span class="line">                           (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), </span><br><span class="line">                           (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64,128),(128, 64, 12)]</span><br><span class="line">        else:</span><br><span class="line">            # 画框设置不同的颜色</span><br><span class="line">            hsv_tuples = [(x / len(self.class_names), 1., 1.)</span><br><span class="line">                        for x in range(len(self.class_names))]</span><br><span class="line">            self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))</span><br><span class="line">            self.colors = list(</span><br><span class="line">                map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),</span><br><span class="line">                    self.colors))</span><br><span class="line"></span><br><span class="line">    def letterbox_image(self ,image, size):</span><br><span class="line">        image = image.convert(&quot;RGB&quot;)</span><br><span class="line">        iw, ih = image.size</span><br><span class="line">        w, h = size</span><br><span class="line">        scale = min(w/iw, h/ih)</span><br><span class="line">        nw = int(iw*scale)</span><br><span class="line">        nh = int(ih*scale)</span><br><span class="line"></span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, size, (128,128,128))</span><br><span class="line">        new_image.paste(image, ((w-nw)//2, (h-nh)//2))</span><br><span class="line">        return new_image,nw,nh</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   检测图片，处理图片</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def detect_image(self, image):</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        image = image.convert(&#x27;RGB&#x27;)</span><br><span class="line">        </span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   对输入图像进行一个备份，后面用于绘图</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        old_img = copy.deepcopy(image)</span><br><span class="line"></span><br><span class="line">        orininal_h = np.array(image).shape[0]</span><br><span class="line">        orininal_w = np.array(image).shape[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   进行不失真的resize，添加灰条，进行图像归一化</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        image, nw, nh = self.letterbox_image(image,(self.model_image_size[1],self.model_image_size[0]))</span><br><span class="line">        a = np.array(image).shape</span><br><span class="line">        images = [np.array(image)/255]</span><br><span class="line"></span><br><span class="line">        images = np.transpose(images,(0,3,1,2))</span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   图片传入网络进行预测</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            images = torch.from_numpy(images).type(torch.FloatTensor) #转化为tensor</span><br><span class="line">            if self.cuda:</span><br><span class="line">                #images =images.cuda()</span><br><span class="line">                images = images.cpu()</span><br><span class="line"></span><br><span class="line">            pr = self.net(images)</span><br><span class="line"></span><br><span class="line">            pr = pr[0]</span><br><span class="line">            pr1 = pr[1]</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            #   取出每一个像素点的种类</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy().argmax(axis=-1)</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            #   将灰条部分截取掉</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            pr = pr[int((self.model_image_size[0]-nh)//2):int((self.model_image_size[0]-nh)//2+nh), </span><br><span class="line">                    int((self.model_image_size[1]-nw)//2):int((self.model_image_size[1]-nw)//2+nw)]</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   创建一副新图，并根据每个像素点的种类赋予颜色</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        seg_img = np.zeros((np.shape(pr)[0],np.shape(pr)[1],3))</span><br><span class="line">        for c in range(self.num_classes):</span><br><span class="line">            seg_img[:,:,0] += ((pr[:,: ] == c )*( self.colors[c][0] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,1] += ((pr[:,: ] == c )*( self.colors[c][1] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,2] += ((pr[:,: ] == c )*( self.colors[c][2] )).astype(&#x27;uint8&#x27;)</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片转换成Image的形式</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        image = Image.fromarray(np.uint8(seg_img)).resize((orininal_w,orininal_h))</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片和原图片混合</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        if self.blend:</span><br><span class="line">             image = Image.blend(old_img,image,0.7)      </span><br><span class="line">        return image,  old_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    unet = Unet()</span><br><span class="line">    while True:</span><br><span class="line">        img = input(&#x27;Input image filename:&#x27;)</span><br><span class="line">        try:</span><br><span class="line">            image = Image.open(img)</span><br><span class="line">        except:</span><br><span class="line">            print(&#x27;Open Error! Try again!&#x27;)</span><br><span class="line">            continue</span><br><span class="line">        else:</span><br><span class="line">            r_image,old_image= unet.detect_image(image)</span><br><span class="line">            old_image.show()</span><br><span class="line">            r_image.show()</span><br></pre></td></tr></table></figure><p>调用这个函数，得到的预测结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/8.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/9.jpg" alt=""></p><h4 id="语义分割的MIOU指标"><a href="#语义分割的MIOU指标" class="headerlink" title="语义分割的MIOU指标"></a>语义分割的MIOU指标</h4><p>语义分割的标准度量。其计算所有类别交集和并集之比的平均值.，语义分割说到底也还是一个分割任务，既然是一个分割的任务，预测的结果往往就是四种情况：</p><ul><li><p>true positive（TP）：预测正确, 预测结果是正类, 真实是正类 </p></li><li><p>false positive（FP）：预测错误, 预测结果是正类, 真实是负类</p></li><li><p>true negative（TN）：预测错误, 预测结果是负类, 真实是正类</p></li><li><p>false negative（FN）：预测正确, 预测结果是负类, 真实是负类  </p></li></ul><p>mIOU 的定义：计算真实值和预测值两个集合的交集和并集之比。这个比例可以变形为TP（交集）比上TP、FP、FN之和（并集）。即：mIOU=TP/(FP+FN+TP)。</p><p>计算公式：</p><script type="math/tex; mode=display">MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{p_{ii}}{\sum_{j=0}^kp_{ij} + \sum_{j=0}^kp_{ji} - p_{ii}}</script><p>等价于：</p><script type="math/tex; mode=display">MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{TP}{FN+FP+TP}</script><p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是基于全局的评价。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/10.jpg" alt=""></p><p><code>MIoU</code>：计算两圆交集（橙色部分）与两圆并集（红色+橙色+黄色）之间的比例，理想情况下两圆重合，比例为1。</p><p>计算本网络的MIoU可以采样训练好的模型进行计算，计算的结果比例越接近1效果越好。</p><p>代码实现后续把，hhhhhhhhhhhhhhhhhhhhh。。。。。。。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉的基本知识介绍</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p><strong>计算机视觉方向：图像分类，图像检测，目标检测，图像分割，图像生成，目标跟踪，超分辨率重构，关键点定位，图像降噪，多模态，图像加密，视频编解码，3D视觉等等</strong></p><h3 id="图像基本概念"><a href="#图像基本概念" class="headerlink" title="图像基本概念"></a>图像基本概念</h3><h4 id="颜色空间"><a href="#颜色空间" class="headerlink" title="颜色空间"></a>颜色空间</h4><ul><li>颜色空间也称彩色模型，用于描述色彩</li><li>常见的颜色空间包括：RGB（常用3通道）、CMYK、YUV（摄像头）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/1.png" alt=""></p><h4 id="RGB色彩模式"><a href="#RGB色彩模式" class="headerlink" title="RGB色彩模式"></a>RGB色彩模式</h4><ul><li>RGB色彩模式是工业界的一种颜色标准</li><li>通过对红（R）、绿（G）、蓝（B）三个颜色通道的变化以及它们相互之间的叠加来<br>得到各式各样的颜色的</li><li>红、绿、蓝三个<strong>颜色通道</strong>每种色各分为256阶亮度，（R，G，B）三维就是一个像素点，（0，0，0）黑，（255，255，255）白</li><li>H <em> W </em> C  H:长，W:宽，C:通道</li></ul><h4 id="HSV色彩模式"><a href="#HSV色彩模式" class="headerlink" title="HSV色彩模式"></a>HSV色彩模式</h4><ul><li>色相（Hue）：指物体传导或反射的波长。更常见的是以颜色如红色，橘色或绿色来辨识，取0到360度的数值来衡量</li><li>饱和度（Saturation）：又称色度，是指色彩的强度或纯度，取值范围为0%~100%</li><li>明度（Value）：表示颜色明亮的程度，取值范围为0%（黑）到100%（白）</li></ul><h4 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h4><ul><li><p>灰度图通常由一个unit8、unit16、单精度类型或者双精度类型的数组描述，也就是上面的 C=1，通道为 1</p></li><li><p>M*N的矩阵，矩阵中每一个元素与图像的一个像素点相对应</p></li><li><p>通常0代表黑色，1、255或65635（为数据矩阵的取值范围上限）代表白色</p><blockquote><p>浮点算法：Gray=R<em>0.3+G</em>0.59+B<em>0.11<br>整数方法：Gray=（R</em>30+G<em>59+B</em>11）/100<br>移位方法：Gray=（R<em>28+G</em>151+B*77）&gt;&gt;8<br>平均值法：Gray=（R+G+B）/3<br>仅取绿色：Gray=G</p></blockquote></li></ul><h3 id="图像处理基本概念"><a href="#图像处理基本概念" class="headerlink" title="图像处理基本概念"></a>图像处理基本概念</h3><h4 id="亮度，对比度，饱和度"><a href="#亮度，对比度，饱和度" class="headerlink" title="亮度，对比度，饱和度"></a>亮度，对比度，饱和度</h4><ul><li>亮度：图像的明亮程度，在单色图像中，最高的值应该对应于白色，最低的值应当对应于黑色；</li><li>对比度：图像暗和亮的落差值，即图像最大灰度级和最小灰度级之间的差值，差异范围越大代表对比越大，差异范围越小代表对比越小</li><li>饱和度：图像颜色种类的多少，饱和度越高，颜色种类越多，外观上看起来图像会更鲜艳</li><li>对于亮度和对比度，可以从RGB图上进行数据增强</li><li>对于饱和度，可以从HSV/HSI/HSL色彩空间上进行增强</li></ul><h4 id="图像平滑-降噪"><a href="#图像平滑-降噪" class="headerlink" title="图像平滑/降噪"></a>图像平滑/降噪</h4><ul><li>图像平滑是指用于突出图像的宽大区域、低频成分、主干部分或抑制图像噪声和干扰高频成分的图像处理方法，使图像亮度平缓渐变，减小突变梯度，改善图像质量。会出现边缘没有，轮廓结构不明显了<ul><li>归一化块滤波器（Normalized Box Filter）</li><li>高斯滤波器（Gaussian Filter）</li><li>中值滤波器（Median Filter）</li><li>双边滤波（Bilateral Filter）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/2.png" alt=""></p><h4 id="图像锐化-增强"><a href="#图像锐化-增强" class="headerlink" title="图像锐化/增强"></a>图像锐化/增强</h4><ul><li>图像锐化与图像平滑是相反的操作，锐化是通过增强高频分量来减少图像中的模糊，增强图像细节边缘和轮廓，增强灰度反差，便于后期对目标的识别和处理。</li><li>锐化处理在增强图像边缘的同时也增加了图像的噪声。</li><li>方法包括：微分法和高通滤波法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/3.jpg" alt=""></p><h4 id="边缘提取算子"><a href="#边缘提取算子" class="headerlink" title="边缘提取算子"></a>边缘提取算子</h4><ul><li><p>图像中的高频和低频的概念理解、</p></li><li><p>通过微分的方式计算图像的边缘（色差或者灰度值做差）</p><blockquote><p>Roberts算子<br>Prewitt算子<br>sobel算子<br>Canny算子<br>Laplacian算子<br>等等</p></blockquote></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/4.jpg" alt=""></p><h4 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h4><ul><li>直方图均衡化是将原图像通过某种变换，得到一幅灰度直方图为均匀分布的新图像的方法。</li><li>对在图像中像素个数多的灰度级进行展宽，而对像素个数少的灰度级进行缩减，从而达到清晰图像的目的。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/5.jpg" alt=""></p><h4 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h4><ul><li><p>图像滤波可以更改或者增强图像。</p></li><li><p>通过滤波，可以强调一些特征或者去除图像中一些不需要的部分。</p></li><li><p>滤波是一个邻域操作算子，利用给定像素周围的像素的值决定此像素的最终的输出值</p></li><li><p>常见的应用包括去噪、图像增强、检测边缘、检测角点、模板匹配等</p><blockquote><p>均值滤波<br>中值滤波<br>高斯滤波<br>双边滤波<br>等等</p></blockquote></li></ul><h4 id="形态学运算"><a href="#形态学运算" class="headerlink" title="形态学运算"></a>形态学运算</h4><ul><li>腐蚀：腐蚀的效果是把图片”变瘦”，其原理是在原图的小区域内取局部最小值。</li><li>膨胀：膨胀与腐蚀相反，取的是局部最大值，效果是把图片”变胖”</li><li>开运算：先腐蚀后膨胀（因为先腐蚀会分开物体，这样容易记住），可以分离物体，消除小区域</li><li>闭运算：先膨胀后腐蚀（先膨胀会使白色的部分扩张，以至于消除/“闭合”物体里面的小黑洞）</li><li>形态学梯度：膨胀图减去腐蚀图，得到轮廓图</li><li>顶帽：原图减去开运算后的图</li><li>黑帽：闭运算后的图减去原图</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/6.jpg" alt=""></p><h3 id="卷积神经网络概念介绍"><a href="#卷积神经网络概念介绍" class="headerlink" title="卷积神经网络概念介绍"></a>卷积神经网络概念介绍</h3><ul><li><p>由卷积核构建，卷积核简称为卷积，也称为滤波器。卷积的大小可以在实际需要时自定义其长和宽（1 <em> 1, 3 </em> 3, 5 * 5）。</p></li><li><p>卷积神经网：以卷积层为主的深度网络结构</p></li><li><strong>卷积层，激活层，BN层，池化层，全连接层（FC层），损失层</strong></li></ul><h4 id="卷积层定义"><a href="#卷积层定义" class="headerlink" title="卷积层定义"></a>卷积层定义</h4><ul><li>对图像和滤波矩阵做内积（逐个元素相乘再求和）的操作<ul><li><strong>nn.Conv2d（in channels，out channels，kernel_size，stride=1，padding=0，dilation=1，groups=1，bias=True）</strong></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/7.jpg" alt=""></p><h4 id="常见的卷积操作"><a href="#常见的卷积操作" class="headerlink" title="常见的卷积操作"></a>常见的卷积操作</h4><ul><li>分组卷积（group参数）</li><li>空洞卷积（dilation参数）</li><li>深度可分离卷积（分组卷积+1×1卷积）。</li><li>反卷积（torch.nn.ConvTranspose2d）</li><li>可变形卷积等等</li></ul><h4 id="理解卷积层的重要概念"><a href="#理解卷积层的重要概念" class="headerlink" title="理解卷积层的重要概念"></a>理解卷积层的重要概念</h4><ul><li><p>感受野（Receptive Field），指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。</p></li><li><p>参数量：参与计算参数的个数，占用内存空间</p></li><li><p>FLOPS：每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</p></li><li><p>FLOPs：浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。</p></li><li><p>MAC：乘加次数，用来衡量计算量。</p></li><li><p>从感受野不变+减少参数量的角度压缩卷积层，压缩卷积层参数&amp;&amp;计算量</p><blockquote><p>采用多个3×3卷积核代替大卷积核<br>采用深度可分离卷积<br>通道Shuffle<br>Pooling层<br>Stride=2<br>等等</p></blockquote></li><li><p>常见卷积层组合结构：<strong>堆叠，跳连，并连</strong></p></li></ul><h4 id="池化层（下采样）"><a href="#池化层（下采样）" class="headerlink" title="池化层（下采样）"></a>池化层（下采样）</h4><ul><li><p>对图片进行压缩（降采样）的一种方法，如max pooling, average pooling等</p></li><li><p>对输入的特征图进行压缩</p><ul><li>一方面使特征图变小，简化网络计算复杂度；</li><li>一方面进行特征压缩，提取主要特征</li></ul></li><li>最大池化（Max Pooling）、平均池化（Average Pooling）等口 </li><li>nn.MaxPool2d（kernel_size，stride=None，padding=0，dilation=1，return_indices=False，ceil_mode=False）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/8.jpg" alt=""></p><h4 id="上采样层"><a href="#上采样层" class="headerlink" title="上采样层"></a>上采样层</h4><ul><li>Resize，如双线性插值直接缩放，类似于图像缩放，概念可见最邻近插值算法和双线性插值算法——图像缩放</li><li>Deconvolution，也叫Transposed Convolution</li><li>实现函数<ul><li>nn.functi onal.interpolate（input，size=None，scale_factor=None，mode=’nearest’，align_corners=None）</li><li>nn.ConvTranspose2d（in channels，out channels，kernel_size，stride=1，padding=0，output padding=0，bias=True）</li></ul></li></ul><h4 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h4><ul><li><p>激活函数的作用就是，在所有的隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数了，因而神经网络的表达能力更加强大了。</p></li><li><p>激活函数：为了增加网络的非线性，进而提升网络的表达能力，<strong>详细见另外一篇博客</strong></p></li><li>ReLU函数、Leakly ReLU函数、ELU函数等</li><li>torch.nn.ReLU（inplace=True）</li></ul><h4 id="BatchNorm层"><a href="#BatchNorm层" class="headerlink" title="BatchNorm层"></a>BatchNorm层</h4><ul><li>通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</li><li>Batchnorm是归一化的一种手段，它会减小图像之间的绝对差异，突出相对差异，加快训练速度</li><li>不适用的问题：image-to-image以及对噪声敏感的任务</li><li>nn.BatchNorm2d（num features，eps=1e-05，momentum=0.1，affine=True，track running_stats=True）</li></ul><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><ul><li><p>口连接所有的特征，将输出值送给分类器（如softmax分类器）（线性）</p><ul><li>对前层的特征进行一个加权和，（卷积层是将数据输入映射到隐层特征空间）将特征空间通过线性变换映射到样本标记空间（也就是label）</li><li>可以通过1×1卷积+global average pooling代替</li><li>可以通过全连接层参数冗余</li><li>全连接层参数和尺寸相关</li></ul></li><li><p>nn.Linear（in features，out features，bias）</p></li></ul><h4 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h4><ul><li>在不同的训练过程中随机扔掉一部分神经元</li><li>测试过程中不使用随机失活，所有的神经元都激活</li><li>为了防止或减轻过拟合而使用的函数，它一般用在全连接层</li><li>nn.dropout</li></ul><h4 id="损失层"><a href="#损失层" class="headerlink" title="损失层"></a>损失层</h4><ul><li><p>损失函数：在深度学习中，损失反映模型最后预测结果与实际真值之间的差距，可以用来分析训练过程的好坏、模型是否收敛等，例如均方损失、交叉熵损失等。</p></li><li><p>损失层：设置一个损失函数用来比较网络的输出和目标值，通过最小化损失来驱动网络的训练</p></li><li><p>网络的损失通过前向操作计算，网络参数相对于损失函数的梯度则通过反向操作计算</p></li><li><p>分类问题损失（分类分割）</p><ul><li>nn.BCELoss；nn.CrossEntropyLoss等等</li></ul></li><li><p>回归问题损失（推测，回归）</p><ul><li>nn.L1Loss；nn.MSELoss；nn.SmoothL1Loss等等</li></ul></li></ul><h3 id="经典卷积神经网络结构"><a href="#经典卷积神经网络结构" class="headerlink" title="经典卷积神经网络结构"></a>经典卷积神经网络结构</h3><p><strong>堆叠，跳连，并连</strong> ，轻量型网络结构，多分支网络结构，attention网络结构</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/9.jpg" alt=""></p><h3 id="其他重要概念"><a href="#其他重要概念" class="headerlink" title="其他重要概念"></a>其他重要概念</h3><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><ul><li><p>学习率作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。</p></li><li><p>合适的学习率能够使目标函数在合适的时间内收敛到局部最小值</p></li><li><p>学习率大，震荡，恐怕到达不了最佳收敛值，学习率小收敛缓慢，消耗时间（如下图到最低点，学习率大小可以看作步长）</p></li><li><p>torch.optim.Ir scheduler</p><blockquote><p>ExponentialLR<br>ReduceLROnPlateau<br>CyclicLR<br>等等</p></blockquote></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/10.jpg" alt=""></p><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>反向传播优化参数</p><ul><li>GD、BGD、SGD、MBGD<ul><li>引入了随机性和噪声</li></ul></li><li>Momentum、NAG等<ul><li>加入动量原则，具有加速梯度下降的作用</li></ul></li><li>AdaGrad，RMSProp，Adam、AdaDelta<ul><li>自适应学习率</li></ul></li><li>torch.optim.Adam</li></ul><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul><li><p>模型出现过拟合现象时，降低模型复杂度</p></li><li><p>L1正则：参数绝对值的和</p></li><li>L2正则：参数的平方和（Pytorch自带，weight decay）</li><li>optimizer=torch.optim.SGD（model.parameters），Ir=0.01，weight_decay=0.001）</li></ul><h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><ul><li><p><strong>参数与超参数</strong></p><ul><li><p><strong>参数：</strong>模型f(x, θ)中的θ 称为模型的参数，可以通过优化算法进行学习。</p></li><li><p><strong>超参数：</strong>用来定义模型结构或优化策略。</p></li></ul></li><li><p><strong>batch_size 批处理</strong></p><ul><li>每次处理的数据数量。</li></ul></li><li><p><strong>epoch 轮次</strong></p><ul><li>把一个数据集，循环运行几轮。</li></ul></li><li><p><strong>transforms 变换</strong></p><ul><li>主要是将图片转换为tensor，旋转图片，以及正则化。</li></ul></li><li><p><strong>nomalize 正则化</strong></p><ul><li>模型出现过拟合现象时，降低模型复杂度</li></ul></li><li><p>前向传播</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/11.png" alt=""></p></li><li><p>反向传播</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/12.jpg" alt=""></p></li><li><p>梯度下降</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/13.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用激活函数（激励函数）理解和总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%EF%BC%89%E7%90%86%E8%A7%A3%E5%92%8C%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/Prior-Knowledge/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%EF%BC%89%E7%90%86%E8%A7%A3%E5%92%8C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近自己刚刚开始学习神经网络的相关知识，在学习构建一个自定义网络的时候，对于在forward函数中突然出现的Relu函数有点奇怪，然后就去百度查询了一波，原来这就是前面了解概念的时候所说的激活函数也就是激励函数，自己也是在百度和知乎上了解的更加透彻一点点，现在就自己的理解和参考一些别人的说法进行一定的总结，这次总结就主要是如下几点</p><ul><li>什么是激活函数</li><li>激活函数的作用（为什么就需要激活函数嘞）</li><li>有哪些常用的激活函数，都各自有什么性质和特点</li><li>在应用中该如何选择合适的激活函数</li></ul><h3 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h3><p>想要了解什么是激活函数，应该先了解神经网络的基本模型，单一的额神经元模型如图1所示。神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/1.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 1 单个神经元结构结构</center><h3 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h3><p>查阅了相关资料和学习，大家普遍对神经网络中的激活函数的额作用主要集中在下面这个观点</p><ul><li><strong>激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题。</strong></li></ul><p>下面举个例子，这个例子是我知乎上看到一位博主写的，个人觉得很不错，就搬过来了，在这里也加入了自己的思考，进一步理解。</p><p>首先我们现在有这么一个需求，就是二分类的问题，如果我要将下图的三角形和圆形进行正确的分类，也就是分隔开来，如图2所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/2.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 2</center><p>利用我们单层的感知机, 下图图3直线是由<img src="https://www.zhihu.com/equation?tex=w_%7B1%7Dx_%7B1%7D+%2B+w_%7B2%7Dx_%7B2%7D%2Bb%3D0+" alt="[公式]">得到，那么该感知器实现预测的功能步骤如下，就是我已经训练好了一个感知器模型，后面对于要预测的样本点，带入模型中，如果<img src="https://www.zhihu.com/equation?tex=y%3E0" alt="[公式]">,那么就说明是直线的右侧，也就是正类（我们定义是三角形），如果<img src="https://www.zhihu.com/equation?tex=y%3C0" alt="[公式]">,那么就说明是直线的左侧，也就是负类（我们定义是圆形)。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/3.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 3</center><p>利用我们上面单层的感知机, 用它可以任意划出一条线, 把平面分割成两部分，如图4所示，很容易能够看出，我给出的样本点根本不是线性可分的，一个感知器无论得到的直线怎么动，都不可能完全正确的将三角形与圆形区分出来，也就是说一条线性结构的直线是无法将三角形和长方形完全分割开来的。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/4.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 4</center><p>此刻我们很容易想到用多个感知器来进行组合（也就是可以多条线性的直线），以便获得更大的分类问题，好的，下面我们上图，看是否可行，如图5，</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/5.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 5</center><p>好的，我们已经得到了多感知器分类器了，那么它的分类能力是否强大到能将非线性数据点正确分类开呢~我们来分析一下：</p><p>我们能够得到</p><script type="math/tex; mode=display">y = W_{2-1}(w_{1-11}x_1+w_{1-21}x_2+b_{1-1}) +  W_{2-2}(w_{1-12}x_1+w_{1-22}x_2+b_{1-2}) + W_{2-3}(w_{1-13}x_1+w_{1-23}x_2+b_{1-3})</script><p>哎呀呀，不得了，这个式子看起来非常复杂，估计应该可以处理我上面的情况了吧，哈哈哈哈~不一定额，我们来给它变个形.上面公式合并同类项后等价于下面公式：</p><script type="math/tex; mode=display">y = x_1(w_{2-1}w_{1-11} + w_{2-2}w_{1-12} + w_{2-3}w_{1-13}) + x_2(w_{2-1}w_{1-21} + w_{2-2}w_{1-22} + w_{2-3}w_{1-23}) + w_{2-1}b_{1-1} + w_{2-2}b_{1-2} +  w_{2-3}b_{1-3}</script><p><strong>啧啧，估计大家都看出了，不管它怎么组合，最多就是线性方程的组合，最后得到的分类器本质还是一个线性方程，该处理不了的非线性问题，它还是处理不了。</strong></p><p><strong>就好像下图图6，直线无论在平面上如果旋转，都不可能完全正确的分开三角形和圆形点：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/6.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 6</center><p>既然是非线性问题，总有线性方程不能正确分类的地方，那么抛开神经网络中神经元需不需要激活函数这点不说，如果没有激活函数，仅仅是线性函数的组合解决的问题太有限了，碰到非线性问题就束手无策了.那么加入激活函数是否可能能够解决呢？</p><p>在上面线性方程的组合过程中（在加入阶跃激活函数的时候），我们其实类似在做三条直线的组合，如下图7：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/7.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 7</center><p>在这里就可以大声说出，激活函数就是来解决非线性因素的，没有太大的问题，就拿sigmoid例子说上面的场景，如图8sigmoid函数</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/8.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 8</center><p><strong>通过这个激活函数映射之后，输出很明显就是一个非线性函数！能不能解决一开始的非线性分类问题不清楚，但是至少说明有可能啊，上面不加入激活函数神经网络压根就不可能解决这个问题</strong></p><p>同理，扩展到多个神经元组合的情况时候，表达能力就会更强~对应的组合图9如下：（现在已经升级为三个非线性感知器在组合了）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/9.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 9</center><p>跟上面线性组合相对应的非线性组合如下，图10：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/10.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 10</center><p><strong>这看起来厉害多了，是不是~最后再通过最优化损失函数的做法，我们能够学习到不断学习靠近能够正确分类三角形和圆形点的曲线，到底会学到什么曲线，不知道到底具体的样子，也许是下面图11这个</strong>，那么随着不断训练优化，我们也就能够解决非线性的问题了。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/11.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 11</center><p><strong>所以到这里为止，我们就解释了这个观点，加入激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题。</strong></p><h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><h4 id="Sigmoid函数，是比较常用的非线性激活函数"><a href="#Sigmoid函数，是比较常用的非线性激活函数" class="headerlink" title="Sigmoid函数，是比较常用的非线性激活函数"></a><strong>Sigmoid函数</strong>，是比较常用的非线性激活函数</h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script></li><li><p>值域：（0，1）；导数值域（0，0.25）</p></li><li><p>函数图像，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/12.jpg" alt=""></p></li><li><p>优点</p><ul><li>值域为(0，1），可以放到模型最后一层，作为模型的概率输出</li><li>特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1</li></ul></li><li><p>缺点</p><ul><li><p>在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。如果我们初始化神经网络的权值为 [ 0 , 1 ]  之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 ( 1 , + ∞ ) (1,+∞)(1,+∞) 区间内的值，则会出现梯度爆炸情况。</p></li><li><p>函数输出不是以0为中心的（不是zero-centered输出问题），这样会使权重更新效率降低。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：如</p><script type="math/tex; mode=display">f = w^Tx + b</script><p>那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。</p></li><li><p>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间</p></li></ul></li></ul><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a><strong>tanh函数</strong></h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script></li><li><p>值域：（-1，1）当|x|&gt;3时，函数容易饱和；导数值域（0，1）当|x|&gt;3时，梯度几乎为0</p></li><li><p>函数图像，如下图：（蓝色是Tanh原函数，紫色是导函数图像）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/13.jpg" alt=""></p></li><li><p>优点</p><ul><li>函数均值为0，一般作为图像生成的最后一层激活函数。</li><li>整个函数是以0为中心的，这个特点比sigmod的好，解决了Sigmoid函数的不是zero-centered输出问题</li></ul></li><li><p>缺点</p><ul><li>仍然存在梯度消失问题；涉及指数运算，复杂度高一些</li></ul></li></ul><h4 id="Relu函数"><a href="#Relu函数" class="headerlink" title="Relu函数"></a><strong>Relu函数</strong></h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = max(0,x)</script></li><li><p>值域：当x<0时，函数值为0，当x>0时，函数值跟x线性增长; 当x<0时，导数值域：导函数值为0，当x>0时，导函数值为1</p></li><li><p>函数图像：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/14.jpg" alt=""></p></li><li><p>优点</p><ul><li>解决了gradient vanishing问题 (在正区间)</li><li>计算速度非常快，只需要判断输入是否大于0</li><li>收敛速度远快于sigmoid和tanh</li></ul></li><li><p>缺点</p><ul><li>当输入是负数的时候，ReLU是完全不被激活的，这就表明一旦输入到了负数，ReLU就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。</li><li>ReLU函数的输出要么是0，要么是正数，这也就是说，ReLU函数也不是以0为中心的函数。</li></ul></li></ul><p>尽管存在这两个问题，<strong>ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！</strong></p><h4 id="ELU函数"><a href="#ELU函数" class="headerlink" title="ELU函数"></a><strong>ELU函数</strong></h4><ul><li><p>数学表达式</p><script type="math/tex; mode=display">f(n) = \begin{cases}x, & \text {x > 0}\\\alpha(e^x-1), & \text {otherwise}\end{cases}</script></li><li><p>函数图像：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/15.png" alt=""></p></li><li><p>ELU函数是针对ReLU函数的一个改进型，相比于ReLU函数，在输入为负数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。这样可以消除ReLU死掉的问题，不过还是有梯度饱和和指数运算的问题。不会有Dead ReLU问题和 输出的均值接近0，zero-centered。理论上虽然好于ReLU，但在实际使用中目前并没有好的证据证明ELU总是优于ReLU。</p></li></ul><h4 id="PReLU函数"><a href="#PReLU函数" class="headerlink" title="PReLU函数"></a><strong>PReLU函数</strong></h4><ul><li><p>函数表达式</p><script type="math/tex; mode=display">f = max(\alpha x,x)</script></li><li><p>函数图像：</p></li></ul><p>  <img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/16_1.png" alt=""></p><ul><li><p>PReLU也是针对ReLU的一个改进型，在负数区域内，PReLU有一个很小的斜率，这样也可以避免ReLU死掉的问题。相比于ELU，PReLU在负数区域内是线性运算，斜率虽然小，但是不会趋于0，这算是一定的优势吧。</p><p>我们看PReLU的公式，里面的参数α一般是取0~1之间的数，而且一般还是比较小的，如零点零几。当α=0.01时，我们叫PReLU为Leaky ReLU，是PReLU的一种特殊情况吧</p></li></ul><h4 id="Maxout函数"><a href="#Maxout函数" class="headerlink" title="Maxout函数"></a><strong>Maxout函数</strong></h4><ul><li>函数表达式：</li></ul><script type="math/tex; mode=display">\sigma(x) = max(W_1x+W_2x+b)</script><ul><li>我们可以看到，Sigmoid函数实际上就是把数据映射到一个(−1,1)的空间上，也就是说，Sigmoid函数如果用来分类的话，只能进行二分类，而这里的softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。Softmax回归模型是logistic回归模型在多分类问题上的推广，在多分类问题中，待分类的类别数量大于2，且类别之间互斥。比如我们的网络要完成的功能是识别0-9这10个手写数字，若最后一层的输出为[0,1,0, 0, 0, 0, 0, 0, 0, 0]，则表明我们网络的识别结果为数字1。</li></ul><h3 id="应用中如何选择激活函数"><a href="#应用中如何选择激活函数" class="headerlink" title="应用中如何选择激活函数"></a>应用中如何选择激活函数</h3><ul><li>深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。</li><li>如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.</li><li>最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.</li><li>在较深层的神经网络中，选用relu激活函数能使梯度更好地传播回去，但当使用softmax作为最后一层的激活函数时，其前一层最好不要使用relu进行激活，而是使用tanh作为替代，否则最终的loss很可能变成Nan；</li><li>当选用高级激活函数时，建议的尝试顺序为ReLU-&gt;ELU-&gt;PReLU-&gt;MPELU，因为前两者没有超参数，而后两者需要自己调节参数使其更适应构建的网络结构。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> Prior Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 激活函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络的来龙去脉</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/</url>
      
        <content type="html"><![CDATA[<h2 id="神经网络的来龙去脉"><a href="#神经网络的来龙去脉" class="headerlink" title="神经网络的来龙去脉"></a>神经网络的来龙去脉</h2><p>神经，名呼其曰，就是动物的神经系统，从外界的条件触感和感知到大脑中枢的控制再到控制神经做出一系列的反应。</p><p>其实，在人工只能领域的神经网络而言，大部分的神经网络都可以用<strong>深度</strong> <strong>（depth）</strong>，和<strong>连接结构（connection）</strong>，但是具体的会具体说明。笼统的说，神经网络是可以分为有监督，无监督，半监督的神经网络，其实在这个分类下，忘忘也是你中有我我中有你的的一个局面，在学习的过程中有时候不必要去抠字眼。下面自己在浏览学习后，对神经网络的一点总结。</p><p>发展历程：</p><p>感知机 ==》多层感知机 ==》深度神经网络 ==》卷积神经网络</p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络即指人工神经网络，或称作连接模型，它是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。这种网络依靠系统的复杂程度，通过调整内部大量节点之间相互连接的关系，从而达到处理信息的目的。神经网络用到的算法是向量乘法，采用符号函数及其各种逼近。<strong>并行、容错、可以硬件实现以及自我学习特性</strong>，是神经网络的几个基本优点，也是神经网络计算方法与传统方法的<strong>区别所在</strong>。</p><h2 id="神经网络发展"><a href="#神经网络发展" class="headerlink" title="神经网络发展"></a>神经网络发展</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是Rosenblatt。但是，Rosenblatt的单层感知机有一个严重得不能再严重的问题，对于计算稍微复杂的函数其计算力显得无能为力。</p><h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人发明的多层感知机（multilayer perceptron)克服。多层感知机，顾名思义，就是有多个隐含层的感知机。</p><p>多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这就是我们现在所说的神经网络( NN)！多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。多层感知机给我们带来的启示是，<strong>神经网络的层数直接决定了它对现实的刻画能力——利用每层更少的神经元拟合更加复杂的函数。</strong></p><p>即便大牛们早就预料到神经网络需要变得更深，但是有一个梦魇总是萦绕左右。随着神经网络层数的加深，<strong>优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优</strong>。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，“<strong>梯度消失”现象更加严重</strong>。具体来说，我们常常使用 sigmoid 作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/575577-20190802141514879-1961811671.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 1</center><p>传统意义上的多层神经网络包含三层：</p><ul><li>输入层</li><li>隐藏层</li><li>输出层</li></ul><p>其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适，多层神经网络做的步骤是：特征映射到值，特征是人工挑选。</p><h3 id="深度神经网络-（DNN）"><a href="#深度神经网络-（DNN）" class="headerlink" title="深度神经网络 （DNN）"></a>深度神经网络 （DNN）</h3><p>传统的人工神经网络（ANN）由三部分组成：输入层，隐藏层，输出层，这三部分各占一层。而深度神经网络的“深度”二字表示它的隐藏层大于2层，这使它有了更深的抽象和降维能力。</p><p>2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层(参考论文：Hinton G E, Salakhutdinov R R. Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006, 313(5786):504-507.)，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了 sigmoid，形成了如今 DNN 的基本形式。<strong>单从结构上来说，全连接的DNN和上图的多层感知机是没有任何区别的</strong>。值得一提的是，今年出现的高速公路网络（highway network）和深度残差学习（deep residual learning）进一步避免了梯度弥散问题，网络层数达到了前所未有的一百多层（深度残差学习：152层）</p><h3 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h3><p>如下图2所示，<strong>我们看到全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接</strong>，带来的潜在问题是<strong>参数数量的膨胀</strong>。假设输入的是一幅像素为1K<em>1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是<em>*通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。</em></em></p><p>对于图像，如果没有卷积操作，学习的参数量是灾难级的。CNN之所以用于图像识别，正是由于CNN模型限制了参数的个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被称为前向神经网络(Feed-forward Neural Networks)。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/e186f18d73fdafa8d4a5e75ed55ed4a3_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 2</center><p>下面，通过一个例子简单说明卷积神经网络的结构。假设如下图3，m-l 是输入层，我们需要识别一幅彩色图像，这幅图像具有四个通道ARGB（透明度和红绿蓝，对应了四幅相同大小的图像），假设卷积核大小为100 <em> 100，共使用100个卷积核w1到w100（从直觉来看，每个卷积核应该学习到不同的结构特征）。用w1在ARGB图像上进行卷积操作，可以得到隐含层的第一幅图像；这幅隐含层图像左上角第一个像素是四幅输入图像左上角100</em>100区域内像素的加权求和，以此类推。同理，算上其他卷积核，隐含层对应100幅“图像”。每幅图像对是对原始图像中不同特征的响应。按照这样的结构继续传递下去。CNN中还有max-pooling等操作进一步提高鲁棒性。</p><p>在这个例子里，我们注意到输入层到隐含层的参数瞬间降低到了100 <em> 100 </em> 100=10`6个！这使得我们能够用已有的训练数据得到良好的模型。题主所说的适用于图像识别，正是由于CNN模型限制参数了个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/440765dbaab356739fb855834f901e7d_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 3卷积神经网络隐含层（摘自Theano教程）</center><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/c71cd39abe8b0dd29e229f37058404da_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 4一个典型的卷积神经网络结构，注意到最后一层实际上是一个全连接层（摘自Theano教程）</center><p>典型的卷积神经网络由3部分构成：</p><ul><li><strong>卷积层</strong>：负责提取图像中的局部特征</li><li><strong>池化层</strong>：大幅降低参数量级(降维)</li><li><strong>全连接层</strong>：类似传统神经网络的部分，用来输出想要的结果。</li></ul><h4 id="1）卷积：提取特征"><a href="#1）卷积：提取特征" class="headerlink" title="1）卷积：提取特征"></a>1）卷积：提取特征</h4><p>卷积层的运算过程如下图，用一个卷积核扫完整张图片：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/v2-6428cf505ac1e9e1cf462e1ec8fe9a68_720w.gif" alt=""></p><p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p><p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/Inkedv2-08a3c438b08715ce15592c7bd0d923ae_720w_LI.jpg" alt=""></p><p><strong>总结：卷积层的通过卷积核的过滤提取出图片中局部的特征，跟上面提到的人类视觉的特征提取类似。</strong></p><h4 id="2）池化层（下采样）：数据降维，避免过拟合"><a href="#2）池化层（下采样）：数据降维，避免过拟合" class="headerlink" title="2）池化层（下采样）：数据降维，避免过拟合"></a>2）池化层（下采样）：数据降维，避免过拟合</h4><p>池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/v2-15e89ec6a866be1f7130655527079786_720w.gif" alt=""></p><p>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p><p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p><p><strong>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</strong></p><h4 id="3）全连接层：输出结果"><a href="#3）全连接层：输出结果" class="headerlink" title="3）全连接层：输出结果"></a>3）全连接层：输出结果</h4><p>这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。</p><p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/575577-20190802141514879-1961811671.png" alt=""></p><p>典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：</p><p><strong>卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/1628932130(1" alt="">.jpg)</p><h4 id="4）相关重点"><a href="#4）相关重点" class="headerlink" title="4）相关重点"></a>4）相关重点</h4><p><strong>1、卷积神经网络有2大特点</strong></p><ul><li>能够有效的将大数据量的图片降维成小数据量</li><li>能够有效的保留图片特征，符合图片处理的原则</li></ul><p><strong>2、卷积神经网络的擅长处理领域</strong></p><p>卷积神经网络 – 卷积神经网络最擅长的就是图片的处理</p><p><strong>3、卷积神经网络*解决了什么问题？*</strong></p><p>在卷积神经网络出现之前，图像对于人工智能来说是一个难题，有2个原因：</p><ul><li>图像需要处理的数据量太大，导致成本很高，效率很低</li><li>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高</li></ul><p><strong>A.需要处理的数据量太大</strong></p><p>图像是由像素构成的，每个像素又是由颜色构成的。现在随随便便一张图片都是 1000×1000 像素以上的， 每个像素都有RGB 3个参数来表示颜色信息。假如我们处理一张 1000×1000 像素的图片，我们就需要处理3百万个参数！</p><blockquote><p><strong>1000×1000×3=3,000,000</strong></p></blockquote><p>这么大量的数据处理起来是非常消耗资源的，而且这只是一张不算太大的图片！</p><p><strong>卷积神经网络 – CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。</strong></p><p><strong>更重要的是：我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。</strong></p><p><strong>B.保留图像特征</strong></p><p>假如一张图像中有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，<strong>图像的内容（本质）并没有发生变化，只是位置发生了变化</strong>。</p><p>所以当我们移动图像中的物体，用传统的方式的得出来的参数会差异很大！这是不符合图像处理的要求的。</p><p><strong>而 CNN 解决了这个问题，他用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。</strong></p><h3 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3><p>全连接的DNN还存在着另一个问题——无法对时间序列上的变化进行建模。然而，<strong>样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要</strong>。对了适应这种需求，就出现了题主所说的另一种神经网络结构——<strong>循环神经网络RNN。</strong></p><p>在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络（Feed-forward Neural Networks）。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！表示成图就是这样的</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/bef6a6073d311e79cad53eb47757af9d_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 5 RNN网络结构</center><p>我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/c2eb9099048761fd25f0e90aa66d363a_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 6 RNN在世间上展开</center><p>完美，<strong>（t+1）时刻网络的最终结果O（t+1）是该时刻输入和所有历史共同作用的结果！</strong>这就达到了对时间序列建模的目的。</p><p>不知题主是否发现，RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，<strong>“梯度消失”现象又要出现了，只不过这次发生在时间轴上</strong>。对于 t 时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。</p><p>为了解决时间上的梯度消失，机器学习领域发展出了<strong>长短时记忆单元LSTM，通过门的开关实现时间上记忆功能，并防止梯度消失</strong>，一个LSTM单元长这个样子：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/a8f4582707b70d41f250fdf0a43812fb_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 7 LSTM单元</center>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>马在棋盘上的概率</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<h5 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h5><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/1.jpg" alt=""></p><p>大意的意思就是在一个棋盘上，马按照象棋中马走日的规则，可以选择走K此后，最后还是留在棋盘的概率。</p><h5 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h5><ul><li><p>首先一匹马在任意位置可以选择八个方向走动，称之为方向向量，分别为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int[] dx = &#123;-2, -1, 1, 2, 2, 1, -1, -2&#125;;    //方向导向数组</span><br><span class="line">int[] dy = &#123;1, 2, 2, 1, -1, -2, -2, -1&#125;;</span><br></pre></td></tr></table></figure></li><li><p>其次由于棋盘是有限的，所以如果选择的步伐超出了棋盘，则视为无效。</p></li><li><p>现在给个例子，模拟一下其的走动方位，N=4，k=3，r=0，c=0</p><ul><li><p>1、一开始的时候，这匹马所在的位置</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/2.jpg" alt=""></p></li><li><p>2、走第一步的时候后可以选择的落脚点：只有两个方向是可以选择的呢，其余的都是超出了棋盘的范围</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/1-2.jpg" alt=""></p><p>走完第一步后的矩阵如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/3.jpg" alt=""></p></li><li><p>3、仿照上述过程，第二步的走向如下</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/4.jpg" alt=""></p></li></ul></li></ul><pre><code>​     走完第二步后的矩阵如下：![](https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/5.jpg)​    </code></pre><ul><li><p>第三步可选的落脚点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/6.jpg" alt=""></p><p>走完第三步后的矩阵：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/7.jpg" alt=""></p></li><li><p>到这里，就已经完成了这匹马在棋盘上面的全部可能走法了，最后一个还有 2+2+6+6+2+2 = 20次还留着棋盘上，所以概率就为 20 / 8 <em> 8 </em> 8</p></li></ul><ul><li><p>状态转移方程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[r][c][steps]:表示马在位置（r，c）移动了 steps 次后还留在棋盘上的概率</span><br></pre></td></tr></table></figure><p>根据马的移动，得到如下递归方程 如下</p></li></ul><script type="math/tex; mode=display">dp[r][c][steps] = \sum_{dr，dc}dp[r+dr][c+dc][steps-1] / 8.0</script><p>dr，dc就是上面说的方向向量的数组，根据这个递归处理的方程，我们可以采取一个二维数组进行编写，即一新一旧，一步一步更新这些数组，最后求和即可</p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">//力扣上的主函数</span><br><span class="line">public double knightProbability(int N, int K, int r, int c) &#123;</span><br><span class="line">       double[][] dp_old = new double[N][N]; //dp_old数组，dp[x][y]表示第i次到达dp[x][y]的方案数</span><br><span class="line">       double[][] dp_new = new double[N][N]; //dp_new数组，dp[i][j]表示第i+1次到达dp[x][y]的方案数</span><br><span class="line">       //初始化</span><br><span class="line">       dp_old[r][c] = 1;   //一开始的位置</span><br><span class="line">       for(int i = 0; i &lt; K; i++)&#123; //K次</span><br><span class="line">           for (int x = 0; x &lt; N; x++) &#123;</span><br><span class="line">               for (int y = 0; y &lt; N; y++) &#123;</span><br><span class="line">                   //四面八方累加dp</span><br><span class="line">                   dp_new[x][y] = computeSumFromDirection(dp_old, x, y);</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           //更新两个数组</span><br><span class="line">           dp_old = dp_new;</span><br><span class="line">           dp_new = new double[N][N];</span><br><span class="line">       &#125;</span><br><span class="line">       //遍历这个数组的总和就是落在棋盘内所有格子的方案数</span><br><span class="line">       double in = 0;</span><br><span class="line">       for (int x = 0; x &lt; N; x++) &#123;</span><br><span class="line">           for (int y = 0; y &lt; N; y++) &#123;</span><br><span class="line">               in += dp_old[x][y];</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return in;</span><br><span class="line">   &#125;</span><br><span class="line">//每次的八个方向的走法，并且判断是否越界</span><br><span class="line">   public double computeSumFromDirection(double[][] dp_old, int x, int y)&#123;</span><br><span class="line">       double sum = 0;</span><br><span class="line">       int[] dx = &#123;-2, -1, 1, 2, 2, 1, -1, -2&#125;;    //方向导向数组</span><br><span class="line">       int[] dy = &#123;1, 2, 2, 1, -1, -2, -2, -1&#125;;</span><br><span class="line">       for (int i = 0; i &lt; dx.length; i++)&#123;</span><br><span class="line">           if(!check(dp_old, x + dx[i], y + dy[i])) continue;</span><br><span class="line">           sum += dp_old[x + dx[i]][y + dy[i]];</span><br><span class="line">       &#125;</span><br><span class="line">       return sum / 8.0;</span><br><span class="line">   &#125;</span><br><span class="line">//给定位置判断是否越界</span><br><span class="line">   public boolean check(double[][] dp_old, int x, int y)&#123;     //越界判断</span><br><span class="line">       return x &gt;= 0 &amp;&amp; x &lt; dp_old.length &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; dp_old[0].length;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>学习使我快乐！！！</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/8.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最长递增子序列</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划—两道有趣的题目"><a href="#动态规划—两道有趣的题目" class="headerlink" title="动态规划—两道有趣的题目"></a>动态规划—两道有趣的题目</h2><h3 id="one：eetcode-300最长递增子序列"><a href="#one：eetcode-300最长递增子序列" class="headerlink" title="one：eetcode 300最长递增子序列"></a>one：eetcode 300最长递增子序列</h3><p><a href="https://leetcode-cn.com/problems/longest-increasing-subsequence/submissions/">https://leetcode-cn.com/problems/longest-increasing-subsequence/submissions/</a></p><h4 id="1、题目大意"><a href="#1、题目大意" class="headerlink" title="1、题目大意"></a>1、题目大意</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/1.jpg" alt=""></p><h4 id="2、分析"><a href="#2、分析" class="headerlink" title="2、分析"></a>2、分析</h4><p>看到题目，判断某一个数组的最长递增子序列，<strong>注意并不是连续的</strong>。</p><h4 id="3、方法1：完全递归"><a href="#3、方法1：完全递归" class="headerlink" title="3、方法1：完全递归"></a>3、方法1：完全递归</h4><ul><li><p>现在假设下标 <strong>i</strong> 结尾的数组的最唱递增子序列为 max，</p><p>若nums[i+1]&gt;nums[i] ; 则下标 <strong>i+1</strong> 结尾的数组的最唱递增子序列为 max+1，否则为 max</p></li><li><p>所以这个题目是可以拆解子问题的，有子问题最后堆砌到最终答案</p></li><li><p>设 函数  <strong>fun(n,nums)</strong>  : 表示在数组nums下，以n作为下标的最大递增序列</p><p>得到递归方程 ：<strong>fun(n,nums) = fun(j,nums)+1</strong> 其中 <strong>0&lt;=j&lt;i</strong> 并且 <strong>dp[j]&lt;dp[i] j为【0，i】</strong>里面的任意值，需要遍历</p></li><li><p>拿下图为递归树（简约哈）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/2.png" alt=""></p></li><li><p><strong>代码：</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">//方法一完全递归</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //直接遍历以每个下标的结尾的最大递增序列，再取其中的最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = fmax(ans,fun(i,nums));</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line">//定义递归函数</span><br><span class="line">//fun(n,nums) : 表示在数组nums下，以n作为下标的最大递增序列</span><br><span class="line">//fun(n,nums) = fun(j,nums)+1 其中 0&lt;=j&lt;i 并且 dp[j]&lt;dp[i] j为[0，i]里面的任意值，需要遍历</span><br><span class="line">int fun(int n,int* nums)&#123;</span><br><span class="line">    //最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    //递归出口,下标为0，即是返回1</span><br><span class="line">    if(n == 0)&#123;</span><br><span class="line">        ans = 1;</span><br><span class="line">        return ans;</span><br><span class="line">    &#125;</span><br><span class="line">    //按照递归方程开始，开始递归求解 </span><br><span class="line">    for(int i=0;i&lt;n;i++)&#123;</span><br><span class="line">        if(nums[i]&lt;nums[n])&#123;</span><br><span class="line">            ans = fmax(ans,fun(i,nums)+1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然，这样递归的结果就是，提交超时，hhhhhhh。但是没关系，思路对了，下面进行优化。</p><h4 id="4、方法2：记忆化加递归"><a href="#4、方法2：记忆化加递归" class="headerlink" title="4、方法2：记忆化加递归"></a>4、方法2：记忆化加递归</h4><ul><li><p>在方法一的基础上，我们可以记录一个记忆化的数组，在递归刚刚开始的时候去判断这个数组是否有值，有的话直接递归返回了，若没有，则进行递归</p></li><li><p>再拿上一张图片来看，比如递归到数字 2 的时候，我们记录好递归到数字 2 的记忆数组值，当下次在别的树枝上需要递归数字 2 的时候，便可以直接取了，而不用继续遍历了，效率会高很多</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/2.png" alt=""></p></li><li><p><strong>代码</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">//方法二，在方法1的基础上：变为 递归+记忆化</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //记忆化数组建立</span><br><span class="line">    int remenber[numsSize];</span><br><span class="line">    memset(remenber, -1,sizeof(int)* numsSize);</span><br><span class="line">    //直接遍历以每个下标的结尾的最大递增序列，再取其中的最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = fmax(ans,fun1(i,nums,remenber));</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line">//定义递归函数</span><br><span class="line">//fun1(n,nums) : 表示在数组nums下，以n作为下标的最大递增序列</span><br><span class="line">//fun1(n,nums) = fun1(j,nums)+1  其中 0&lt;j&lt;n</span><br><span class="line">int fun1(int n,int* nums,int *remenber)&#123;</span><br><span class="line">    //先判断记忆化数组里面是否有这个值，有直接返回，不用继续递归了</span><br><span class="line">    if(remenber[n]!=-1)return remenber[n];</span><br><span class="line">    //最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    //递归出口,下标为0，即是返回1</span><br><span class="line">    if(n == 0)&#123;</span><br><span class="line">        ans = 1;</span><br><span class="line">        return ans;</span><br><span class="line">    &#125;</span><br><span class="line">    //按照递归方程开始，开始递归求解 </span><br><span class="line">    for(int i=0;i&lt;n;i++)&#123;</span><br><span class="line">        if(nums[i]&lt;nums[n])&#123;</span><br><span class="line">            ans = fmax(ans,fun1(i,nums,remenber)+1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //同时添加到记忆化数组</span><br><span class="line">    remenber[n] = ans;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这次提交，是可以通过的，足见时间效率上提高了可不少。</p><h4 id="5、方法3：动态规划"><a href="#5、方法3：动态规划" class="headerlink" title="5、方法3：动态规划"></a>5、方法3：动态规划</h4><ul><li><p>根据上述两个方法的分析的，可以很容易得到动态规划的状态转移方程</p></li><li><p>dp[i] : 表示以 i 为下标结尾的数组的最长递增字符串</p></li><li><p>状态转移方程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i] = max(dp[j]+1) 其中 0&lt;=j&lt;i 并且 dp[j]&lt;dp[i] j为[0，i]里面的任意值，需要遍历</span><br></pre></td></tr></table></figure></li><li><p><strong>代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //最后的最大值</span><br><span class="line">    int ansMax = 1;</span><br><span class="line">    //1、建立dp数组</span><br><span class="line">    int dp[numsSize];</span><br><span class="line">    memset(dp, 0, sizeof(int) * numsSize);</span><br><span class="line">    //2、递归遍历，封装dp数组</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        //记录遍历j属于【0，i】里面的最大值。初始值为1，表示本身</span><br><span class="line">        int max = 1;</span><br><span class="line">        for (int j = 0; j &lt; i;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(nums[j]&lt;nums[i]) max = fmax(max, dp[j] + 1);</span><br><span class="line">        &#125;</span><br><span class="line">        //给dp赋值</span><br><span class="line">        dp[i] = max;</span><br><span class="line">        //每次比较，取最大</span><br><span class="line">        ansMax = dp[i] &gt; ansMax ? dp[i] : ansMax;</span><br><span class="line">    &#125;</span><br><span class="line">    return ansMax;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个是比较常规的方法了：时间复杂度是 <strong>O（N*N）</strong></p></li></ul><h4 id="6、方法4：贪心-二分查找"><a href="#6、方法4：贪心-二分查找" class="headerlink" title="6、方法4：贪心+二分查找"></a>6、方法4：贪心+二分查找</h4><ul><li>考虑一个简单的贪心，如果我们要使上升子序列尽可能的长，则我们需要让序列上升得尽可能慢，因此我们希望每次在上升子序列最后加上的那个数尽可能的小。</li><li>基于上面的贪心思路，我们维护一个数组 dp[i] ，表示长度为 <strong>i</strong> 的最长上升子序列的末尾元素的最小值，用 <strong>len</strong> 记录目前最长上升子序列的长度，起始时<strong>len =1，d[1]=nums[0]</strong>。</li></ul><ul><li><p>由定义知dp数组必然是一个递增数组,  对原数组<strong>nums</strong>进行迭代, 依次判断每个数<strong>num</strong>将其插入dp数组相应的位置:</p><ol><li><strong>num &gt; dp[len]</strong>, 表示num比所有已知递增序列的尾数都大, 将num添加入dp 数组尾部, 并将最长递增序列长度len加1</li><li><strong>dp[i-1] &lt; num &lt;= dp[i]</strong>, 只更新相应的dp[i]=num</li></ol></li><li><p>以 nums=[4,10,3,8,9]：</p><p>1)第一步插入4，则 dp=[4]</p><p>2) 第二步插入10，则dp=[4，10]</p><p>3) 第三步插入3，原数组4的位置更新为3 则dp=[4，10]==》dp=[3，10]</p><p>4) 第四步插入8，原数组10的位置更新为8 则dp=[4，10]==》dp=[3，8]</p><p>5) 第五步插入9，则dp=[3，8，9] </p><p>所以最后的答案为 <strong>len(dp) = 3</strong>;</p></li><li><p><strong>代码：</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI3(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">int lengthOfLI(int* nums, int numsSize)&#123;</span><br><span class="line">    //1、建立dp</span><br><span class="line">    int dp[numsSize];</span><br><span class="line">    memset(dp, 0, sizeof(int) * numsSize);</span><br><span class="line">    int index = 0;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;   </span><br><span class="line">        //直接二分查找dp中的第一个大于等于nums[i]的值</span><br><span class="line">        int left = 0, right = index;</span><br><span class="line">        while(left&lt;right)&#123;</span><br><span class="line">            int mid = (left + right) / 2;</span><br><span class="line">            if(dp[mid]&lt;nums[i])left = mid + 1;</span><br><span class="line">            else right = mid;</span><br><span class="line">        &#125;</span><br><span class="line">        dp[left] = nums[i];</span><br><span class="line">        if(right == index) index++;</span><br><span class="line">    &#125;</span><br><span class="line">    return index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="two-：354-俄罗斯套娃"><a href="#two-：354-俄罗斯套娃" class="headerlink" title="two ：354 俄罗斯套娃"></a>two ：354 俄罗斯套娃</h3><p><a href="https://leetcode-cn.com/problems/russian-doll-envelopes/">https://leetcode-cn.com/problems/russian-doll-envelopes/</a></p><h4 id="1、题目大意-1"><a href="#1、题目大意-1" class="headerlink" title="1、题目大意"></a>1、题目大意</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/3.jpg" alt=""></p><h4 id="2、分析-1"><a href="#2、分析-1" class="headerlink" title="2、分析"></a>2、分析</h4><ul><li><p>根据题目的要求，如果我们选择了 k 个信封，它们的</p></li><li><p>宽度依次为 w0, w1, ···, w k-1 高度依次为 h0, h1,···, h k-1 ，那么需要满足如下了两个条件：</p><script type="math/tex; mode=display">W0<W1<···<Wk-1</script><script type="math/tex; mode=display">h0<h1<···<hk-1</script><p>同时控制 w 和 h 两个维度并不是那么容易，因此我们考虑固定一个维度，再在另一个维度上进行选择。例如，我们固定 w 维度，那么我们将数组<strong>envelopes</strong> 中的所有信封按照 w 升序排序。这样一来，我们只要按照信封在数组中的出现顺序依次进行选取，就一定保证满足：</p><script type="math/tex; mode=display">W0≤W1≤···≤Wk-1</script></li><li><p>然而小于等于 ≤ 和小于 &lt;还是有区别的，但我们不妨首先考虑一个简化版本的问题：</p><p>如果我们保证所有信封的 w 值互不相同，那么我们可以设计出一种得到答案的方法吗？</p><p>在 w 值互不相同的前提下，小于等于≤ 和小于 &lt; 是等价的，那么我们在排序后，就可以完全忽略 w 维度，只需要考虑 h 维度了。此时，我们需要解决的问题即为：</p><p>给定一个序列，我们需要找到一个最长的子序列，使得这个子序列中的元素严格单调递增，即上面要求的：</p><script type="math/tex; mode=display">h0<h1<···<hk-1</script><p>那么这个问题就是经典的「最长严格递增子序列」问题，问题得到解决，</p></li><li><p>当我们解决了简化版本的问题之后，我们来想一想使用上面的方法解决原问题，会产生什么错误？当 w 值相同时，如果我们不规定 h 值的排序顺序，那么可能会有如下的情况：</p><p>排完序的结果为 [(w, h)] = [(1, 1), (1, 2), (1, 3), (1, 4)][(w,h)]=[(1,1),(1,2),(1,3),(1,4)]，由于这些信封的 w 值都相同，不存在一个信封可以装下另一个信封，那么我们只能在其中选择 1 个信封。然而如果我们完全忽略 w 维度，剩下的 h 维度为 [1, 2, 3, 4][1,2,3,4]，这是一个严格递增的序列，那么我们就可以选择所有的 4 个信封了，这就产生了错误。</p><p>因此，我们必须要保证对于每一种 w 值，我们最多只能选择 1 个信封。</p><p>我们可以将 h 值作为排序的第二关键字进行降序排序，这样一来，对于每一种 w 值，其对应的信封在排序后的数组中是按照 h 值递减的顺序出现的，那么这些 h 值不可能组成长度超过 1 的严格递增的序列，这就从根本上杜绝了错误的出现。</p></li><li><p>因此我们就可以得到解决本题需要的方法：</p><ul><li><p>首先我们将所有的信封按照 w 值第一关键字升序、h 值第二关键字降序进行排序；</p></li><li><p>随后我们就可以忽略 w 维度，求出 h 维度的最长严格递增子序列，其长度即为答案。</p></li></ul></li></ul><ul><li>至此分析完了，归根到底就是<strong>最长递增子序列</strong>的问题了</li></ul><h4 id="3、代码"><a href="#3、代码" class="headerlink" title="3、代码"></a>3、代码</h4><p><strong><em>直接show code no say say</em></strong></p><ul><li><p>常规动态规划</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int* p[4];</span><br><span class="line">    int points[4][4] = &#123;&#123;5,4&#125;,&#123;6,4&#125;,&#123;6,7&#125;,&#123;2,3&#125;&#125;;</span><br><span class="line">    p[0] = &amp;points[0][0];</span><br><span class="line">    p[1] = &amp;points[1][0];</span><br><span class="line">    p[2] = &amp;points[2][0];</span><br><span class="line">    p[3] = &amp;points[3][0];</span><br><span class="line">    int envelopesSize = 4;</span><br><span class="line">    int envelopesColSize[] = &#123;2,2,2,2&#125;;</span><br><span class="line">    int ans = maxEnvelopes1(p,envelopesSize,envelopesColSize);</span><br><span class="line">    printf(&quot;%d&quot;, ans);</span><br><span class="line">&#125;</span><br><span class="line">//方法一：普通动态规划</span><br><span class="line">//dp[i]：表示以下标i为结尾的最大增序列  再次遍历取其最大</span><br><span class="line">//状态转移方程 dp[i] = dp[j]+1 其中 j&lt;i 并且排好序的envelopes中envelopes[j][1]     &lt;envelopes[i][1] </span><br><span class="line">int maxEnvelopes(int** envelopes, int envelopesSize, int* envelopesColSize)&#123;</span><br><span class="line">    //1、先排序，首先按照第一列升序排序，若第一列的值相同，则按照第二列的值降序排序</span><br><span class="line">    qsort(envelopes, envelopesSize, sizeof(int*), compare);</span><br><span class="line">    //2、构建dp数组</span><br><span class="line">    int dp[envelopesSize+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0]) * (envelopesSize + 1));</span><br><span class="line">    //3、根据状态转移方程，递推求dp</span><br><span class="line">    for (int i = 1; i &lt;= envelopesSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        //遍历【1，i】位置 找符合条件的最大dp[j]+1</span><br><span class="line">        int max = 1;</span><br><span class="line">        for (int j = 1; j &lt; i;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(envelopes[i-1][1]&gt;envelopes[j-1][1]) max = fmax(max, dp[j] + 1);</span><br><span class="line">        &#125;</span><br><span class="line">        //赋值给dp</span><br><span class="line">        dp[i] = max;</span><br><span class="line">    &#125;</span><br><span class="line">    //4、取其最大</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; sizeof(dp) / sizeof(dp[0]); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = dp[i] &gt; ans ? dp[i] : ans;</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//第一个参数升序 第二个参数降序</span><br><span class="line">int compare(const void *a, const void *b)&#123;</span><br><span class="line">    int* num1 = *(int**)a;</span><br><span class="line">    int* num2 = *(int**)b;</span><br><span class="line">    if(num1[0]==num2[0]) return num2[1] - num1[1];</span><br><span class="line"> else  return num1[0] - num2[0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>基于二分查找的动态规划</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int* p[4];</span><br><span class="line">    int points[4][4] = &#123;&#123;5,4&#125;,&#123;6,4&#125;,&#123;6,7&#125;,&#123;2,3&#125;&#125;;</span><br><span class="line">    p[0] = &amp;points[0][0];</span><br><span class="line">    p[1] = &amp;points[1][0];</span><br><span class="line">    p[2] = &amp;points[2][0];</span><br><span class="line">    p[3] = &amp;points[3][0];</span><br><span class="line">    int envelopesSize = 4;</span><br><span class="line">    int envelopesColSize[] = &#123;2,2,2,2&#125;;</span><br><span class="line">    int ans = maxEnvelopes1(p,envelopesSize,envelopesColSize);</span><br><span class="line">    printf(&quot;%d&quot;, ans);</span><br><span class="line">&#125;</span><br><span class="line">//方法二：基于二分查找的动态规划</span><br><span class="line">int maxEnvelopes1(int** envelopes, int envelopesSize, int* envelopesColSize) &#123;</span><br><span class="line">    if (envelopesSize == 0)  return 0;</span><br><span class="line">    //1、先排序，首先按照第一列升序排序，若第一列的值相同，则按照第二列的值降序排序</span><br><span class="line">    qsort(envelopes, envelopesSize, sizeof(int*), compare);</span><br><span class="line">    //2、构建dp数组</span><br><span class="line">    int dp[envelopesSize], indexSize = 0;</span><br><span class="line">    dp[indexSize++] = envelopes[0][1];</span><br><span class="line">    for (int i = 1; i &lt; envelopesSize; ++i) &#123;</span><br><span class="line">        int num = envelopes[i][1];</span><br><span class="line">        if (num &gt; dp[indexSize - 1])  dp[indexSize++] = num;</span><br><span class="line">        else &#123;</span><br><span class="line">            //在dp中寻找第一个大于等于num的值的下标，进而替换她</span><br><span class="line">            int index = lower_bound(dp, indexSize, num);</span><br><span class="line">            dp[index] = num;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return indexSize;</span><br><span class="line">&#125;</span><br><span class="line">//二分查找</span><br><span class="line">int lower_bound(int* arr, int arrSize, int val) </span><br><span class="line">&#123;</span><br><span class="line">    int left = 0, right = arrSize - 1;</span><br><span class="line">    while (left &lt;= right) &#123;</span><br><span class="line">        int mid = (left + right) &gt;&gt; 1;</span><br><span class="line">        if (val &lt; arr[mid])  right = mid - 1;</span><br><span class="line">        else if (val &gt; arr[mid]) left = mid + 1;</span><br><span class="line">        else  return mid;</span><br><span class="line">    &#125;</span><br><span class="line">    if (arr[left] &gt;= val)  return left;</span><br><span class="line">    return -1;</span><br><span class="line">&#125;</span><br><span class="line">//第一个参数升序 第二个参数降序</span><br><span class="line">int compare(const void *a, const void *b)&#123;</span><br><span class="line">    int* num1 = *(int**)a;</span><br><span class="line">    int* num2 = *(int**)b;</span><br><span class="line">    if(num1[0]==num2[0])&#123;</span><br><span class="line">        return num2[1] - num1[1];</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return num1[0] - num2[0];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="summary："><a href="#summary：" class="headerlink" title="summary："></a>summary：</h3><p>套路：遇到这种数组这种问题，经常想到以 <strong>下标i</strong> 为结尾作为的子问题，直接定义 <strong>dp【i】</strong>：为以 <strong>i</strong> 作为下标结尾的数组怎么怎么。。。。 </p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/4.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
            <tag> 子序列问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BFS和DFS模板</title>
      <link href="%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/BFS%E5%92%8CDFS%E6%A8%A1%E6%9D%BF/"/>
      <url>%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/BFS%E5%92%8CDFS%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<p><strong>首先，总结一定的模板做题是十分有作用的，善于总结才是我们加强算法能力的表现。做总结可以提高我们的代码能力，可以比较快速解决算法问题，也会更加清晰算法的流程！！十分有必要！！</strong></p><p><strong>BFS的模板：</strong></p><ul><li>1、如果不需要确定当前遍历到了哪一层，BFS 模板如下。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while queue 不空：</span><br><span class="line">    cur = queue.pop()</span><br><span class="line">    for 节点 in cur的所有相邻节点：</span><br><span class="line">        if 该节点有效且未访问过：</span><br><span class="line">            queue.push(该节点)</span><br></pre></td></tr></table></figure><ul><li>2、如果要确定当前遍历到了哪一层，BFS 模板如下。 这里增加了 level 表示当前遍历到二叉树中的哪一层了，也可以理解为在一个图中，现在已经走了多少步了。size 表示在当前遍历层有多少个元素，也就是队列中的元素数，我们把这些元素一次性遍历完，即把当前层的所有元素都向外走了一步。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">level = 0</span><br><span class="line">while queue 不空：</span><br><span class="line">    size = queue.size()</span><br><span class="line">    while (size --) &#123;</span><br><span class="line">        cur = queue.pop()</span><br><span class="line">        for 节点 in cur的所有相邻节点：</span><br><span class="line">            if 该节点有效且未被访问过：</span><br><span class="line">                queue.push(该节点)</span><br><span class="line">    &#125;</span><br><span class="line">    level ++;</span><br></pre></td></tr></table></figure><p><strong>DFS模板（回溯）</strong></p><ul><li>1、最本质的法宝是“画图”，千万不能偷懒，拿纸和笔“画图”能帮助我们更好地分析递归结构，这个“递归结构”一般是“树形结构”，而符合题意的解正是在这个“树形结构”上进行一次“深度优先遍历”，这个过程有一个形象的名字，叫“搜索”；我们写代码也几乎是“看图写代码”，所以“画树形图”很重要。</li><li>2、然后使用一个状态变量，一般我习惯命名为 path、pre ，在这个“树形结构”上使用“深度优先遍历”，根据题目需要在适当的时候把符合条件的“状态”的值加入结果集；这个“状态”可能在叶子结点，也可能在中间的结点，也可能是到某一个结点所走过的路径。</li><li>3、在某一个结点有多个路径可以走的时候，使用循环结构。当程序递归到底返回到原来执行的结点时，“状态”以及与“状态”相关的变量需要“重置”成第 1 次走到这个结点的状态，这个操作有个形象的名字，叫“回溯”，“回溯”有“恢复现场”的意思：意即“回到当时的场景，已经走过了一条路，尝试走下一条路”。第 2 点中提到的状态通常是一个列表结构，因为一层一层递归下去，需要在列表的末尾追加，而返回到上一层递归结构，需要“状态重置”，因此要把列表的末尾的元素移除，符合这个性质的列表结构就是“栈”（只在一头操作）。</li><li>4、当我们明确知道一条路走不通的时候，例如通过一些逻辑计算可以推测某一个分支不能搜索到符合题意的结果，可以在循环中 continue 掉，这一步操作叫“剪枝”。“剪枝”的意义在于让程序尽量不要执行到更深的递归结构中，而又不遗漏符合题意的解。因为搜索的时间复杂度很高，“剪枝”操作得好的话，能大大提高程序的执行效率。“剪枝”通常需要对待搜索的对象做一些预处理，例如第 47 题、第 39 题、第 40 题、第 90 题需要对数组排序。“剪枝”操作也是这一类问题很难的地方，有一定技巧性。总结一下：“回溯” = “深度优先遍历” + “状态重置” + “剪枝”，写好“回溯”的前提是“画图”。因此，非要写一个模板，我想它可能长这个样子：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def backtrack(待搜索的集合, 递归到第几层, 状态变量 1, 状态变量 2, 结果集):</span><br><span class="line">    # 写递归函数都是这个套路：先写递归终止条件</span><br><span class="line">    if 可能是层数够深了:</span><br><span class="line">        # 打印或者把当前状态添加到结果集中</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    for 可以执行的分支路径 do           //分支路径</span><br><span class="line">        </span><br><span class="line">        # 剪枝</span><br><span class="line">        if 递归到第几层, 状态变量 1, 状态变量 2, 符合一定的剪枝条件:</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line">        对状态变量状态变量 1, 状态变量 2 的操作（#）</span><br><span class="line">   </span><br><span class="line">        # 递归执行下一层的逻辑</span><br><span class="line">        backtrack(待搜索的集合, 递归到第几层, 状态变量 1, 状态变量 2, 结果集)</span><br><span class="line"></span><br><span class="line">        对状态变量状态变量 1, 状态变量 2 的操作（与标注了 # 的那一行对称，称为状态重置）</span><br><span class="line">        </span><br><span class="line">    end for</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小tips </tag>
            
            <tag> DFS </tag>
            
            <tag> BFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>背包问题</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划（Dynamic Programming，简称DP）动态规划常常适用于有<strong>重叠子问题</strong>和<strong>最优子结构</strong>性质的问题，动态规划方法所耗时间往往远少于朴素解法。</p><p>动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。 通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量： 一旦某个给定子问题的解已经算出，则将其<strong>记忆化</strong>存储，以便下次需要同一个子问题解之时直接查表。 这种做法在重复子问题的数目关于输入的规模呈<strong>指数增长</strong>时特别有用。虽然抽象后进行求解的思路并不复杂，但具体的形式千差万别，找出问题的子结构以及通过子结构重新构造最优解的过程很难统一，为了解决动态规划问题，只能靠多练习、多思考了。</p><p><strong><em>\</em>动态规划问题满足三大重要性质**</strong></p><p><strong>最优子结构性质：</strong>如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态规划算法解决问题提供了重要线索。</p><p><strong>子问题重叠性质：</strong>子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。</p><p><strong>无后效性</strong>：将各阶段按照一定的次序排列好之后，对于某个给定的阶段状态，它以前各阶段的状态无法直接影响它未来的决策，而只能通过当前的这个状态。换句话说，每个状态都是过去历史的一个完整总结。这就是无后向性，又称为无后效性。</p><p><strong><em>重点</em></strong>   dp数组的含义 + 状态转移方程 （具体问题具体分析）</p><p><strong>首先背包问题是我们接触动态规划比不可取的经典问题，重要的问题说三遍，经典经典经典。</strong></p><ul><li>0-1背包问题</li><li>完全背包问题</li><li>多重背包问题</li></ul><h2 id="0-1背包问题"><a href="#0-1背包问题" class="headerlink" title="0-1背包问题"></a>0-1背包问题</h2><h3 id="1、题目描述"><a href="#1、题目描述" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p>​    这里你有N件物品和一个容量为M的背包，这N件物品的第 i 件物品的 价值是V[i] ，重量是W[i].问题是拿取这N件物品的哪几件时，使得背包可以装下（<u>意思就是物品的重量总和小于或等于M</u>）且价值最大。</p><p><strong>关键问题</strong>：其实这堆物品在你选择的时候无非就是两种路子可以选择：<strong>选 or 不选</strong></p><p>第一步：构建dp数组的含义，<code>dp[i][j]</code>  ：代表的是前 <strong>i</strong> 个物品加入容量为 <strong>j</strong> 的背包里面价值总和的最大值</p><p>第二步：分析状态转移方程</p><p>​    对于一个物品来说：要么选要么不选，</p><ul><li><p>选择这个物品：就是第<strong>i</strong>件物品放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j-w[i]] + V[i]   （j&gt;W[i]）</span><br></pre></td></tr></table></figure></li><li><p>不选择这个物品：就是舍弃第<strong>i</strong>件物品，不放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j]</span><br></pre></td></tr></table></figure><p>经过这两步的分析，可以得出这个问题的状态转移方程（即为重要，very very important）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i-1][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 4; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,2,4,3,7&#125;; //物品的价值 前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,5,5&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(j&lt;W[i])&#123;</span><br><span class="line">                dp[i][j] = dp[i - 1][j];  //装不下第i件物品，只能不要咯</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                dp[i][j] = fmax(dp[i - 1][j], dp[i - 1][j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、深入分析理解"><a href="#3、深入分析理解" class="headerlink" title="3、深入分析理解"></a>3、深入分析理解</h3><p>这个代码就是自下而上的方法，思路也是比较简单，就是不断遍历，不断填充dp表：</p><p>第一：初始化时候的表格：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti1.jpg" alt=""></p><p>第二：当 <strong>i</strong>=1的时候，只有物品1能够选择，如果背白容量够的话，那么此时的最大价值就是物品1的价值了</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti2.jpg" alt=""></p><p>第三：当<strong>i</strong>=2的时候，根据状态转移方程，此时取<strong>i</strong>=2，<strong>j</strong>=3的时候有如下转换：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti3.jpg" alt=""></p><p>最后，根据这样的规则：逐一填表得：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti4.jpg" alt=""></p><p>这样，就可以得到最后的结果：13了，我们也可以根据状态转移方程方向得到选择的物品是第1 2 4号物品。</p><p>到此，分析时间复杂度为填表的时间为<strong>O(N*M)</strong>  ， 空间复杂度为<strong>O(N*M)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti5.jpg" alt=""></p><h3 id="4、优化"><a href="#4、优化" class="headerlink" title="4、优化"></a>4、优化</h3><p>在这个问题上，其实时间上没什么好优化的了，只能从空间上进行一点优化 ，</p><p>首先我们再看看状态转移方程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i-1][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti6.jpg" alt=""></p><p>可以明显看出，在填<strong>i+1</strong>行的数据的时候，只用到了第<strong>i</strong>行的数据，根本就没有用到<strong>i-1</strong>行的数据，换句话说，填某一行的数据的时候只与其前一行有关，根据这个规律，我们就可以使用将二维dp降为一维dp，缩减空间。此所谓滚动数组。</p><p><strong>总结</strong>：<code>dp[i][j]</code>所依赖的值必须是没有更新的，所以后到前。（ps：完全背包正好相反）</p><p>此时状态转移方程  <code>dp[j]</code> : 表示容量不超过 <strong>j</strong> 的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[j] = Max(dp[j],dp[j-w[i]] + V[i])  j&gt;W[i]</span><br></pre></td></tr></table></figure><p><strong>代码实现：</strong></p><p>和上面的代码有一点区别，在填充dp数组的第二层循环的时候，不应该从前到后（左到右），而应该从后到前（右到左），因为如果选择从前到后（左到右），会导致前面的值被修改，而后面的的值确实依赖前面的值的，要保证后面值得依赖是不变了。所以在第二轮扫描得时候需要从后到前扫描。（下图看做一行滴数据哈）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti7.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 4; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,2,4,3,7&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,5,5&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化得</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = M; j &gt;= W[i]; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时时间复杂度 <strong>O(N*M）</strong>    空间复杂度<strong>O(M)</strong>   空间复杂度优化了挺多哦</p><h2 id="完全背包问题"><a href="#完全背包问题" class="headerlink" title="完全背包问题"></a>完全背包问题</h2><h3 id="1、题目描述-1"><a href="#1、题目描述-1" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p> 有N种物品和一个容量为M的背包，每种物品都就可以选择任意多个，第<strong>i</strong>种物品的价值为 V[i]，重量是 W[i]，求解：选哪些物品放入背包，可因使得这些物品的价值最大，并且体积总和不超过背包容量。</p><p><strong>分析：</strong></p><p>完全背包问题是在0-1背包问题的基础上略有不同，不同的是在0-1背包问题中，某一件物品要么取一件要么不取，但是在完全背包的问题中，某一件物品可以无限（任意）的取。</p><p>从物品的选择角度说也不是 <strong>选 OR 不选 </strong>的问题了，而是选 0 1 2 3 4 ，，，件的问题了，</p><p>默默嘀咕：曾经的我刚刚 接触的时候，贪心（有手就行），事实证明我还是年轻，打扰了，贪心解决不了。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti8.jpg" alt=""></p><p><strong>动态规划方法：</strong></p><p>第一步：构建dp数组的含义，<code>dp[i][j]</code>  ：代表的是前 <strong>i</strong> 个物品加入容量为 <strong>j</strong> 的背包里面价值总和的最大值</p><p>第二步：分析状态转移方程</p><p>​    对于一件新的物品来说：可以选可以不选，</p><ul><li><p>选择这个新物品：此时（拿了 <strong>i</strong> 号物品，我们还是继续拿 <strong>i</strong> 号物品）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i][j-w[i]] + V[i]   （j&gt;W[i]）</span><br></pre></td></tr></table></figure></li><li><p>不选择这个物品：就是舍弃全部的第 <strong>i</strong> 件物品，不放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j]</span><br></pre></td></tr></table></figure><p>经过这两步的分析，可以得出这个问题的状态转移方程（即为重要，very very important）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、代码实现-1"><a href="#2、代码实现-1" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,5,8&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//没有优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(j&lt;W[i])&#123;</span><br><span class="line">                dp[i][j] = dp[i - 1][j];  //装不下第i件物品，只能不要咯</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                //可以装下，可以选择 拿或者不拿  只是将 0-1背包问题中的 i-1 改为 i</span><br><span class="line">                dp[i][j] = fmax(dp[i - 1][j], dp[i][j - W[i]] + V[i]); </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注解</strong> ： 其实这边也还可以利用另外一个状态转移方程解决（自个摸索把）直接给出答案</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">f[i][j]=max(f[i-1][j-k*V[i]]+k*W[i],f[i][j])     0&lt;=k*c[i]&lt;=j</span><br><span class="line"></span><br><span class="line">代码：</span><br><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;5,8&#125;; //物品的价值</span><br><span class="line">    int W[] = &#123;5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//f[i][j]=max(f[i-1][j-k*V[i]]+k*W[i],f[i][j])     0&lt;=k*c[i]&lt;=j</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">            for (int j = 0; j &lt;= M; j++)&#123;</span><br><span class="line">                for (int k = 0; k * V[i] &lt;= j; k++)&#123;</span><br><span class="line">                    dp[i+1][j] = fmax(dp[i+1][j], dp[i][j-k * V[i]] + k * W[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、深入分析理解-1"><a href="#3、深入分析理解-1" class="headerlink" title="3、深入分析理解"></a>3、深入分析理解</h3><p>这边也可以画出表格来一步一步填这个表格的问题，自底向上，</p><p>第一步：初始化时的表格：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti9.jpg" alt=""></p><p>第二步：在第 <strong>i</strong> 个物品的时候，我们其实可以选择上一层中的几个位置中价值最高的那一个，在这里M=10，所以只需要将两个数值进行比较，如果M大于10，那么就需要将取0个、1个和两个i2物品的情况进行比较，然后选出最大值.</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti10.jpg" alt=""></p><p>到此，分析时间复杂度为填表的时间为<strong>O(N*M)</strong>  ， 空间复杂度为<strong>O(N*M)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti11.jpg" alt=""></p><h3 id="4、优化-1"><a href="#4、优化-1" class="headerlink" title="4、优化"></a>4、优化</h3><p>优化思路和0-1背包问题一模一样的，就不在这里赘述了，直接上状态方程和代码。</p><p><strong>总结</strong>：<code>dp[i][j]</code>所依赖的值必须是已经更新的，所以前到后。（ps：0-1背包正好相反）</p><p><strong>状态转移方程为</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f[j]=max(f[j-w[i]]+c[i], f[j]);</span><br></pre></td></tr></table></figure><p><strong>代码实现</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,5,8&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax1(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化得</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = W[i]; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时时间复杂度 <strong>O(N*M）</strong>    空间复杂度<strong>O(M)</strong>   空间复杂度也是变成线性的了。</p><p><strong>注解</strong>：其实还有一个小优化</p><p>比如两件物品 ：<strong>i</strong>   <strong>j</strong>    当  <strong>i</strong> 物品的重量比 <strong>j</strong> 的重，但是 <strong>i</strong> 的价值确比 <strong>j</strong> 的低，那我们岂不是可以直接跳过 <strong>i</strong> 了，直接选择 <strong>j</strong> 物品了。道理很简单，难道这世界上会有人去买一个又贵又难吃的东西？（富豪除外）</p><h2 id="多重背包问题"><a href="#多重背包问题" class="headerlink" title="多重背包问题"></a>多重背包问题</h2><h3 id="1、题目描述-2"><a href="#1、题目描述-2" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p>有N种物品和一个容量为V的背包。第i种物品最多有n[i]件可用，每件费用是w[i]，价值是c[i]。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。</p><p><strong>分析</strong></p><p>这里既不像0-1背包每种物品只有1件，也不像完全背包那样每种物品有无数件，而是限定来了每种物品的数量，并不是你想取多少就取多少，得看看人家有没有。</p><p>经过前面两个的分析，这个多重背包问题得状态转移方程和完全背包的状态转移方程岂不是一个爹娘的样子，<strong>就是K多了一个限制</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i,j] = max(dp(i-1, j - V[i] * k) + P[i] * k); （0 &lt;= k * V[i] &lt;= j &amp;&amp; 0 &lt;= k &lt;= n[i]）</span><br></pre></td></tr></table></figure><h3 id="2、代码实现-2"><a href="#2、代码实现-2" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[],int n[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 3; //不同物品的数量</span><br><span class="line">    int M = 15; //背包的容量</span><br><span class="line">    int V[] = &#123;3,4,5&#125;; //物品的价值</span><br><span class="line">    int W[] = &#123;2,3,4&#125;; //物品的重量</span><br><span class="line">    int n[] = &#123;4, 3, 2&#125;;//每种物品的个数</span><br><span class="line">    int re = seekMax(N, M, V, W,n);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//没有优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[],int n[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">            for (int j = 0; j &lt;= M; j++)&#123;</span><br><span class="line">                //k限制了条件 不加限制就是我上面讲的完全背包问题中在我没有优化的时候提出的另外一个方程的解</span><br><span class="line">                for (int k = 0; k &lt;= n[i] &amp;&amp; k * V[i] &lt;= j; k++)&#123;  </span><br><span class="line">                    dp[i+1][j] = fmax(dp[i+1][j], dp[i][j-k * V[i]] + k * W[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分析就不再分析了，和前面的一模一样的</p><h3 id="3、优化"><a href="#3、优化" class="headerlink" title="3、优化"></a>3、优化</h3><ul><li>优化1：这边优化也可以项0-1背包和完全背包的样子，将dp二维数组改为一维的dp数组，改为滚动数组，这里不再赘述</li><li>优化2：这里有一个比较巧妙的方法，完美将多重背包问题顺利转为0-1背包的问题了。下面会详细讲解这个优化（鄙人比较pick）</li></ul><p>举个例子：比如有一种物品，她一共有8件，我们再取的时候可以取得 0 1 2 3 4 5 6 7 8 件这九种情况，但是我们在取得时候是不知道该取多少件得，这时候我们可以把这八件物品分堆，使得我们可以取得上述得九种情况的任意一种，所以分堆便是重点了，这里分堆采用2进制的方法进行分堆</p><p><strong>统一</strong>：分为 1 2 4 8 。。。总的减去前面的总和（因为最后一个并不一定是2的整数次幂）</p><script type="math/tex; mode=display">n = 2^0 + 2^1 + 2^2 + 2^3...+ 2^h+(n-2^c+1)       （其中 h=c-1）</script><p>例如八件同一件物品：分为大小为 1 2 4 1 的四个堆即可，任意组合可以得到上述选择的九种可能，此时将这四个堆想象成0-1背包问题种的不一样的物品即可。 </p><p>状态转移方程和0-1背包相同</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[],int n[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 3; //不同物品的数量</span><br><span class="line">    int M = 15; //背包的容量</span><br><span class="line">    int V[] = &#123;0,3,4,5&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,4&#125;; //物品的重量</span><br><span class="line">    int n[] = &#123;0,4,3,2&#125;;//每种物品的个数</span><br><span class="line">    int re = seekMax(N, M, V, W,n);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[],int n[])</span><br><span class="line">&#123;</span><br><span class="line">    //创建分堆后的价值和重量数组 这个大小可以根据题目给的数据范围来确定</span><br><span class="line">    int newW[N * 20];</span><br><span class="line">    int newV[M * 20]; </span><br><span class="line">    newW[0] = 0;</span><br><span class="line">    newV[0] = 0;</span><br><span class="line">    //先分堆 完善上面两个数组</span><br><span class="line">    int number = 0; //分堆后的总堆数</span><br><span class="line">    for (int i = 1; i &lt;= N;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= n[i];j *= 2)</span><br><span class="line">        &#123;</span><br><span class="line">            number++;</span><br><span class="line">            newW[number] = W[i] * j;</span><br><span class="line">            newV[number] = V[i] * j;</span><br><span class="line">            n[i] -= j;</span><br><span class="line">        &#125;</span><br><span class="line">        //最后那个</span><br><span class="line">        if(n[i]&gt;0)&#123;</span><br><span class="line">            number++;</span><br><span class="line">            newW[number] = W[i] * n[i];</span><br><span class="line">            newV[number] = V[i] * n[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= number; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = M; j &gt;= newW[i]; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - newW[i]] + newV[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此三个经典的背包问题解决啦。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti13.jpg" alt=""></p><h2 id="混合背包问题"><a href="#混合背包问题" class="headerlink" title="混合背包问题"></a>混合背包问题</h2><p>所谓混合背包的问题无非就是前面三种背包的杂糅操作，比如有的物品符合0-1背包（只能够取1件或者不取），有的物品符合完全背包问题（一件物品能够取任意件），有的物品符合多重背包问题（一种物品只能怪取限定件）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">伪代码：</span><br><span class="line">for(i = 0 ;i&lt;N;i++)&#123;</span><br><span class="line">if i属于0-1背包问题 </span><br><span class="line">采用0-1解决方法</span><br><span class="line">else if i属于完全背包问题</span><br><span class="line">采用完全解决方法</span><br><span class="line">else if i属于多重背包问题</span><br><span class="line">采用多重解决方法</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti12.jpg" alt=""></p><p>上述三种经典背包问题可谓了解动态规划的经典之作了，其实三种背包问题 都是有着异曲同工之妙呀，认真解读会让自己了解的更加深刻，舒服。其实关于背包问题的变形变异还有很多类似的题目，后续加以继续撸。。。鄙人不才，若有误望指正，本文章也是采取一些其他博客的思路，谢谢各路大神。</p><p>解决拥有子问题的问题，可以有三个方法</p><ul><li>朴素递归 （效率很低）</li><li>递归 + 记忆化 （效率较高）</li><li>递推完善dp数组（效率最高）动态规划常用</li></ul><p><strong><em>重点</em></strong>   dp数组的含义 + 状态转移方程 （具体问题具体分析）</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><p>[]: <a href="https://blog.csdn.net/woshi250hua/article/details/7636866">https://blog.csdn.net/woshi250hua/article/details/7636866</a></p><p>这位好心的博主列举了一些背包模型的例子，可以利于自己继续练习</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti21.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
            <tag> 背包问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程中无穷大常量(ox3f3f3f3f)的设定技巧</title>
      <link href="%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B%E4%B8%AD%E6%97%A0%E7%A9%B7%E5%A4%A7%E5%B8%B8%E9%87%8F-ox3f3f3f3f-%E7%9A%84%E8%AE%BE%E5%AE%9A%E6%8A%80%E5%B7%A7/"/>
      <url>%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B%E4%B8%AD%E6%97%A0%E7%A9%B7%E5%A4%A7%E5%B8%B8%E9%87%8F-ox3f3f3f3f-%E7%9A%84%E8%AE%BE%E5%AE%9A%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h4 id="编程中无穷大常量的设定技巧"><a href="#编程中无穷大常量的设定技巧" class="headerlink" title="编程中无穷大常量的设定技巧"></a>编程中无穷大常量的设定技巧</h4><p>首先，在做某一些算法的时候，会很常求最大最小值一类的问题，通常我们会设置一个初始一个answer最大int类型的最大或者最小，然后每次比较取大取小即可。</p><p>其实如果问题中各数据的范围明确，那么无穷大的设定不是问题，在不明确的情况下，很多程序员都取0x7fffffff作为无穷大，因为这是32-bit int的最大值。如果这个无穷大只用于一般的比较（比如求最小值时min变量的初值），那么0x7fffffff确实是一个完美的选择，但是在更多的情况 下，0x7fffffff并不是一个好的选择。理由如下：</p><p>很多时候我们并不只是单纯拿无穷大来作比较，而是会运算后再做比较，例如在大部分最短路径算法中都会使用的松弛操作：<br>if (d[u]+w[u][v]&lt;d[v]) d[v]=d[u]+w[u][v];<br>我们知道如果u,v之间没有边，那么w[u][v]=INF，如果我们的INF取0x7fffffff，那么d[u]+w[u][v]会溢出而变成负数， 我们的松弛操作便出错了，更一般的说，0x7fffffff不能满足“无穷大加一个有穷的数依然是无穷大”，它变成了一个很小的负数。</p><p>计算机不会表示出“无穷大”的概念，所以我们只能以一个定值来表示“最大”。那么使用什么值呢？</p><p>32-bit int举例，我们选择的最大应该满足两个条件</p><ul><li><strong>这个最大值真的很大，是和定义的最大值是同一个数量级的</strong></li><li><strong>这个最大值+这个最大值并不会溢出的，也就是无穷大嘉无穷大依然是无穷大</strong></li></ul><p>所以我们需要一个更好的家伙来顶替 0x7fffffff ，最严谨的办法当然是对无穷大进行特别处理而不是找一个很大很大的常量来代替它（或者说模拟 它），但是这样会让我们的编程过程变得很麻烦。</p><p>在我看的大佬上面，最精巧的无穷大常量取值是 0x3f3f3f3f，我不知道是谁最先开始使用这个精妙的常 量来做无穷大，自己也是学以致用，你还别说发现非常好用，而当我对这个常量做更深入的分析时，就发现它真的是非常精巧了。</p><p> 第一、0x3f3f3f3f的十进制是1061109567，也就是10^9级别的（和 0x7fffffff一个数量级），而一般场合下的数据都是小于10^9的，所以它可以作为无穷大使用而不致出现数据大于无穷大的情形。</p><p>第二、由于一般的数据都不会大于10^9，所以当我们把无穷大加上一个数据时，它并不会溢出（这就满足了“无穷大加一个有穷的数依然是无穷 大”），事实上 0x3f3f3f3f + 0x3f3f3f3f = 2122219134，这非常大但却没有超过32-bit int的表示范围，所以 0x3f3f3f3f 还满足了我们“无穷大加无穷大还是无穷大”的需求。</p><p>第三、0x3f3f3f3f还能给我们带来一个意想不到的额外好处：如果我们想要将某个数组清零，我们通常会使用 <code>memset(a,0,sizeof(a))</code>这样的代码来实现（方便而高效），但是当我们想将某个数组全部赋值为无穷大时（例如解决图论问题时邻接矩阵的 初始化），就不能使用memset函数而得自己写循环了（写这些不重要的代码真的很痛苦），我们知道这是因为 memset 是按字节操作的，它能够对数组清 零是因为0的每个字节都是0，现在好了，如果我们将无穷大设为 0x3f3f3f3f，那么奇迹就发生了，0x3f3f3f3f 的每个字节都是0x3f！所 以要把一段内存全部置为无穷大，我们只需要<code>memset(a,0x3f,sizeof(a))</code>。所以在通常的场合下，0x3f3f3f3f 真的是一个非常棒的选择。</p><p>补充：memset以字节为单位进行填充，可以全部置为0，-1，和某个int类型的值四个字节都是一样的表示的数值，别问问就是巧合！！</p>]]></content>
      
      
      <categories>
          
          <category> 算法总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小tips </tag>
            
            <tag> 常量设定 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tcp详解</title>
      <link href="%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcp%E8%AF%A6%E8%A7%A3/"/>
      <url>%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcp%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、tcp协议的特点"><a href="#1、tcp协议的特点" class="headerlink" title="1、tcp协议的特点"></a>1、tcp协议的特点</h4><p>TCP是在不可靠的IP层之上实现的可靠的数据传输协议，它主要解决传输的可靠、有序、无丢失和不重复问题。TCP 是TCP/IP 体系中非常复杂的一个协议，主要特点如下：</p><ul><li><p>TCP 是面向连接的传输层协议。</p></li><li><p>每条TCP 连接只能有两个端点，每条TCP 连接只能是点对点的（一对一）。</p></li><li><p>TCP 提供可靠的交付服务，保证传送的数据无差错、不丢失、不重复且有序。</p><ul><li><p>如何保证数据无差错、不丢失、不重复且有序的？有哪些机制来保证？</p><p>答：TCP 使用了校验、序号、确认和重传等机制来达到这一目的。</p></li></ul></li><li><p>TCP 提供全双工通信，允许通信双方的应用进程在任何时候都能发送数据，为此TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据。</p><ul><li><p>为什么需要设置缓存，缓存的作用？ </p><p>答：发送缓存用来暂时存放以下数据：1.发送应用程序传送给发送方TCP 准备发送的数据；2.TCP已发送但尚未收到确认的数据。</p><p>接收缓存用来暂时存放以下数据：1.按序到达但尚未被接收应用程序读取的数据；2.不按序到达的数据。</p></li></ul></li><li><p>TCP是面向字节流的，虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅视为一连串的无结构的字节流。</p><ul><li>一个字节占一个序号，每个报文段用第一个字节的序号来标识,例如，一报文段的序号字段值是301, 而携带的数据共有l00B, 表明本报文段的数据的最后一个字节的序号是400, 因此下一个报文段的数据序号应从401开始，也就是期望的下一个序号（确认号）。</li></ul></li></ul><h4 id="2、tcp报文段格式"><a href="#2、tcp报文段格式" class="headerlink" title="2、tcp报文段格式"></a>2、tcp报文段格式</h4><p><img src="https://www.hualigs.cn/image/60a3a18937d44.jpg" alt=""></p><p>部分字段解释：</p><p>1) 序号字段（就是seq）：序号字段的值指的是本报文段所发送的数据的第一个字节的序号。</p><p>2) 确认号字段（就是ack）：是期望收到对方的下一个报文段的数据的第一个字节的序号。若确认号为N, 则表明到序号N-1为止的所有数据都已正确收到。（累积确认）</p><p>3) 确认位ACK：只有当ACK=1时确认号字段才有效。当ACK=0时，确认号无效。TCP 规定，在连接建立后所有传送的报文段都必须把ACK置1。</p><p>4) 同步位SYN。同步SYN=1表示这是一个连接请求或连接接收报文。当SYN= 1, ACK=0 时，表明这是一个连接请求报文，对方若同意建立连接，则在响应报文中使用SYN= 1, ACK=1。即SYN=1表示这是一个连接请求或连接接收报文。</p><p>5) 终止位FIN (Finish) 。用来释放一个连接。FIN=1表明此报文段的发送方的数据已发送完毕了并要求释放传输连接。</p><h4 id="3、tcp连接管理"><a href="#3、tcp连接管理" class="headerlink" title="3、tcp连接管理"></a>3、tcp连接管理</h4><p>TCP 是面向连接的协议，因此每个TCP 连接都有三个阶段：连接建立、数据传送和连接释放。TCP 连接的管理就是使运输连接的建立和释放都能正常进行。</p><p>在TCP 连接建立的过程中，要解决以下三个问题：</p><p>1) 要使每一方都能够确知对方的存在。</p><p>2) 要允许双方协商一些参数（如最大窗口值、是否使用窗口扩大选项、时间戳选项及服务质量等）。</p><p>3) 能够对运输实体资源（如缓存大小、连接表中的项目等）进行分配。</p><p>每条TCP 连接唯一地被通信两端的两个端点（即两个套接字）确定。 端口拼接到IP地址即为套接字，tcp的连接采用的是客户机/服务器方式，主动发起连接建立的应用进程称为客户机，而被动等待连接建立的应用进程称为服务器。</p><h4 id="4、tcp连接的建立"><a href="#4、tcp连接的建立" class="headerlink" title="4、tcp连接的建立"></a>4、tcp连接的建立</h4><p><img src="https://www.hualigs.cn/image/60a3a1bfa6710.jpg" alt=""></p><p>1) 第一次握手：客户机的TCP首先向服务器的TCP发送一个连接请求报文段。这个特殊的报文段中不含应用层数据，其首部中的SYN标志位被置为1。另外，客户机会随机选择一个起始序号seq = x（连接请求报文不携带数据，但要消耗一个序号）。</p><p>2) 第二次握手：服务器的TCP 收到连接请求报文段后，如同意建立连接，就向客户机发回确认，并为该TCP连接分配TCP缓存和变量。在确认报文段中，SYN 和ACK 位都被置为1, 确认号字段的值为x+1,并且服务器随机产生起始序号seq= y( 确认报文不携带数据，但也要消耗一个序号）。确认报文段同样不包含应用层数据。</p><p>3) 第三次握手：当客户机收到确认报文段后，还要向服务器给出确认，并且也要给该连接分配缓存和变量。这个报文段的ACK 标志位被置1, 序号字段为x+1, 确认号字段ack=y+1。该报文段可以携带数据，若不携带数据则不消耗序号 http中的tcp连接的第三次握手的报文段中就捎带了客户对万维网文档的请求 。成功进行以上三步后，就建立了TCP 连接，接下来就可以传送应用层数据。TCP 提供的是全双工通信，因此通信双方的应用进程在任何时候都能发送数据。</p><p><strong>【总结】</strong></p><ul><li>1) SYN = 1,ACK = 0,seq = x;</li><li>2) SYN = 1,ACK = 1,seq = y,ack = x+1;</li><li>3) SYN = 0,ACK = 1,seq = x+1,ack=y+1。</li></ul><h4 id="5、tcp释放连接"><a href="#5、tcp释放连接" class="headerlink" title="5、tcp释放连接"></a>5、tcp释放连接</h4><p><img src="https://www.hualigs.cn/image/60a3a1dfde76f.jpg" alt=""></p><p>1) 第一次握手：客户机打算关闭连接时，向其TCP发送一个连接释放报文段，并停止发送数据，主动关闭TCP 连接，该报文段的FIN 标志位被置1, seq= u, 它等于前面已传送过的数据的最后一个字节的序号加1 (FIN 报文段即使不携带数据，也要消耗一个序号）。TCP是全双工的，即可以想象为一条TCP 连接上有两条数据通路。发送FIN 报文时，发送FIN 的一端不能再发送数据，即关闭了其中一条数据通路，但对方还可以发送数据。</p><p>2) 第二次握手：服务器收到连接释放报文段后即发出确认，确认号是ack = u + 1, 而这个报文段自己的序号是v, 等千它前面已传送过的数据的最后一个字节的序号加1 。此时，从客户机到服务器这个方向的连接就释放了，TCP连接处千半关闭状态。但服务器若发送数据，客户机仍要接收，即从服务器到客户机这个方向的连接并未关闭。</p><p>3) 第三次握手：若服务器已经没有要向客户机发送的数据，就通知TCP释放连接，此时其发出FIN=1的连接释放报文段。</p><p>4) 第四次握手：客户机收到连接释放报文段后，必须发出确认。在确认报文段中，ACK字段被置为1, 确认号ack= w + 1, 序号seq= u + 1 。此时TCP连接还未释放，必须经过时间等待计时器设置的时间2MSL（最长报文段寿命）后，A才进入连接关闭状态。</p><p><strong>【总结】</strong></p><ul><li><p>1) FIN = 1,seq = u;</p></li><li><p>2) ACK = 1,seq = v,ack = u+1;</p></li><li><p>3) FIN = 1,ACK = 1,seq = w,ack =u+1;(确认第一次的u)</p></li><li><p>4) ACK = 1,seq = u+1,ack = w+1。</p></li></ul><p><strong>question one : 什么是SYN洪泛攻击？（三次握手机制有什么问题？）</strong></p><p>答：由于服务器端的资源是在完成第二次握手时分配的，而客户端的资源是在完成第三次握手时分配的，攻击者发送TCP的SYN报文段，SYN是TCP三次握手中的第一个数据包，而当服务器返回ACK后，该攻击者就不对其进行再确认，那这个TCP连接就处于挂起状态，也就是所谓的半连接状态，服务器收不到再确认的话，还会重复发送ACK给攻击者。这样更加会浪费服务器的资源。攻击者就对服务器发送非常大量的这种TCP连接，由于每一个都没法完成三次握手，所以在服务器上，这些TCP连接会因为挂起状态而消耗CPU和内存，最后服务器可能死机，就无法为正常用户提供服务了。</p><p><strong>question two :为什么不采用“两次握手”建立连接呢？</strong></p><p>答：这主要是为了<strong>防止两次握手情况下已失效的连接请求报文段突然又传送到服务器而产生错误</strong>。考虑下面这种情况。客户A 向服务器B 发出TCP 连接请求，第一个连接请求报文在网络的某个结点长时间滞留， A 超时后认为报文丢失，于是再重传一次连接请求， B 收到后建立连接。数据传输完毕后双方断开连接。而此时，前一个滞留在网络中的连接请求到达服务器B, 而B 认为A又发来连接请求，此时若使用“三次握手”，则B 向A 返回确认报文段，由于是一个失效的请求，因此A 不予理睬，建立连接失败。若采用的是“两次握手”，则这种情况下B 认为传输连接已经建立，并一直等待A 传输数据，而A 此时并无连接请求，因此不予理睬，这样就造成了B的资源白白浪费。</p><p><strong>question three :如果已经建立了连接，但是客户端突然出现故障了怎么办?</strong></p><p>答：TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。</p><p><strong>question four :为什么连接的时候是三次握手，关闭的时候却是四次握手?</strong></p><p>答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次握手。</p><p><strong>question four :为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？</strong></p><p>答：1)虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL(Maximum SegmentLifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。2)防止出现“已失效的连接请求报文段“（和上面的为啥不用二次握手类似）。A 在发送最后一个确认报文段后，再经过2MSL可保证本连接持续的时间内所产生的所有报文段从网络中消失.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tcp协议 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="all/hello-world/"/>
      <url>all/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
