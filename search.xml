<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SETR:Rethinking_Semantic_Segmentation_from_a_Sequence-to-Sequence_Perspective_with_Transformers</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-CVPR/SETR-Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers"><a href="#Rethinking-Semantic-Segmentation-from-a-Sequence-to-Sequence-Perspective-with-Transformers" class="headerlink" title="Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with  Transformers"></a>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with  Transformers</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/fudan-zvg/SETR">https://github.com/fudan-zvg/SETR</a></li><li>论文：<a href="https://arxiv.org/abs/2012.15840">https://arxiv.org/abs/2012.15840</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>最新的语义分割方法采用具有编码器-解码器体系结构的全卷积网络（FCN）。编码器逐渐降低空间分辨率，并通过更大的感受野学习更多的抽象/语义视觉概念。由于上下文建模对于分割至关重要，因此最新的工作集中在通过扩张/空洞卷积或插入注意力模块来增加感受野。但是，基于编码器-解码器的FCN体系结构保持不变。</p><p>在本文中，我们旨在通过将语义分割视为序列到序列的预测任务来提供替代视角。具体来说，我们部署一个纯 transformer（即，不进行卷积和分辨率降低）将图像编码为一系列patch。通过在 transformer的每一层中建模全局上下文，此编码器可以与简单的解码器组合以提供功能强大的分割模型，称为SEgmentation TRansformer（SETR）。</p><p><strong>一个标准的FCN分割模型有一个编码器-解码器结构：编码器用于特征表示学习，而解码器用于编码器产生的特征表示的像素级分</strong>类。编码器由堆叠的卷积层组成，特征图的分辨率逐渐降低，编码器能够以逐渐增加的感受野学习更多的抽象/语义视觉概念。<br><strong>优点</strong>：translation equivariance：尊重了成像过程的本质，支持了模型对看不见的图像数据的泛化能力<br><strong>局部性</strong>：通过跨空间共享参数来控制模型的复杂性。<br><strong>缺点</strong>：感受野有限，难以学习无约束场景图像中的语义分割的长期依赖信息。<br><strong>解决方法：</strong><br>直接操作卷积运算：大内核尺寸（large kernel sizes），atrous卷积和图像/特征金字塔。<br>注意力模块集成到FCN架构中：对特征图中所有像素的全局交互进行建模。当应用于语义分割时，通常是是将注意力模块与位于顶部的注意力层结合到FCN架构中。不改变FCN模型结构的本质：<strong>编码器下采样输入的空间分辨率，用于区分语义类别的低分辨率特征映射；解码器将特征表示上采样为全分辨率分割映射</strong>。<br>本文中，我们用纯transformer 取代空间分辨率逐渐降低的基于堆叠卷积层的编码器，这种编码器将输入图像视为由学习到的面片嵌入表示的图像面片序列，并使用全局自关注建模对该序列进行转换，以进行有区别的特征表示学习。</p><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><ul><li><p>问题</p><p>典型的语义分割Encoder-Decoder结构以多次下采样损失空间分辨率为代价来抽取局部/全局特征。网络Layer一旦固定,每一层的感受野是受限的,因此要获得更大范围的语义信息,理论上需要更大的感受野即更深的网络结构。</p><p>如何既能够抽取<strong>全局的语义信息,</strong>又能尽量<strong>不损失分辨率,</strong>一直是语义分割的<strong>难点</strong></p></li><li><p>解决方法</p><ul><li>用常用于NLP领域的transformer作为Encoder来抽取全局的语义信息(整个过程不损失image分辨率),代替传统FCN的编码部分,从序列-序列学习的角度,为语义分割问题提供了一种新的视角；</li><li>将图像序列化处理,利用Transformer框架、完全用注意力机制来实现Encoder的功能；</li><li>提出三种复杂度不同的Decoder结构</li></ul></li><li><p>整体网络架构</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/2.jpg" alt=""></p></li><li><p><strong>part 1：图像序列化处理</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/5.jpg" alt=""></p></li></ul><p>image to sequenc，因为NLP中transformation的输入是一维序列,所以需要把图像(H <em> W </em> C)转换成 1D序列。<br>1）： 按pixel-wise进行flatten。考虑到计算量问题所以此方法不通。<br>2）： 按patch-wise进行flatten。本文采用此方法。</p><p>1、输入图像的大小 H <em> W </em> 3（256 <em> 256 </em> 3）的大小，patch_size = 16 <em> 16，因此图像序列化为 256/16 </em> 256 /16 = 256个 （16 <em> 16 </em> 3）的图片大小</p><p>2、向量化后的patch<code>p_i</code>经过<code>Linear Projection</code>function得到向量<code>e_i</code> ，旁边注释<code>e_i</code>是patch embedding,<code>p_i</code>是position embedding。</p><ul><li><strong>part 2：Transformer</strong></li></ul><p>这边采用的是纯 Transformer 的encoder结构，只不过中间重复叠用了24次，具体的使用可以查看 PIT， PVT，Swin Transformer 的总结文档。</p><ul><li><p><strong>part3 Decoder</strong></p><p>本文就提出了三种不一样的 decoder 的设计，分别如下</p><ul><li><p>Navite Upsampling（Naive）</p><p>2-layer：（1 <em> 1）conv  +  sync batch norm（w/ReLU）+ （1 </em> 1）conv</p><p>将Transformer输出的特征维度降到分类类别数后经过双线性上采样恢复原分辨率</p></li><li><p>Progressive UPsampling (PUP）</p><p>交替使用卷积层和两倍上采样操作，为了从<code>H/16 × W/16 × 1024</code> 恢复到<code>H × W × 19</code>(19是cityscape的类别数) 需要4次操作,以恢复到原分辨率。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/6.jpg" alt=""></p></li><li><p>Multi-Level feature Aggregation (MLA)</p><p>首先将Transformer的输出<code>&#123;Z1,Z2,Z3…ZLe&#125;</code>均匀分成M等份,每份取一个特征向量。如下图,24个transformer的输出均分成4份,每份取最后一个,即<code>&#123;Z6,Z12,Z18,Z24&#125;</code> .后面的Decoder只处理这些取出的向量。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/7.jpg" alt=""></p></li></ul><p>具体步骤：</p><p>1.2D —&gt; 3D。将<code>ZL</code> 从2D <code>(H × W)/256 × C</code>恢复到3D <code>H/16 × W/16 × C</code></p><p>2.经过3-layer的卷积<code>1 × 1, 3 × 3, and 3 × 3</code></p><p>3.双线性上采样<code>4×</code></p><p>4.自上而下的融合。以增强<code>Zl</code> 之间的相互联系,如上图最后一个<code>Zl</code>理论上拥有全部上面三个feature的信息,融合即cat</p><p>5.<code>3 × 3</code></p><p>6.双线性插值<code>4×</code> 恢复至原分辨率。</p></li><li><p><strong>损失函数设计</strong></p><p>totalloss = auxiliary loss + main loss</p><p>其中main loss为<code>CrossEntropyLoss</code> ,auxiliary loss在17年CVPR有提及</p></li></ul><h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h3><ul><li>We reformulate the image semantic segmentation problem from a sequence-to-sequence learning perspective, offering an alternative to the dominating encoder-decoder FCN model design.</li><li>As an instantiation, we exploit the transformer framework to implement our fully attentive feature representation encoder by sequentializing images.</li><li>To extensively examine the self-attentive feature presentations, we further introduce three different decoder designs with varying complexities. Extensive experiments show that our SETR models can learn superior feature representations as compared to different FCNs with and without attention modules, yielding new state of the art on ADE20K (50.28%), Pascal Context (55.83%) and competitive results on Cityscapes. Particularly, our entry is ranked the 1stplace in the highly competitive ADE20K test server leaderboard.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>在Cityscapes/ADE20K/PASCAL Context三个数据集上进行了实验。实验结果优于用传统FCN(with &amp; without attention module)抽特征的方法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/4.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211203/3.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><ul><li>卷积操作的感受野有限是传统FCN体系结构的一个内在限制。为突破该限制，逐渐提出两类方法<ul><li>改变卷积：包括增大卷积核kernel_size、Non-local(跟本文有点像，每次抽取的都是全局特征)和特征金字塔。例如DeepLab引入空洞卷积/SPP/ASPP。</li><li>将注意力模块集成到FCN体系结构中：一次对所有像素的全局信息抽取特征。例如PSANet提出点向空间注意模块、DANet嵌入channel attention和spatial attention。</li></ul></li><li>有人提到，本文是把ViT模型原封不动迁移过来了，替换了encoder，虽带来了精度的提升但模型的计算量和参数量都非常大。</li></ul><blockquote><p>ViT(Vision Transformer)首次证明了纯基于transformer的图像分类模型可以达到sota。</p></blockquote><ul><li>CNN是通过不断地堆积卷积层来完成对图像从局部信息到全局信息的提取,不断堆积的卷积层慢慢地扩大了感受野直至覆盖整个图像;但是transformer并不假定从局部信息开始,而且一开始就可以拿到全局信息,学习难度更大一些,但transformer学习长依赖的能力更强。</li><li>CNN结构更适合底层特征,Transformer更匹配高层语义。二者无绝对差别,就是看问题的尺度差异,本质都是消息传递。</li><li>现在对transformer理解还不充分,为啥Transformer之后没有改变分辨率,还要用Decoder来恢复原image的分辨率(得看看transformer那篇论文,或者评论区有好心人解答一下嘛)。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(CVPR) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(CVPR) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransFuse:Fusing_Transformers_and_CNNs_for_Medical_Image_Segmentation</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers/TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation"><a href="#TransFuse-Fusing-Transformers-and-CNNs-for-Medical-Image-Segmentation" class="headerlink" title="TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation"></a>TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/Rayicer/TransFuse">https://github.com/Rayicer/TransFuse</a></li><li>论文：<a href="https://arxiv.org/abs/2102.08005">https://arxiv.org/abs/2102.08005</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>传统CNN网络很难捕获长距离的依赖关系，而且一味的加深网络的深度会带来大量的计算冗余。</p><p>文章提出了一种并行分支的TransFuse网络，<strong>结合transformer和CNN两种网络架构，能同时捕获全局依赖关系和低水平的空间细节</strong>，文中还提出了一种BiFusion module用来混合两个分支所提取的图像特征。使用TransFuse，可以以较浅的方式有效地捕获全局依赖性和low-level空间细节</p><ul><li>transformer：good at modeling global context，<strong>but</strong> lack of spatial inductive-bias in modelling local information and limitations in capturing fine-grained details</li><li>CNN：low-level spatial details can be efficiently captured <strong>but</strong> lack of efficiency in capturing global context information</li></ul><p>TransFuse在多个医学分割任务中达到SOTA，并在降低参数和提高推理速度方面得到很大的提升。</p><ul><li><strong>advantage：</strong>firstly, by leveraging the merits of CNNs and Transformers, we argue that  TransFuse can capture global information without building very deep nets while  preserving sensitivity on low-level context; secondly, our proposed BiFusion  module may simultaneously exploit different characteristics of CNNs and  Transformers during feature extraction, thus making the fused representation  powerful and compact.</li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><ul><li><p><strong>整体网络架构</strong>，TransFuse包含两个分支，左边是transformer分支，右边是CNN分支，模型通过BiFusion层整合两个分支的特征，然偶经过上采样和attention-gated skip-connection输出分割结果</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/2.jpg" alt=""></p></li><li><p><strong>Transformer Branch</strong><br>Transformer Branch是一个完整的编解码结构，编码器部分使用的是transformer结构，解码器部分使用的是SERT中提到的渐进上采样（PUP）结构。</p></li><li><p><strong>CNN Branch</strong><br>CNN Branch使用ResNet的第四层，第三层和第二层的输出作为这一分支的输出，由于transformer可以捕获全局的上下文信息，故而CNN Branch并不需要设计的很深 。</p></li><li><p><strong>BiFusion Module</strong><br>BiFusion Module主要由通道注意力和空间注意力组成，对Transformer Branch做通道注意力，对CNN Branch做空间注意力。然后经过卷积，相乘，拼接，残差操作，实现两个分支的特征融合。</p><ul><li><p>通道注意力</p><p>特征的每一个通道都代表着一个专门的检测器，因此，通道注意力是关注什么样的特征是有意义的。为了汇总空间特征，作者采用了全局平均池化和最大池化两种方式来分别利用不同的信息。<strong>通道注意力聚焦在“什么”是有意义的输入图像</strong></p></li><li><p>空间注意力</p><p>空间注意力模块来关注哪里的特征是有意义的，<strong>空间注意力聚焦在“哪里”是最具信息量的部分</strong>，这是对通道注意力的补充。为了计算空间注意力，沿着通道轴应用平均池化和最大池操作，然后将它们连接起来生成一个有效的特征描述符</p></li><li><p>通道注意力和空间注意力详细请看 代表性论文 <strong>CBAM: Convolutional Block Attention Module</strong></p></li></ul></li><li><p>最后通过上采样和attention-gated skip-connection输出分割结果。</p></li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li><p>本文采用四个数据集进行验证，分别是，Polyp Segmentation，Skin Lesion Segmentation， Hip Segmentation，Prostate Segmentation</p></li><li><p>定量结果</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/4.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/5.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211212/3.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>文章使用transformer结构捕捉图像的全局上下文信息，并利用这一优点减小CNN结构的层数，只是用很少的卷积层提取局部空间信息作为transformer的补充，并通过BiFusion进行特征融合，最后通过Attention-gate，上采样输出分割结果。文中一共出现四种注意力机制。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>simpler-is-better:Few-shot_Semantic_Segmentation_with_Classifier_Weight_Transformer</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer"><a href="#Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer" class="headerlink" title="Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer"></a>Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/1.jpg" alt=""></p><ul><li>代码：<a href="https://github.com/zhiheLu/CWT-for-FSS">https://github.com/zhiheLu/CWT-for-FSS</a></li><li>论文：<a href="https://arxiv.org/abs/2108.03032">https://arxiv.org/abs/2108.03032</a></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>得益于大型的标签数据和深度学习算法的发展，语义分割方法近几年取得了很大的进步。但这些方法有两个局限：</p><p><strong>1）过度依赖带标签数据，而这些数据的获得通常消耗大量人力物力；</strong></p><p><strong>2）训练好的模型并不能处理训练过程中未见的新类别。</strong></p><p>面对这些局限，小样本语义分割被提出来，它的目的是通过对少量样本的学习来分割新类别。一般来说，小样本语义分割方法是通过用训练数据模拟测试环境进行元学习使得训练的模型有很好的泛化能力，从而在测试时可以仅仅利用几个样本的信息来迭代模型完成对新类别的分割。具体地，小样本分割模型是在大量的模拟任务上进行训练，每个模拟任务有两个数据组：Support set and Query set。Support set 是有标签的K-shot样本，而Query set只在训练的时候有标签。这样的模拟任务可以有效地模拟测试环境</p><ul><li>针对元学习的概念，可以参考：<a href="https://zhuanlan.zhihu.com/p/289043310">https://zhuanlan.zhihu.com/p/289043310</a></li><li>针对小样本学习的一些概念，可以参考：<a href="https://zhuanlan.zhihu.com/p/84290146">https://zhuanlan.zhihu.com/p/84290146</a></li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><p>一个<strong>小样本分类系统</strong>一般由三部分构成：<strong>编码器，解码器和分类器</strong>。其中，前两个模块模型比较复杂，最后一个分类器结构简单。我们发现现存的小样本分类方法通常在<strong>元学习</strong>的过程中更新所有模块或者除编码器外的模块，而所利用更新模块的数据仅仅有几个样本。在这样的情况下，我们认为模型更新的参数量相比于数据提供的信息量过多，从而不足以优化模型参数。基于此分析，我们提出了一个全新的元学习训练范式，即只对分类器进行元学习。为了方便读者更好的理解，论文给出了两种方法的对比，如下图，图(a)为传统方法，要训练三个模块，图(b)为本文方法，编码器解码器在经过大量有标记数据训练后便冻结，只调整分类器权重</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/2.jpg" alt=""></p><p>具体地，我们采用常规的分割方法对编码器和解码器进行训练，训练后在元学习的过程中不在更新。这是基于我们的假设：在大量标签数据训练下的模型可以提取有辨别性的特征，对一些新类别也有效，这也可以解释很多方法直接使用ImageNet预训练的模型进行特征提取。在分析数据的时候，我们发现Support set和Query set的数据有时有较大的类内差异，如下图。同样的类别，不同的角度即可产生很大的区别。这就使得利用Support set迭代的模型不能很好地作用在Query set上。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/3.jpg" alt=""></p><p>为了解决这个问题，我们提出了<strong>Classifier Weight Transformer</strong>来动态地利用Query set的特征信息来进一步更新分类器模块，从而提升分割任务性能。一个少镜头分割模型通常由三个模块组成：编码器、解码器和分类器。为了学习适应新类别，现有方法通常在编码器在 ImageNet 上进行预训练后对整个模型进行元学习 [23]。在情节训练阶段，模型的所有三个部分都是元学习的。训练后，给定一个带有带注释的支持集图像和用于测试的查询图像的新类，该模型有望使所有三个部分都适应新类。由于只有很少的带注释的支持集图像和复杂且相互关联的三部分，这种适应通常是次优的。为了克服这些限制，我们提出了一个分两个阶段的简单而有效的训练范式。在第一阶段，我们通过监督学习对编码器和解码器进行预训练，以获得更强的特征表示。在第二阶段，连同冻结的编码器和解码器，我们仅对分类器进行元训练。这是因为我们认为预训练的特征表示部分（即编码器和解码器）足以泛化到任何看不见的类别；因此，少样本语义分割的关键在于调整二元分类器（分离前景和背景像素），而不是从少样本样本中调整整个模型。我们方法的概述如下图所示。具体的算法参考原文。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/4.jpg" alt=""></p><p>者假设，一个学习了大量图片和信息的传统分割网络已经能够从任何一张图片中捕捉到充分的，有利于区分背景和前景的信息，无论训练时是否遇到了同类的图。那么面对少样本的新类时，只要对分类器进行元学习即可。 对于分类器的学习，作者提出了一种分类器权重转移方法CWT，根据每一张查询集图象，临时调整分类器参数。借助Transformer的思想，作者将分类器权重转化为Query，将图象提取出来的特征转化为Key和Value，然后根据这三个值调整分类器权重，最后通过残差连接，与原分类器参数求和     </p><p>本文提出的元学习名为episodic training。一般来说，本文提出的元学习针对是小样本的类进行学习。元学习分两步</p><ul><li>第一步是内循环，和预训练一样，根据支持集上的图片和mask进行训练，不过只修改分类器参数。文中指出，当新类样本数够大时，只使用外循环，即只更新分类器，就能匹敌SOTA，但是当面对小样本时，表现就不尽如人意。</li><li>第二步是外循环，根据每一副查询图片，微调分类器参数，而且微调后的参数只针对这一张查询图片，不能用于其他查询图象，也不覆盖修改原分类器参数。</li></ul><h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h3><ul><li>We propose a novel model training paradigm for few-shot semantic  segmentation. Instead of meta-learning the whole, complex segmentation model, we  focus on the simplest classifier part to make new-class adaptation more  tractable. </li><li>We introduce a novel meta-learning algorithm that leverages a  Classifier Weight Transformer (CWT) for adapting dynamically the classifier  weights to every query sample.</li><li>Extensive experiments with two popular  backbones (ResNet-50 and ResNet-101) show that the proposed methodyieldsa  newstate-of-the-artperformance, often surpassing existing alternatives,  especially on 5-shot case, by a large margin. Further, under a more challenging  yet practical cross-domain setting, the margin becomes even bigger.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>在两个标准小样本分割数据集PASCAL和COCO上，我们的方法在大多数情况下取得了最优的结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/5.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/6.jpg" alt=""></p><ul><li>跨数据集的情景下测试了我们模型的性能，可以看出我们的方法展示了很好的鲁棒性</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/7.jpg" alt=""></p><ul><li>可视化结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211108/7.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>提出了一种新的元学习训练范式来解决小样本语义分割问题。相比于现有的方法，这种方法更加简洁有效，只对分类器进行元学习。为了解决类内差异问题，我们提出<strong>Classifier Weight Transformer</strong>来利用Query特征信息来迭代分类器，从而获得更加鲁棒的分割效果。通过大量的实验，我们证明了方法的有效性。针对我们现在的创新点，找到存在的问题，采取简便的方法提升性能未必不是一个新的思路，就拿现在这个而言，并没有对编码器-解码-分类器三个进行元学习，而是只针对其中最简单的分类器进行创新，也采纳了transformer，也算是transformer的一个应用把。果然现在在cv领域transformer大放异彩，针对这种思想，之前也看过类似输入网络的图像大小对性能的影响，也看过动态预测调整输入图片大小的预测，也是一种思路，所以咋创新方面不用单单停留在网络的结构上，而是应该放在整体的架构的，因为任何一处的改变都可能对最后的分割性能产生不一样的影响。道阻且长，望君继续努力。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(ICCV) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(ICCV) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SOTR-Segmenting-Objects-with-Transformers</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/papers-ICCV/SOTR-Segmenting-Objects-with-Transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="SOTR-Segmenting-Objects-with-Transformers"><a href="#SOTR-Segmenting-Objects-with-Transformers" class="headerlink" title="SOTR: Segmenting Objects with Transformers"></a>SOTR: Segmenting Objects with Transformers</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/first.png" alt=""></p><ul><li><p>代码：<a href="https://github.com/easton-cau/SOTR">https://github.com/easton-cau/SOTR</a></p></li><li><p>论文：<a href="https://arxiv.org/abs/2108.06747">https://arxiv.org/abs/2108.06747</a> </p></li></ul><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><ul><li><p>首先作者研究了实例分割的发展历程，以及各个阶段方法的大概总结，提出个各个阶段的优点与不足的，在实例分割这块，主要的方法就是包括</p><p>Top-down instance segmentation 和 Bottom-up instance segmentation</p><ul><li><strong>Top-down instance segmentation</strong><ul><li><strong>proposal-based</strong>方法：基于目标检测，在得到目标检测框之后再在框内做语义分割分割前景背景，由于这种方法需要借助目标检测中的区域提议，因此该方法称为<strong>proposal-based</strong>方法.，这种方法就是遵循先检测后分割的范式的。缺点如下 例如 Mak-RCNN<ul><li>1）由于有限的感受野，CNN在高级视觉语义信息中相对缺乏 特征的连贯性来关联实例 , 导致对大对象的次优结果；</li><li>2）分割质量和推理速度都严重依赖对象检测 器，在复杂场景中性能较差。</li></ul></li></ul></li><li><p><strong>Bottom-up instance segmentation</strong></p><ul><li><strong>proposal-free方法</strong>：在语义分割图的基础上，将像素聚集到不同的实例上。这类方法的基本思想是利用CNN学到每个像素实例级的特征，接着用一种聚合方法将像素聚合成实例。这种方法通常分两步，一个是分割，一个是聚合。语义分割图获得之后，将像素一步步的聚合到不同的实例中。学习每像素嵌入(per-pixel embedding)和实例感知 特征(instance-aware features)，然后使用后处理技术依次分组，根据嵌入特征(embedding characteristics)将它们转换为实例，只要缺点如下<ul><li>不稳定的聚类（例如碎片化和联合掩码）以及对不同场景数据集的泛化能力差，当场景非常复杂并且一张图像中存在密集的物体时，背景像素上不可避免地 会损失大量的计算和时间</li></ul></li></ul></li><li><p><strong>STOR</strong>：就是一种 (Bottom-up) 自底向上的model模型，也就是基于<strong>proposal-free方法</strong>，有效地学习了位置敏感特征(position-sensitive features )，动态生成实例掩码 (dynamically generates instance masks )，无需后处理分组以及边界框位置和尺度的界限，<strong>SOTR</strong> 以图像为输入，结合 CNN 和 Transformer 模块来 提取特征，并直接对类概率和实例掩码进行预测。</p></li></ul></li><li><p><strong>Transformer</strong></p><ul><li><p>近几年来，由于 Transformer 的崛起，其在cv领域的重要性可想而知，在cv领域上，很多人试图完全 替代 卷积运算 或 将类 CNN 架构与transformer结合用于视觉任务中的特征提取，它可以轻松捕 获全局范围特征(global-range characteristics)并自然地对长距离语义依赖项进行建模</p><ul><li>self-attention，作为transformers 的关键机制，广泛地聚 合了来自整个输入域的特征和位置信息。因此基于transformer的模型可以更好地区分具有相同语义类 别的重叠实例，这使得它们比CNN更适合高级视觉任务。但是他也有不足：<ul><li>（1）典型的attention在提取<strong>(low-level features)</strong>方低级特征方面表现不佳，导致对小对象的错误预测</li><li>（2）由于广泛的特征图(feature map)，需要大量的内存和时间，尤其是在训练阶段</li><li><strong>（1）的问题可以通过结合 CNN 主干得到有效解决</strong></li></ul></li></ul></li><li><p>为了降低传统 self-attention 的内存和计算复杂度，我们提出了 <strong>twin attention</strong>（双注意力），一种替代的自注意 力自回归块，通过将全局空间 attention (注意力) 分解为独立的垂直和水平 attention (注意力) 来显着 减少计算和内存。与原始 Transformer 相比，这种精心设计的架构在计算和内存方面具有显着的节省，尤其是在用于密集预测（如实例分割）的大输入上。</p></li></ul></li><li><p>我们提出了一种创新的自底向上模型(bottom-up model)，称为 <strong>SOTR</strong>，巧妙地结 合了 CNN 和 Transformer 的优点。具体地说，我们采用Transformer模 型来获 取全局依赖关系并提取高级特征(<strong>high-level features</strong>) 以用于后续的函数头部（<strong>Functional heads</strong>）的预 测。其简化了分割管道，建 立在附加了两个并行子任务的替代 CNN 主干上：（1）通过transformer预测每个实例的类别和（2） 动态生成具有多个的(segmentation mask)分割掩码级上采样模块。 SOTR 可以分别通过特征金字塔 网络 (FPN) 和<strong>twin Transformer</strong>有效地提取较低级别的特征表示并捕获远程上下文依赖关系</p></li></ul><h3 id="Methods-and-Creativity"><a href="#Methods-and-Creativity" class="headerlink" title="Methods and Creativity"></a>Methods and Creativity</h3><p><strong>SOTR 是一种 CNN-transformer 混合实例分割模型</strong>，可以同时学习 2D 平面信息表示并轻松捕获远程信息。 它遵循直接分割范式，首先将输入特征图划分为     <strong>patches</strong> （补丁），然后在动态分割每个实例的同时预测每个<strong>patches</strong> （补丁）的类别。 具体来说，我们的模型主要由三部分组成：1）从输入图像中提取图像特征的主干，尤其是低级和局部特征，2）一个用于建模全局和语义依赖关系的 <strong>Transformer</strong>  ，它附加了功能头以 分别预测每个<strong>patches</strong> （补丁）的类别和卷积核，以及 3) 一个多级上采样模块，通过在生成特征图和相应的卷积核之间执行动态卷积操作来生成最终的分割掩码(segmentation mask)。 总体框架如<strong>图 2</strong> 所示。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-2.jpg" alt=""></p><p>SOTR 可以分别通过特征金字塔网络 (FPN) 和双Transformer有效地提取较低级别的特征表示并捕获远程上下文依赖关系。同时，与原始Transformer相比，所提出的双Transformer具有时间和资源效率，因为只涉及一行和一列注意力来编码像素。</p><ul><li><strong>twin attention（双注意力）</strong>机制，用稀疏表示来简化 <strong>attention(注意力)</strong> 矩阵。 我们的策略主要将感受野限制为固定步幅的设计块模式。 它首先计算每列中的 <strong>attention(注意力)</strong>，同时保持不同列中的元素独立。 该策略可以在水平尺度上聚合元素之间的上下文信息（见图 3（1））。 然后，在每一行内执行类似的 <strong>attention(注意力) </strong>以充分利用垂直尺度上的特征交互（如图 3（2）所示）。 两个尺度中的注意力依次连接为最后一个尺度，具有全局感受野，涵盖了两个维度的信息</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/figure-3.png" alt=""></p><ul><li><strong>作者称之为 Twins</strong>，代码写的很复杂，但其实就是提前将行维度和列维度分别整合到前一个维度再输入 Attention 模块。这样做可以将时间复杂度降低为：</li></ul><script type="math/tex; mode=display">O((H \times W)^2) \quad to  \quad O(H \times W^2 + W \times H^2)^1</script><ul><li><strong>Functional heads</strong></li></ul><p>来自 <strong>Transformer</strong> 模块的特征图（feature maps）被输入到不同的<strong>函数头（functional heads）</strong>以进行后续预测。 <strong>类头(class head)</strong> 包括 单个线性层(linear)以输出 $N ×N ×M $ 的分类结果，<strong>其中 M 是类的数量</strong>。 由于每个patch(补丁) 只为一个中心落入patch(补丁) 的单个对象分配一个类别，如 YOLO [32]，我们利用多级预测并在不同特征级别共享头部，以进一步提高不同尺度对象的模型性能和效率 . <strong>核头(kernel head)</strong> 也由一个线性层(linear)组成，与 <strong>类头(class head)</strong>并行输出一个 $N×N×D$ 张量用于后续的 掩码(mask) 生成，其中张量表示具有D个参数的 $ N×N $ 卷积核。 在训练期间，<strong>Focal Loss [26]</strong> 被应用于分类，而这些卷积核的所有监督都来自最终的掩码(mask) 损失。</p><ul><li><strong>Mask</strong></li></ul><p>为了构建实例感知和位置敏感分割的掩码特征表示，一种直接的方法是对不同尺度的每个特征图进行预测（[36, 12] 等）。 但是，它会增加时间和资源。 受 <strong>Panoptic FPN [22]</strong> 的启发，我们设计了<strong>多级上采样模块(multi-level upsampling module)</strong>，将来自每个 <strong>FPN 级</strong>和 <strong>transformer</strong> 的特征合并为一个统一的掩码特征。 首先，从 <strong>transformer</strong>模块获得带有位置信息的相对低分辨率特征图P5，并结合<strong>FPN中的P2-P4</strong>执行融合。 对于每个尺度的特征图，操作了 3×3 Conv、Group Norm [39] 和 ReLU 的几个阶段。 然后P3-P5被双线性上采样 2×、4×、8×，分别为  $(\frac{H}{4},\frac{W}{4})$分辨率。 最后，将处理后的 P2-P5 加在一起后，执行逐点卷积和上采样以创建最终统一的 $ H×W$ 特征图。</p><p>例如掩码(mask)预测，SOTR 通过对上述统一特征图执行动态卷积运算为每个 patch(补丁) 生成掩码(mask)。 给定来自内核头部(kernel head)的预测卷积核  $K \quad \epsilon \quad K^{N \times N \times D} $  ，每个内核负责相应 patch(补丁)中实例的掩码(mask)生成。 具体操作可以表示如下：</p><script type="math/tex; mode=display">Z^{H \times W \times N^2} = F^{H \times W \times C} * K^{N \times N  \times D}</script><p>其中 <strong>*</strong> 表示卷积操作，Z是最终生成的掩码，维度为 $ H×W×N^2 $。 需要注意的是，D 的取值取决于卷积核的形状，即D等于$λ^2C$，其中 <strong>λ</strong> 为核大小。 最终的实例分割掩码可以由 <strong>Matrix NMS [37]</strong> 生成，每个掩码都由 <strong>Dice Loss [30]</strong> 独立监督。，</p><ul><li><p>代码的具体实现可以查看github上提供的源代码。参考知乎老哥的讲解：</p><p><a href="https://zhuanlan.zhihu.com/p/424036708">https://zhuanlan.zhihu.com/p/424036708</a> </p></li></ul><h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul><li>We introduce an innovative CNN-transformer-hybrid instance segmentation  framework, termed SOTR. It can effectively model local connectivity and  longrange dependencies leveraging CNN backbone and transformer encoder in the  input domain to make them highly expressive. What’s more, SOTR considerably  streamlines the overall pipeline by directly segmenting object instances without  relying on box detection. </li><li>We devise the twin attention, a new  position-sensitive self-attention mechanism, which is tailored for our  transformer. This well-designed architecture enjoys a significant saving in  computation and memory compared with original transformer, especially on large  inputs for a dense prediction like instance segmentation. </li><li>Apart from pure  transformer based models, the proposed SOTR does not need to be pre-trained on  large datasets to generalize inductive biases well. Thus, SOTR is easier applied  to insufficient amounts of data. </li><li>The performance of SOTR achieves 40.2% of AP  with the ResNet-101-FPN backbone on the MS COCO benchmark, outperforming most of  state-of-the-art approaches in accuracy. Furthermore, SOTR demonstrates  significantly better performance on medium (59.0%) and large objects (73.0%),  thanks to the extraction of global information by twin transformer.</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li>定量的结果：SOTR 在 MS COCO 数据集上表现良好，并超越了最先进的实例分割方法。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-7.jpg" alt=""></p><ul><li>定性结果：比起其他的分割方法，SOTR具有较好的分割性能</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20211027/table-4.jpg" alt=""></p><ul><li>还有较多的 Ablation experiments，以验证参数选择的有效性。具体的可以查看论文详细内容</li></ul><h3 id="Rethingking"><a href="#Rethingking" class="headerlink" title="Rethingking"></a>Rethingking</h3><p>rethingking的话，我觉得，现在transformer那么火爆的时刻，本文其实相对简单，但是其的创新点很突出，很新颖。面对某些具体的任务，了解现在主流的方法并且找出他们各自的优点与不足，然后主要针对这些不足提出新的解决方案。在transformer广泛应用于cv领域后，找到transformer在cv领域后的不足之处，就比如提取low-level farture map特征和计算量内存上面存在不足，所以本文也是比较针对这个方面进行了研究，所以最后才可以提出一种创新的分割框架SOTR</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> papers(ICCV) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> Semantic Segmentation(ICCV) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析UNet</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B5%85%E6%9E%90UNet/</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>研究一个深度学习算法，可以先看网络结构，看懂网络结构后，再Loss计算方法、训练方法等。本文主要针对UNet的网络结构进行讲解</p><p>卷积神经网络被大规模的应用在分类任务中，输出的结果是整个图像的类标签。但是UNet是像素级分类，输出的则是每个像素点的类别，且不同类别的像素会显示不同颜色，UNet常常用在生物医学图像上，而该任务中图片数据往往较少。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。</p><ul><li><strong>优点</strong><ul><li>输出结果可以定位出目标类别的位置；</li><li>由于输入的训练数据是patches，这样就相当于进行了数据增强，从而解决了生物医学图像数量少的问题，数据增强有利于模型的训练</li></ul></li><li><strong>缺点</strong><ul><li>训练过程较慢，网络必须训练每个patches，由于每个patches具有较多的重叠部分，这样持续训练patches，就会导致相当多的图片特征被多次训练，造成资源的浪费，导致训练时间加长且效率会低下。但是也会认为网络对这个特征进行多次训练，会对这个特征影响十分深刻，从而准确率得到改进。但是这里你拿一张图片复制100次去训练，很可能会出现过拟合的现象，对于这张图片确实十分敏感，但是拿另外一张图片来就可能识别不出了啦</li><li>定位准确性和获取上下文信息不可兼得，大的patches需要更多的max-pooling，这样会减少定位准确性，因为最大池化会丢失目标像素和周围像素之间的空间关系，而小patches只能看到很小的局部信息，包含的背景信息不够。</li></ul></li></ul><h4 id="网络结构原理"><a href="#网络结构原理" class="headerlink" title="网络结构原理"></a>网络结构原理</h4><p>UNet网络结构，最主要的两个特点是：U型网络结构和Skip Connection跳层连接。</p><p>UNet网络结构分为三个部分，原理图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/1.jpg" alt=""></p><ul><li><p>第一部分是<strong>主干特征提取部分</strong>，我们可以利用<strong>主干部分</strong>获得一个又一个的<strong>特征层</strong>，Unet的主干特征提取部分与VGG相似，为卷积和最大池化的堆叠。<strong>利用主干特征提取部分我们可以获得五个初步有效特征层</strong>，在第二步中，我们会利用这五个有效特征层可以进行特征融合。</p><ul><li><p>下采样</p></li><li><p>左边特征提取网络：使用conv和pooling，就是每次向下采样之前都会进行两次的卷积操作，然后向下采样，然后再进行两次卷积操作，以此往复，向下连续采样五次</p></li></ul></li><li><p>第二部分是<strong>加强特征提取部分</strong>，我们可以利用主干部分获取到的<strong>五个初步有效特征层</strong>进行上采样，并且进行特征融合，获得一个最终的，融合了<strong>所有特征的有效特征层</strong>。</p><ul><li><p>上采样</p></li><li><p>右边网络为特征融合网络：使用上采样产生的特征图与左侧特征图进行concatenate操作</p></li><li><p>Skip Connection中间四条灰色的平行线，Skip Connection就是在上采样的过程中，融合下采样过过程中的feature map。Skip Connection用到的融合的操作也很简单，就是将feature map的通道进行叠加，俗称Concat。</p></li><li><p>Concat操作也很好理解，举个例子：一本大小为10cm<em>10cm，厚度为3cm的书A，和一本大小为10cm</em>10cm，厚度为4cm的书B。将书A和书B，边缘对齐地摞在一起。这样就得到了，大小为10cm*10cm厚度为7cm的一摞书（就是直接把书叠起来的意思）</p></li><li><p>对于feature map，一个大小为<strong>256 <em> 256 </em> 64</strong>的feature map，即feature map的w（宽）为256，h（高）为256，c（通道数）为64。和一个大小为<strong>256 <em> 256 </em> 32</strong>的feature map进行Concat融合，就会得到一个大小为<strong>256 <em> 256 </em> 96</strong>的feature map。</p><p>在实际使用中，Concat融合的两个feature map的大小不一定相同，例如<strong>256 <em> 256 </em> 64</strong>的feature map和<strong>240 <em> 240 </em> 32</strong>的feature map进行Concat。</p><p>这种时候，就有两种办法：</p><ul><li><p>第一种：将大<strong>256 <em> 256 </em> 64</strong>的feature map进行裁剪，裁剪为<strong>240 <em> 240 </em> 64</strong>的feature map，比如上下左右，各舍弃8 pixel，裁剪后再进行Concat，得到<strong>240 <em> 240 </em> 96</strong>的feature map。</p></li><li><p>第二种：将小<strong>240 <em> 240 </em> 32</strong>的feature map进行padding操作，padding为<strong>256 <em> 256 </em> 32</strong>的feature map，比如上下左右，各补8 pixel，padding后再进行Concat，得到<strong>256 <em> 256 </em> 96</strong>的feature map。</p></li></ul><p>UNet采用的Concat方案就是第二种，将小的feature map进行padding，padding的方式是补0，一种常规的常量填充。</p></li></ul></li><li><p>第三部分是<strong>预测部分</strong>，我们会利用<strong>最终获得的最后一个有效特征层</strong>对每一个特征点进行分类，相当于对每一个像素点进行分类。<strong>（将最后特征层调整通道数，也就是我们要分类个数）</strong></p><ul><li>最后再经过两次卷积操作，生成特征图，再用两个卷积核大小为<strong>1*1</strong>的卷积做分类得到最后的两张heatmap，例如第一张表示第一类的得分，第二张表示第二类的得分heatmap，然后作为softmax函数的输入，算出概率比较大的softmax，然后再进行loss，反向传播计算。</li></ul></li></ul><h4 id="网络代码实现"><a href="#网络代码实现" class="headerlink" title="网络代码实现"></a>网络代码实现</h4><p>按照UNet的网络结构分parts去实现Unet结构，<strong>采取一种搭积木的方式，先定义各个独立的模块，最后组合拼接就可以！</strong></p><h5 id="DoubleConv模块"><a href="#DoubleConv模块" class="headerlink" title="DoubleConv模块"></a>DoubleConv模块</h5><p>如下图所示模块，连续的两个卷积的操作，在整个UNet网络中，主干特征提取网络和加强特征网络中各自使用了五次，每一层都会采取这个操作，故可以提取出来：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/2.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class DoubleConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.double_conv(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>nn.Sequential 是一个时许的容器，会将里面的 modle 逐一执行，执行顺序为：<strong>卷积-&gt;BN-&gt;ReLU-&gt;卷积-&gt;BN-&gt;ReLU。</strong></p></li><li><p>in_channels, out_channels，输入输出通道定义为参数，增强扩展使用</p></li><li><p>卷积 nn.Conv2d 的输出：</p><ul><li><p><strong>nn. Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,dilation=1, groups=1, bias=True, padding_mode= ‘zeros’ )</strong></p><ul><li>in_channels:输入的四维张量[N, C, H, W]中的C，也就是说输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。</li><li>out_channels:也很好理解，即期望的四维输出张量的channels数，不再多说。</li><li>kernel_size:卷积核的大小，一般我们会使用5x5、3x3这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）。<br>stride = 1:卷积核在图像窗口上每次平移的间隔，即所谓的步长。这个概念和Tensorflow等其他框架没什么区别，不再多言。</li><li>padding:这是Pytorch与Tensorflow在卷积层实现上最大的差别，padding也就是指图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。需要注意的是这里的填充包括图像的上下左右，以padding=1为例，若原始图像大小为32 <em> 32，那么padding后的图像大小就变成了34 </em> 34，而不是33*33。<br>Pytorch不同于Tensorflow的地方在于，Tensorflow提供的是padding的模式，比如same、valid，且不同模式对应了不同的输出图像尺寸计算公式。而Pytorch则需要手动输入padding的数量，当然，Pytorch这种实现好处就在于输出图像尺寸计算公式是唯一的，</li><li>dilation:这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）。更形象和直观的图示可以观察Github上的Dilated convolution animations，展示了dilation=2的情况。</li><li>groups:决定了是否采用分组卷积，groups参数可以参考groups参数详解</li><li>bias:即是否要添加偏置参数作为可学习参数的一个，默认为True。</li><li>padding_mode:即padding的模式，默认采用零填充。</li></ul></li><li><p>输出通道就是 out_channels</p></li><li><p>输出的 <strong>X * X</strong> 计算公式：</p><script type="math/tex; mode=display">O = （I - K + 2P）/ S +1</script></li></ul></li></ul><pre><code>- I 为输入feature map的大小，O为输出feature map的大小，K为卷积核的大小，P为padding的大小，S为步长</code></pre><h5 id="Down（下采样模块）"><a href="#Down（下采样模块）" class="headerlink" title="Down（下采样模块）"></a>Down（下采样模块）</h5><p>UNet的下采样模块有着4次的下采样过程，过程如下</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/3.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Down(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.maxpool_conv(x)</span><br></pre></td></tr></table></figure><ul><li>代码很简单，就是一个maxpool池化层，进行下采样，然后接一个DoubleConv模块。</li><li>到这里，左边的网络完成！！</li></ul><h5 id="Up（上采样模块）"><a href="#Up（上采样模块）" class="headerlink" title="Up（上采样模块）"></a>Up（上采样模块）</h5><p>上采样模块就是出来常规的上采样操作以外，还需要进行特征融合，</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/4.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Up(nn.Module):</span><br><span class="line">   </span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)</span><br><span class="line">        self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    def forward(self, x1, x2):</span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        # input is CHW</span><br><span class="line">        diffY = x2.size()[2] - x1.size()[2]</span><br><span class="line">        diffX = x2.size()[3] - x1.size()[3]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,</span><br><span class="line">                        diffY // 2, diffY - diffY // 2])</span><br><span class="line">     </span><br><span class="line">        x = torch.cat([x2, x1], dim=1)</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><ul><li><p>初始化函数里定义的上采样方法（反卷积）以及卷积采用DoubleConv</p></li><li><p>反卷积，顾名思义，就是反着卷积。卷积是让featuer map越来越小，反卷积就是让feature map越来越大，</p><p>下面蓝色为原始图片，周围白色的虚线方块为padding结果，通常为0，上面绿色为卷积后的图片。</p><p>这个示意图，就是一个从 <strong>2 * 2</strong>的feature map  —-&gt;  <strong>4 * 4 </strong>的feature map过程。</p><p>在forward前向传播函数中，x1接收的是<strong>上采样</strong>的数据，x2接收的是<strong>特征融合</strong>的数据。特征融合方法就是，上文提到的，先对小的feature map进行padding，再进行concat。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/6.gif" alt=""></p></li></ul><h5 id="OutConv模块"><a href="#OutConv模块" class="headerlink" title="OutConv模块"></a><strong>OutConv模块</strong></h5><p>用上述的DoubleConv模块、Down模块、Up模块就可以拼出UNet的主体网络结构了。UNet网络的输出需要根据分割数量，整合输出通道。</p><p>利用前面的模块，我们可以获取输入进来的图片的特征，此时，我们需要利用特征获得预测结果</p><p>利用特征获得预测结果的过程为：</p><ul><li><strong>利用一个1x1卷积进行通道调整，将最终特征层的通道数调整成num_classes。</strong>  <strong>（即对每一个像素点进行分类）</strong></li></ul><p>这个过程简单，顺便也包装一下吧</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class OutConv(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, out_channels):</span><br><span class="line">        super(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.conv(x)</span><br></pre></td></tr></table></figure><p>到这里，所有的积木已经完成了，接下来就是搭建的过程了。</p><h5 id="UNet模块"><a href="#UNet模块" class="headerlink" title="UNet模块"></a>UNet模块</h5><p>到这里，按照UNet网络结构，设置每个模块的输入输出通道个数以及调用顺序，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from nets.net_of_me.unet_parts import *</span><br><span class="line">class UNet(nn.Module):</span><br><span class="line">    def __init__(self, n_channels, n_classes, bilinear=False):</span><br><span class="line">        super(UNet, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, 64)</span><br><span class="line">        self.down1 = Down(64, 128)</span><br><span class="line">        self.down2 = Down(128, 256)</span><br><span class="line">        self.down3 = Down(256, 512)</span><br><span class="line">        self.down4 = Down(512, 1024)</span><br><span class="line">        self.up1 = Up(1024, 512, bilinear)</span><br><span class="line">        self.up2 = Up(512, 256, bilinear)</span><br><span class="line">        self.up3 = Up(256, 128, bilinear)</span><br><span class="line">        self.up4 = Up(128, 64, bilinear)</span><br><span class="line">        self.outc = OutConv(64, n_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        return logits</span><br></pre></td></tr></table></figure><h4 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练网络的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.backends.cudnn as cudnn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">from nets.unet_training import CE_Loss, Dice_loss, LossHistory</span><br><span class="line">from utils.dataloader import DeeplabDataset, deeplab_dataset_collate</span><br><span class="line">from utils.metrics import f_score</span><br><span class="line"></span><br><span class="line">def get_lr(optimizer):</span><br><span class="line">    for param_group in optimizer.param_groups:</span><br><span class="line">        return param_group[&#x27;lr&#x27;]</span><br><span class="line"></span><br><span class="line">def fit_one_epoch(net,epoch,epoch_size,epoch_size_val,gen,genval,Epoch,cuda):</span><br><span class="line">    net = net.train()</span><br><span class="line">    total_loss = 0</span><br><span class="line">    total_f_score = 0</span><br><span class="line"></span><br><span class="line">    val_toal_loss = 0</span><br><span class="line">    val_total_f_score = 0</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(gen):</span><br><span class="line">        if iteration &gt;= epoch_size:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        #进行训练</span><br><span class="line">        outputs = net(imgs)</span><br><span class="line">        loss    = CE_Loss(outputs, pngs, num_classes = NUM_CLASSES)</span><br><span class="line">        if dice_loss:</span><br><span class="line">            main_dice = Dice_loss(outputs, labels)</span><br><span class="line">            loss      = loss + main_dice</span><br><span class="line"></span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            #-------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">        waste_time = time.time() - start_time #训练epoch需要的时间</span><br><span class="line">        start_time = time.time()</span><br><span class="line"></span><br><span class="line">        if (iteration % 50 == 0):</span><br><span class="line">            print(&quot;epoch = &#123;&#125; and loss = &#123;&#125; and waste_time = &#123;&#125;&quot;.format(epoch,loss.item(),waste_time))</span><br><span class="line">            #写入日志文件</span><br><span class="line">            with open(&quot;log/train_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch,loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Training&#x27;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Validation&#x27;)</span><br><span class="line">    for iteration, batch in enumerate(genval):</span><br><span class="line">        if iteration &gt;= epoch_size_val:</span><br><span class="line">            break</span><br><span class="line">        imgs, pngs, labels = batch</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            imgs = torch.from_numpy(imgs).type(torch.FloatTensor)</span><br><span class="line">            pngs = torch.from_numpy(pngs).type(torch.FloatTensor).long()</span><br><span class="line">            labels = torch.from_numpy(labels).type(torch.FloatTensor)</span><br><span class="line">            if cuda:</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                pngs = pngs.cuda()</span><br><span class="line">                labels = labels.cuda()</span><br><span class="line"># 开始训练</span><br><span class="line">            outputs = net(imgs)</span><br><span class="line">            #计算损失函数</span><br><span class="line">            val_loss = CE_Loss(outputs, pngs, num_classes=NUM_CLASSES)</span><br><span class="line">            if dice_loss:</span><br><span class="line">                main_dice = Dice_loss(outputs, labels)</span><br><span class="line">                val_loss = val_loss + main_dice</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            #   计算f_score</span><br><span class="line">            # -------------------------------#</span><br><span class="line">            _f_score = f_score(outputs, labels)</span><br><span class="line"></span><br><span class="line">            val_toal_loss += val_loss.item()</span><br><span class="line">            val_total_f_score += _f_score.item()</span><br><span class="line"></span><br><span class="line">            if (iteration % 50 == 0):</span><br><span class="line">                print(&quot;epoch = &#123;&#125; and val_loss = &#123;&#125; &quot;.format(epoch, val_loss.item()))</span><br><span class="line">                # 写入日志文件</span><br><span class="line">                with open(&quot;log/val_logs.txt&quot;, &quot;a&quot;) as f:  # 格式化字符串还能这么用！</span><br><span class="line">                    f.write(&quot;epoch = &#123;&#125; and loss = &#123;&#125;&quot;.format(epoch, val_loss.item()) + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finish Validation&#x27;)</span><br><span class="line">    print(&#x27;Epoch:&#x27; + str(epoch + 1) + &#x27;/&#x27; + str(Epoch))</span><br><span class="line">    print(&#x27;Total Loss: %.4f || Val Loss: %.4f &#x27; % (total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line">    print(&#x27;Saving state, iter:&#x27;, str(epoch + 1))</span><br><span class="line">    torch.save(model.state_dict(), &#x27;model/Epoch%d-Total_Loss%.4f-%.4f.pth&#x27; % ((epoch + 1), total_loss / (epoch_size + 1), val_toal_loss / (epoch_size_val + 1)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    #------------------------------#</span><br><span class="line">    #   输入图片的大小</span><br><span class="line">    #------------------------------#</span><br><span class="line">    inputs_size = [512,512,3]</span><br><span class="line">    #---------------------#</span><br><span class="line">    #   分类个数+1</span><br><span class="line">    #   2+1</span><br><span class="line">    #---------------------#</span><br><span class="line">    NUM_CLASSES = 21</span><br><span class="line">    #   Cuda的使用</span><br><span class="line">    #-------------------------------#</span><br><span class="line">    Cuda = True</span><br><span class="line">    #linux服务器</span><br><span class="line">    dataset_path = &quot;/data/xwd/pro_datas/VOCdevkit/VOC2007&quot;</span><br><span class="line"></span><br><span class="line">    #网络</span><br><span class="line">    model = UNet(n_channels=inputs_size[-1], n_classes=NUM_CLASSES).train()</span><br><span class="line"></span><br><span class="line">    if Cuda:</span><br><span class="line">        net = torch.nn.DataParallel(model)</span><br><span class="line">        cudnn.benchmark = True</span><br><span class="line">        net = net.cuda()</span><br><span class="line"></span><br><span class="line">    # 打开训练数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/train.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        train_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    # 打开验证数据集的txt</span><br><span class="line">    with open(os.path.join(dataset_path, &quot;ImageSets/Segmentation/val.txt&quot;),&quot;r&quot;) as f:</span><br><span class="line">        val_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">    if True:</span><br><span class="line">        lr = 1e-4</span><br><span class="line">        Init_Epoch = 0</span><br><span class="line">        Interval_Epoch = 5</span><br><span class="line">        Batch_size = 4</span><br><span class="line"></span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr)  #优化器</span><br><span class="line">        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.92) #学习率的调整</span><br><span class="line">        </span><br><span class="line">        #封装数据</span><br><span class="line">        train_dataset = DeeplabDataset(train_lines, inputs_size, NUM_CLASSES, True, dataset_path)</span><br><span class="line">        val_dataset = DeeplabDataset(val_lines, inputs_size, NUM_CLASSES, False, dataset_path)</span><br><span class="line">        gen = DataLoader(train_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                         drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line">        gen_val = DataLoader(val_dataset, batch_size=Batch_size, num_workers=4, pin_memory=True,</span><br><span class="line">                             drop_last=True, collate_fn=deeplab_dataset_collate)</span><br><span class="line"></span><br><span class="line">        epoch_size = len(train_lines) // Batch_size</span><br><span class="line">        epoch_size_val = len(val_lines) // Batch_size</span><br><span class="line"></span><br><span class="line">        if epoch_size == 0 or epoch_size_val == 0:</span><br><span class="line">            raise ValueError(&quot;数据集过小，无法进行训练，请扩充数据集。&quot;)</span><br><span class="line"></span><br><span class="line">        for epoch in range(Init_Epoch, Interval_Epoch):</span><br><span class="line">            fit_one_epoch(model, epoch, epoch_size, epoch_size_val, gen, gen_val, Interval_Epoch, Cuda)</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>封装数据集</strong>的办法主要采用：自定义类继承Dataset，下面展示的是他的伪代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># ================================================================== #</span><br><span class="line">#                Input pipeline for custom dataset                 #</span><br><span class="line"># ================================================================== </span><br><span class="line"># You should build your custom dataset as below.</span><br><span class="line">class CustomDataset(torch.utils.data.Dataset):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Initialize file paths or a list of file names. </span><br><span class="line">        pass</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        # TODO</span><br><span class="line">        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span><br><span class="line">        # 2. Preprocess the data (e.g. torchvision.Transform).</span><br><span class="line">        # 3. Return a data pair (e.g. image and label).</span><br><span class="line">        pass</span><br><span class="line">    def __len__(self):</span><br><span class="line">        # You should change 0 to the total size of your dataset.</span><br><span class="line">        return 0 </span><br><span class="line"># You can then use the prebuilt data loader. </span><br><span class="line">custom_dataset = CustomDataset()</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,</span><br><span class="line">                                           batch_size=64, </span><br><span class="line">                                           shuffle=True)</span><br></pre></td></tr></table></figure><ul><li><strong>init</strong>函数是这个类的初始化函数，根据指定的图片路径，读取所有图片数据，</li><li><strong>len</strong>函数可以返回数据的多少，这个类实例化后，通过len()函数调用。</li><li><strong>getitem</strong>函数是数据获取函数，在这个函数里你可以写数据怎么读，怎么处理，并且可以一些数据预处理、数据增强都可以在这里进行</li></ul><p>下面的是自定义的这个方法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">class DeeplabDataset(Dataset):</span><br><span class="line">    def __init__(self,train_lines,image_size,num_classes,random_data,dataset_path):</span><br><span class="line">        super(DeeplabDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.train_lines    = train_lines</span><br><span class="line">        self.train_batches  = len(train_lines)</span><br><span class="line">        self.image_size     = image_size</span><br><span class="line">        self.num_classes    = num_classes</span><br><span class="line">        self.random_data    = random_data</span><br><span class="line">        self.dataset_path   = dataset_path</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.train_batches</span><br><span class="line"></span><br><span class="line">    def rand(self, a=0, b=1):</span><br><span class="line">        return np.random.rand() * (b - a) + a</span><br><span class="line"></span><br><span class="line">    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=1.5, val=1.5):</span><br><span class="line">        label = Image.fromarray(np.array(label))</span><br><span class="line"></span><br><span class="line">        h, w = input_shape</span><br><span class="line">        # resize image</span><br><span class="line">        rand_jit1 = rand(1-jitter,1+jitter)</span><br><span class="line">        rand_jit2 = rand(1-jitter,1+jitter)</span><br><span class="line">        new_ar = w/h * rand_jit1/rand_jit2</span><br><span class="line"></span><br><span class="line">        scale = rand(0.5,1.5)</span><br><span class="line">        if new_ar &lt; 1:</span><br><span class="line">            nh = int(scale*h)</span><br><span class="line">            nw = int(nh*new_ar)</span><br><span class="line">        else:</span><br><span class="line">            nw = int(scale*w)</span><br><span class="line">            nh = int(nw/new_ar)</span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        label = label.resize((nw,nh), Image.NEAREST)</span><br><span class="line">        label = label.convert(&quot;L&quot;)</span><br><span class="line">        </span><br><span class="line">        # flip image or not</span><br><span class="line">        flip = rand()&lt;.5</span><br><span class="line">        if flip: </span><br><span class="line">            image = image.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">            label = label.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">        </span><br><span class="line">        # place image</span><br><span class="line">        dx = int(rand(0, w-nw))</span><br><span class="line">        dy = int(rand(0, h-nh))</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, (w,h), (128,128,128))</span><br><span class="line">        new_label = Image.new(&#x27;L&#x27;, (w,h), (0))</span><br><span class="line">        new_image.paste(image, (dx, dy))</span><br><span class="line">        new_label.paste(label, (dx, dy))</span><br><span class="line">        image = new_image</span><br><span class="line">        label = new_label</span><br><span class="line"></span><br><span class="line">        # distort image</span><br><span class="line">        hue = rand(-hue, hue)</span><br><span class="line">        sat = rand(1, sat) if rand()&lt;.5 else 1/rand(1, sat)</span><br><span class="line">        val = rand(1, val) if rand()&lt;.5 else 1/rand(1, val)</span><br><span class="line">        x = cv2.cvtColor(np.array(image,np.float32)/255, cv2.COLOR_RGB2HSV)</span><br><span class="line">        x[..., 0] += hue*360</span><br><span class="line">        x[..., 0][x[..., 0]&gt;1] -= 1</span><br><span class="line">        x[..., 0][x[..., 0]&lt;0] += 1</span><br><span class="line">        x[..., 1] *= sat</span><br><span class="line">        x[..., 2] *= val</span><br><span class="line">        x[x[:,:, 0]&gt;360, 0] = 360</span><br><span class="line">        x[:, :, 1:][x[:, :, 1:]&gt;1] = 1</span><br><span class="line">        x[x&lt;0] = 0</span><br><span class="line">        image_data = cv2.cvtColor(x, cv2.COLOR_HSV2RGB)*255</span><br><span class="line">        return image_data,label</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        if index == 0:</span><br><span class="line">            shuffle(self.train_lines)  </span><br><span class="line">        annotation_line = self.train_lines[index]</span><br><span class="line">        name = annotation_line.split()[0]</span><br><span class="line">        # 从文件中读取图像</span><br><span class="line">        jpg = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;JPEGImages&quot;), name + &quot;.jpg&quot;))</span><br><span class="line">        png = Image.open(os.path.join(os.path.join(self.dataset_path, &quot;SegmentationClass&quot;), name + &quot;.png&quot;))</span><br><span class="line"></span><br><span class="line">        if self.random_data:</span><br><span class="line">            jpg, png = self.get_random_data(jpg,png,(int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        else:</span><br><span class="line">            jpg, png = letterbox_image(jpg, png, (int(self.image_size[1]),int(self.image_size[0])))</span><br><span class="line">        png = np.array(png)</span><br><span class="line">        png[png &gt;= self.num_classes] = self.num_classes</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        #   转化成one_hot的形式</span><br><span class="line">        #   在这里需要+1是因为voc数据集有些标签具有白边部分</span><br><span class="line">        #   我们需要将白边部分进行忽略，+1的目的是方便忽略。</span><br><span class="line">        #-------------------------------------------------------#</span><br><span class="line">        seg_labels = np.eye(self.num_classes+1)[png.reshape([-1])]</span><br><span class="line">        seg_labels = seg_labels.reshape((int(self.image_size[1]),int(self.image_size[0]),self.num_classes+1))</span><br><span class="line">        jpg = np.transpose(np.array(jpg),[2,0,1])/255</span><br><span class="line">        return jpg, png, seg_labels</span><br></pre></td></tr></table></figure><p>我这边设置的epoch并不算很大，采用3090的显卡也是运行了一段时间是时间，可以看到网络，loss实在逐渐在收敛的：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/7.jpg" alt=""></p><p>采用训练好的模型进行预测，看看结果如何：</p><p>这边采用的是在网络上copy的图片预处理和后续处理的代码，本人目前对图片处理还是比较菜，把别人的代码贴在这里，最后给出自己的预测结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line">import colorsys</span><br><span class="line">import copy</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from PIL import Image</span><br><span class="line">from torch import nn</span><br><span class="line">from nets.net_of_me.unet_model import UNet</span><br><span class="line">#-------------------------------------------#</span><br><span class="line">#   使用自己训练好的模型预测需要修改2个参数</span><br><span class="line">#   model_path和num_classes都需要修改！</span><br><span class="line">#   如果出现shape不匹配</span><br><span class="line">#   一定要注意训练时的model_path和num_classes数的修改</span><br><span class="line">#--------------------------------------------#</span><br><span class="line">class Unet(object):</span><br><span class="line">    _defaults = &#123;</span><br><span class="line">        &quot;model_path&quot;        : &#x27;model\Epoch2-Total_Loss1.0039-0.8573.pth&#x27;, #保存的训练模型的路径</span><br><span class="line">        &quot;model_image_size&quot;  : (512, 512, 3), #输入图片的大小</span><br><span class="line">        &quot;num_classes&quot;       : 21, </span><br><span class="line">        &quot;cuda&quot;              : True,</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        #   blend参数用于控制是否</span><br><span class="line">        #   让识别结果和原图混合</span><br><span class="line">        #--------------------------------#</span><br><span class="line">        &quot;blend&quot;             : True</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   初始化UNET</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        self.__dict__.update(self._defaults)</span><br><span class="line">        self.device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">        self.generate()</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   获得所有的分类</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def generate(self):</span><br><span class="line">        self.net = UNet(n_channels=self.model_image_size[-1],n_classes=self.num_classes).eval()</span><br><span class="line"></span><br><span class="line">        # 加载本地的模型参数</span><br><span class="line">        state_dict = torch.load(self.model_path,self.device)</span><br><span class="line">        self.net.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line">        if self.cuda:</span><br><span class="line">            self.net = nn.DataParallel(self.net) #可以调用多个GPU，帮助加速训练</span><br><span class="line">            self.net = self.net.to(self.device)</span><br><span class="line"></span><br><span class="line">        print(&#x27;&#123;&#125; model loaded.&#x27;.format(self.model_path))</span><br><span class="line"></span><br><span class="line">        if self.num_classes &lt;= 21:</span><br><span class="line">            self.colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),                              (0, 128, 128),  (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), </span><br><span class="line">                           (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), </span><br><span class="line">                           (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64,128),(128, 64, 12)]</span><br><span class="line">        else:</span><br><span class="line">            # 画框设置不同的颜色</span><br><span class="line">            hsv_tuples = [(x / len(self.class_names), 1., 1.)</span><br><span class="line">                        for x in range(len(self.class_names))]</span><br><span class="line">            self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))</span><br><span class="line">            self.colors = list(</span><br><span class="line">                map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),</span><br><span class="line">                    self.colors))</span><br><span class="line"></span><br><span class="line">    def letterbox_image(self ,image, size):</span><br><span class="line">        image = image.convert(&quot;RGB&quot;)</span><br><span class="line">        iw, ih = image.size</span><br><span class="line">        w, h = size</span><br><span class="line">        scale = min(w/iw, h/ih)</span><br><span class="line">        nw = int(iw*scale)</span><br><span class="line">        nh = int(ih*scale)</span><br><span class="line"></span><br><span class="line">        image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        new_image = Image.new(&#x27;RGB&#x27;, size, (128,128,128))</span><br><span class="line">        new_image.paste(image, ((w-nw)//2, (h-nh)//2))</span><br><span class="line">        return new_image,nw,nh</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    #   检测图片，处理图片</span><br><span class="line">    #---------------------------------------------------#</span><br><span class="line">    def detect_image(self, image):</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。</span><br><span class="line">        #---------------------------------------------------------#</span><br><span class="line">        image = image.convert(&#x27;RGB&#x27;)</span><br><span class="line">        </span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   对输入图像进行一个备份，后面用于绘图</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        old_img = copy.deepcopy(image)</span><br><span class="line"></span><br><span class="line">        orininal_h = np.array(image).shape[0]</span><br><span class="line">        orininal_w = np.array(image).shape[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   进行不失真的resize，添加灰条，进行图像归一化</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        image, nw, nh = self.letterbox_image(image,(self.model_image_size[1],self.model_image_size[0]))</span><br><span class="line">        a = np.array(image).shape</span><br><span class="line">        images = [np.array(image)/255]</span><br><span class="line"></span><br><span class="line">        images = np.transpose(images,(0,3,1,2))</span><br><span class="line"></span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        #   图片传入网络进行预测</span><br><span class="line">        #---------------------------------------------------#</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            images = torch.from_numpy(images).type(torch.FloatTensor) #转化为tensor</span><br><span class="line">            if self.cuda:</span><br><span class="line">                #images =images.cuda()</span><br><span class="line">                images = images.cpu()</span><br><span class="line"></span><br><span class="line">            pr = self.net(images)</span><br><span class="line"></span><br><span class="line">            pr = pr[0]</span><br><span class="line">            pr1 = pr[1]</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            #   取出每一个像素点的种类</span><br><span class="line">            #---------------------------------------------------#</span><br><span class="line">            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy().argmax(axis=-1)</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            #   将灰条部分截取掉</span><br><span class="line">            #--------------------------------------#</span><br><span class="line">            pr = pr[int((self.model_image_size[0]-nh)//2):int((self.model_image_size[0]-nh)//2+nh), </span><br><span class="line">                    int((self.model_image_size[1]-nw)//2):int((self.model_image_size[1]-nw)//2+nw)]</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   创建一副新图，并根据每个像素点的种类赋予颜色</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        seg_img = np.zeros((np.shape(pr)[0],np.shape(pr)[1],3))</span><br><span class="line">        for c in range(self.num_classes):</span><br><span class="line">            seg_img[:,:,0] += ((pr[:,: ] == c )*( self.colors[c][0] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,1] += ((pr[:,: ] == c )*( self.colors[c][1] )).astype(&#x27;uint8&#x27;)</span><br><span class="line">            seg_img[:,:,2] += ((pr[:,: ] == c )*( self.colors[c][2] )).astype(&#x27;uint8&#x27;)</span><br><span class="line"></span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片转换成Image的形式</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        image = Image.fromarray(np.uint8(seg_img)).resize((orininal_w,orininal_h))</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        #   将新图片和原图片混合</span><br><span class="line">        #------------------------------------------------#</span><br><span class="line">        if self.blend:</span><br><span class="line">             image = Image.blend(old_img,image,0.7)      </span><br><span class="line">        return image,  old_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    unet = Unet()</span><br><span class="line">    while True:</span><br><span class="line">        img = input(&#x27;Input image filename:&#x27;)</span><br><span class="line">        try:</span><br><span class="line">            image = Image.open(img)</span><br><span class="line">        except:</span><br><span class="line">            print(&#x27;Open Error! Try again!&#x27;)</span><br><span class="line">            continue</span><br><span class="line">        else:</span><br><span class="line">            r_image,old_image= unet.detect_image(image)</span><br><span class="line">            old_image.show()</span><br><span class="line">            r_image.show()</span><br></pre></td></tr></table></figure><p>调用这个函数，得到的预测结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/8.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/9.jpg" alt=""></p><h4 id="语义分割的MIOU指标"><a href="#语义分割的MIOU指标" class="headerlink" title="语义分割的MIOU指标"></a>语义分割的MIOU指标</h4><p>语义分割的标准度量。其计算所有类别交集和并集之比的平均值.，语义分割说到底也还是一个分割任务，既然是一个分割的任务，预测的结果往往就是四种情况：</p><ul><li><p>true positive（TP）：预测正确, 预测结果是正类, 真实是正类 </p></li><li><p>false positive（FP）：预测错误, 预测结果是正类, 真实是负类</p></li><li><p>true negative（TN）：预测错误, 预测结果是负类, 真实是正类</p></li><li><p>false negative（FN）：预测正确, 预测结果是负类, 真实是负类  </p></li></ul><p>mIOU 的定义：计算真实值和预测值两个集合的交集和并集之比。这个比例可以变形为TP（交集）比上TP、FP、FN之和（并集）。即：mIOU=TP/(FP+FN+TP)。</p><p>计算公式：</p><script type="math/tex; mode=display">MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{p_{ii}}{\sum_{j=0}^kp_{ij} + \sum_{j=0}^kp_{ji} - p_{ii}}</script><p>等价于：</p><script type="math/tex; mode=display">MIoU = \frac{1}{k+1} \sum_{i=0}^k\frac{TP}{FN+FP+TP}</script><p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是基于全局的评价。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210924/10.jpg" alt=""></p><p><code>MIoU</code>：计算两圆交集（橙色部分）与两圆并集（红色+橙色+黄色）之间的比例，理想情况下两圆重合，比例为1。</p><p>计算本网络的MIoU可以采样训练好的模型进行计算，计算的结果比例越接近1效果越好。</p><p>代码实现后续把，hhhhhhhhhhhhhhhhhhhhh。。。。。。。</p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉的基本知识介绍</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p><strong>计算机视觉方向：图像分类，图像检测，目标检测，图像分割，图像生成，目标跟踪，超分辨率重构，关键点定位，图像降噪，多模态，图像加密，视频编解码，3D视觉等等</strong></p><h3 id="图像基本概念"><a href="#图像基本概念" class="headerlink" title="图像基本概念"></a>图像基本概念</h3><h4 id="颜色空间"><a href="#颜色空间" class="headerlink" title="颜色空间"></a>颜色空间</h4><ul><li>颜色空间也称彩色模型，用于描述色彩</li><li>常见的颜色空间包括：RGB（常用3通道）、CMYK、YUV（摄像头）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/1.png" alt=""></p><h4 id="RGB色彩模式"><a href="#RGB色彩模式" class="headerlink" title="RGB色彩模式"></a>RGB色彩模式</h4><ul><li>RGB色彩模式是工业界的一种颜色标准</li><li>通过对红（R）、绿（G）、蓝（B）三个颜色通道的变化以及它们相互之间的叠加来<br>得到各式各样的颜色的</li><li>红、绿、蓝三个<strong>颜色通道</strong>每种色各分为256阶亮度，（R，G，B）三维就是一个像素点，（0，0，0）黑，（255，255，255）白</li><li>H <em> W </em> C  H:长，W:宽，C:通道</li></ul><h4 id="HSV色彩模式"><a href="#HSV色彩模式" class="headerlink" title="HSV色彩模式"></a>HSV色彩模式</h4><ul><li>色相（Hue）：指物体传导或反射的波长。更常见的是以颜色如红色，橘色或绿色来辨识，取0到360度的数值来衡量</li><li>饱和度（Saturation）：又称色度，是指色彩的强度或纯度，取值范围为0%~100%</li><li>明度（Value）：表示颜色明亮的程度，取值范围为0%（黑）到100%（白）</li></ul><h4 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h4><ul><li><p>灰度图通常由一个unit8、unit16、单精度类型或者双精度类型的数组描述，也就是上面的 C=1，通道为 1</p></li><li><p>M*N的矩阵，矩阵中每一个元素与图像的一个像素点相对应</p></li><li><p>通常0代表黑色，1、255或65635（为数据矩阵的取值范围上限）代表白色</p><blockquote><p>浮点算法：Gray=R<em>0.3+G</em>0.59+B<em>0.11<br>整数方法：Gray=（R</em>30+G<em>59+B</em>11）/100<br>移位方法：Gray=（R<em>28+G</em>151+B*77）&gt;&gt;8<br>平均值法：Gray=（R+G+B）/3<br>仅取绿色：Gray=G</p></blockquote></li></ul><h3 id="图像处理基本概念"><a href="#图像处理基本概念" class="headerlink" title="图像处理基本概念"></a>图像处理基本概念</h3><h4 id="亮度，对比度，饱和度"><a href="#亮度，对比度，饱和度" class="headerlink" title="亮度，对比度，饱和度"></a>亮度，对比度，饱和度</h4><ul><li>亮度：图像的明亮程度，在单色图像中，最高的值应该对应于白色，最低的值应当对应于黑色；</li><li>对比度：图像暗和亮的落差值，即图像最大灰度级和最小灰度级之间的差值，差异范围越大代表对比越大，差异范围越小代表对比越小</li><li>饱和度：图像颜色种类的多少，饱和度越高，颜色种类越多，外观上看起来图像会更鲜艳</li><li>对于亮度和对比度，可以从RGB图上进行数据增强</li><li>对于饱和度，可以从HSV/HSI/HSL色彩空间上进行增强</li></ul><h4 id="图像平滑-降噪"><a href="#图像平滑-降噪" class="headerlink" title="图像平滑/降噪"></a>图像平滑/降噪</h4><ul><li>图像平滑是指用于突出图像的宽大区域、低频成分、主干部分或抑制图像噪声和干扰高频成分的图像处理方法，使图像亮度平缓渐变，减小突变梯度，改善图像质量。会出现边缘没有，轮廓结构不明显了<ul><li>归一化块滤波器（Normalized Box Filter）</li><li>高斯滤波器（Gaussian Filter）</li><li>中值滤波器（Median Filter）</li><li>双边滤波（Bilateral Filter）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/2.png" alt=""></p><h4 id="图像锐化-增强"><a href="#图像锐化-增强" class="headerlink" title="图像锐化/增强"></a>图像锐化/增强</h4><ul><li>图像锐化与图像平滑是相反的操作，锐化是通过增强高频分量来减少图像中的模糊，增强图像细节边缘和轮廓，增强灰度反差，便于后期对目标的识别和处理。</li><li>锐化处理在增强图像边缘的同时也增加了图像的噪声。</li><li>方法包括：微分法和高通滤波法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/3.jpg" alt=""></p><h4 id="边缘提取算子"><a href="#边缘提取算子" class="headerlink" title="边缘提取算子"></a>边缘提取算子</h4><ul><li><p>图像中的高频和低频的概念理解、</p></li><li><p>通过微分的方式计算图像的边缘（色差或者灰度值做差）</p><blockquote><p>Roberts算子<br>Prewitt算子<br>sobel算子<br>Canny算子<br>Laplacian算子<br>等等</p></blockquote></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/4.jpg" alt=""></p><h4 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h4><ul><li>直方图均衡化是将原图像通过某种变换，得到一幅灰度直方图为均匀分布的新图像的方法。</li><li>对在图像中像素个数多的灰度级进行展宽，而对像素个数少的灰度级进行缩减，从而达到清晰图像的目的。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/5.jpg" alt=""></p><h4 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h4><ul><li><p>图像滤波可以更改或者增强图像。</p></li><li><p>通过滤波，可以强调一些特征或者去除图像中一些不需要的部分。</p></li><li><p>滤波是一个邻域操作算子，利用给定像素周围的像素的值决定此像素的最终的输出值</p></li><li><p>常见的应用包括去噪、图像增强、检测边缘、检测角点、模板匹配等</p><blockquote><p>均值滤波<br>中值滤波<br>高斯滤波<br>双边滤波<br>等等</p></blockquote></li></ul><h4 id="形态学运算"><a href="#形态学运算" class="headerlink" title="形态学运算"></a>形态学运算</h4><ul><li>腐蚀：腐蚀的效果是把图片”变瘦”，其原理是在原图的小区域内取局部最小值。</li><li>膨胀：膨胀与腐蚀相反，取的是局部最大值，效果是把图片”变胖”</li><li>开运算：先腐蚀后膨胀（因为先腐蚀会分开物体，这样容易记住），可以分离物体，消除小区域</li><li>闭运算：先膨胀后腐蚀（先膨胀会使白色的部分扩张，以至于消除/“闭合”物体里面的小黑洞）</li><li>形态学梯度：膨胀图减去腐蚀图，得到轮廓图</li><li>顶帽：原图减去开运算后的图</li><li>黑帽：闭运算后的图减去原图</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/6.jpg" alt=""></p><h3 id="卷积神经网络概念介绍"><a href="#卷积神经网络概念介绍" class="headerlink" title="卷积神经网络概念介绍"></a>卷积神经网络概念介绍</h3><ul><li><p>由卷积核构建，卷积核简称为卷积，也称为滤波器。卷积的大小可以在实际需要时自定义其长和宽（1 <em> 1, 3 </em> 3, 5 * 5）。</p></li><li><p>卷积神经网：以卷积层为主的深度网络结构</p></li><li><strong>卷积层，激活层，BN层，池化层，全连接层（FC层），损失层</strong></li></ul><h4 id="卷积层定义"><a href="#卷积层定义" class="headerlink" title="卷积层定义"></a>卷积层定义</h4><ul><li>对图像和滤波矩阵做内积（逐个元素相乘再求和）的操作<ul><li><strong>nn.Conv2d（in channels，out channels，kernel_size，stride=1，padding=0，dilation=1，groups=1，bias=True）</strong></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/7.jpg" alt=""></p><h4 id="常见的卷积操作"><a href="#常见的卷积操作" class="headerlink" title="常见的卷积操作"></a>常见的卷积操作</h4><ul><li>分组卷积（group参数）</li><li>空洞卷积（dilation参数）</li><li>深度可分离卷积（分组卷积+1×1卷积）。</li><li>反卷积（torch.nn.ConvTranspose2d）</li><li>可变形卷积等等</li></ul><h4 id="理解卷积层的重要概念"><a href="#理解卷积层的重要概念" class="headerlink" title="理解卷积层的重要概念"></a>理解卷积层的重要概念</h4><ul><li><p>感受野（Receptive Field），指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。</p></li><li><p>参数量：参与计算参数的个数，占用内存空间</p></li><li><p>FLOPS：每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</p></li><li><p>FLOPs：浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。</p></li><li><p>MAC：乘加次数，用来衡量计算量。</p></li><li><p>从感受野不变+减少参数量的角度压缩卷积层，压缩卷积层参数&amp;&amp;计算量</p><blockquote><p>采用多个3×3卷积核代替大卷积核<br>采用深度可分离卷积<br>通道Shuffle<br>Pooling层<br>Stride=2<br>等等</p></blockquote></li><li><p>常见卷积层组合结构：<strong>堆叠，跳连，并连</strong></p></li></ul><h4 id="池化层（下采样）"><a href="#池化层（下采样）" class="headerlink" title="池化层（下采样）"></a>池化层（下采样）</h4><ul><li><p>对图片进行压缩（降采样）的一种方法，如max pooling, average pooling等</p></li><li><p>对输入的特征图进行压缩</p><ul><li>一方面使特征图变小，简化网络计算复杂度；</li><li>一方面进行特征压缩，提取主要特征</li></ul></li><li>最大池化（Max Pooling）、平均池化（Average Pooling）等口 </li><li>nn.MaxPool2d（kernel_size，stride=None，padding=0，dilation=1，return_indices=False，ceil_mode=False）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/8.jpg" alt=""></p><h4 id="上采样层"><a href="#上采样层" class="headerlink" title="上采样层"></a>上采样层</h4><ul><li>Resize，如双线性插值直接缩放，类似于图像缩放，概念可见最邻近插值算法和双线性插值算法——图像缩放</li><li>Deconvolution，也叫Transposed Convolution</li><li>实现函数<ul><li>nn.functi onal.interpolate（input，size=None，scale_factor=None，mode=’nearest’，align_corners=None）</li><li>nn.ConvTranspose2d（in channels，out channels，kernel_size，stride=1，padding=0，output padding=0，bias=True）</li></ul></li></ul><h4 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h4><ul><li><p>激活函数的作用就是，在所有的隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数了，因而神经网络的表达能力更加强大了。</p></li><li><p>激活函数：为了增加网络的非线性，进而提升网络的表达能力，<strong>详细见另外一篇博客</strong></p></li><li>ReLU函数、Leakly ReLU函数、ELU函数等</li><li>torch.nn.ReLU（inplace=True）</li></ul><h4 id="BatchNorm层"><a href="#BatchNorm层" class="headerlink" title="BatchNorm层"></a>BatchNorm层</h4><ul><li>通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</li><li>Batchnorm是归一化的一种手段，它会减小图像之间的绝对差异，突出相对差异，加快训练速度</li><li>不适用的问题：image-to-image以及对噪声敏感的任务</li><li>nn.BatchNorm2d（num features，eps=1e-05，momentum=0.1，affine=True，track running_stats=True）</li></ul><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><ul><li><p>口连接所有的特征，将输出值送给分类器（如softmax分类器）（线性）</p><ul><li>对前层的特征进行一个加权和，（卷积层是将数据输入映射到隐层特征空间）将特征空间通过线性变换映射到样本标记空间（也就是label）</li><li>可以通过1×1卷积+global average pooling代替</li><li>可以通过全连接层参数冗余</li><li>全连接层参数和尺寸相关</li></ul></li><li><p>nn.Linear（in features，out features，bias）</p></li></ul><h4 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h4><ul><li>在不同的训练过程中随机扔掉一部分神经元</li><li>测试过程中不使用随机失活，所有的神经元都激活</li><li>为了防止或减轻过拟合而使用的函数，它一般用在全连接层</li><li>nn.dropout</li></ul><h4 id="损失层"><a href="#损失层" class="headerlink" title="损失层"></a>损失层</h4><ul><li><p>损失函数：在深度学习中，损失反映模型最后预测结果与实际真值之间的差距，可以用来分析训练过程的好坏、模型是否收敛等，例如均方损失、交叉熵损失等。</p></li><li><p>损失层：设置一个损失函数用来比较网络的输出和目标值，通过最小化损失来驱动网络的训练</p></li><li><p>网络的损失通过前向操作计算，网络参数相对于损失函数的梯度则通过反向操作计算</p></li><li><p>分类问题损失（分类分割）</p><ul><li>nn.BCELoss；nn.CrossEntropyLoss等等</li></ul></li><li><p>回归问题损失（推测，回归）</p><ul><li>nn.L1Loss；nn.MSELoss；nn.SmoothL1Loss等等</li></ul></li></ul><h3 id="经典卷积神经网络结构"><a href="#经典卷积神经网络结构" class="headerlink" title="经典卷积神经网络结构"></a>经典卷积神经网络结构</h3><p><strong>堆叠，跳连，并连</strong> ，轻量型网络结构，多分支网络结构，attention网络结构</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/9.jpg" alt=""></p><h3 id="其他重要概念"><a href="#其他重要概念" class="headerlink" title="其他重要概念"></a>其他重要概念</h3><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><ul><li><p>学习率作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。</p></li><li><p>合适的学习率能够使目标函数在合适的时间内收敛到局部最小值</p></li><li><p>学习率大，震荡，恐怕到达不了最佳收敛值，学习率小收敛缓慢，消耗时间（如下图到最低点，学习率大小可以看作步长）</p></li><li><p>torch.optim.Ir scheduler</p><blockquote><p>ExponentialLR<br>ReduceLROnPlateau<br>CyclicLR<br>等等</p></blockquote></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/10.jpg" alt=""></p><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>反向传播优化参数</p><ul><li>GD、BGD、SGD、MBGD<ul><li>引入了随机性和噪声</li></ul></li><li>Momentum、NAG等<ul><li>加入动量原则，具有加速梯度下降的作用</li></ul></li><li>AdaGrad，RMSProp，Adam、AdaDelta<ul><li>自适应学习率</li></ul></li><li>torch.optim.Adam</li></ul><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul><li><p>模型出现过拟合现象时，降低模型复杂度</p></li><li><p>L1正则：参数绝对值的和</p></li><li>L2正则：参数的平方和（Pytorch自带，weight decay）</li><li>optimizer=torch.optim.SGD（model.parameters），Ir=0.01，weight_decay=0.001）</li></ul><h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><ul><li><p><strong>参数与超参数</strong></p><ul><li><p><strong>参数：</strong>模型f(x, θ)中的θ 称为模型的参数，可以通过优化算法进行学习。</p></li><li><p><strong>超参数：</strong>用来定义模型结构或优化策略。</p></li></ul></li><li><p><strong>batch_size 批处理</strong></p><ul><li>每次处理的数据数量。</li></ul></li><li><p><strong>epoch 轮次</strong></p><ul><li>把一个数据集，循环运行几轮。</li></ul></li><li><p><strong>transforms 变换</strong></p><ul><li>主要是将图片转换为tensor，旋转图片，以及正则化。</li></ul></li><li><p><strong>nomalize 正则化</strong></p><ul><li>模型出现过拟合现象时，降低模型复杂度</li></ul></li><li><p>前向传播</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/11.png" alt=""></p></li><li><p>反向传播</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/12.jpg" alt=""></p></li><li><p>梯度下降</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210828/13.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用激活函数（激励函数）理解和总结</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%EF%BC%89%E7%90%86%E8%A7%A3%E5%92%8C%E6%80%BB%E7%BB%93/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%EF%BC%89%E7%90%86%E8%A7%A3%E5%92%8C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近自己刚刚开始学习神经网络的相关知识，在学习构建一个自定义网络的时候，对于在forward函数中突然出现的Relu函数有点奇怪，然后就去百度查询了一波，原来这就是前面了解概念的时候所说的激活函数也就是激励函数，自己也是在百度和知乎上了解的更加透彻一点点，现在就自己的理解和参考一些别人的说法进行一定的总结，这次总结就主要是如下几点</p><ul><li>什么是激活函数</li><li>激活函数的作用（为什么就需要激活函数嘞）</li><li>有哪些常用的激活函数，都各自有什么性质和特点</li><li>在应用中该如何选择合适的激活函数</li></ul><h3 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h3><p>想要了解什么是激活函数，应该先了解神经网络的基本模型，单一的额神经元模型如图1所示。神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/1.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 1 单个神经元结构结构</center><h3 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h3><p>查阅了相关资料和学习，大家普遍对神经网络中的激活函数的额作用主要集中在下面这个观点</p><ul><li><strong>激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题。</strong></li></ul><p>下面举个例子，这个例子是我知乎上看到一位博主写的，个人觉得很不错，就搬过来了，在这里也加入了自己的思考，进一步理解。</p><p>首先我们现在有这么一个需求，就是二分类的问题，如果我要将下图的三角形和圆形进行正确的分类，也就是分隔开来，如图2所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/2.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 2</center><p>利用我们单层的感知机, 下图图3直线是由<img src="https://www.zhihu.com/equation?tex=w_%7B1%7Dx_%7B1%7D+%2B+w_%7B2%7Dx_%7B2%7D%2Bb%3D0+" alt="[公式]">得到，那么该感知器实现预测的功能步骤如下，就是我已经训练好了一个感知器模型，后面对于要预测的样本点，带入模型中，如果<img src="https://www.zhihu.com/equation?tex=y%3E0" alt="[公式]">,那么就说明是直线的右侧，也就是正类（我们定义是三角形），如果<img src="https://www.zhihu.com/equation?tex=y%3C0" alt="[公式]">,那么就说明是直线的左侧，也就是负类（我们定义是圆形)。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/3.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 3</center><p>利用我们上面单层的感知机, 用它可以任意划出一条线, 把平面分割成两部分，如图4所示，很容易能够看出，我给出的样本点根本不是线性可分的，一个感知器无论得到的直线怎么动，都不可能完全正确的将三角形与圆形区分出来，也就是说一条线性结构的直线是无法将三角形和长方形完全分割开来的。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/4.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 4</center><p>此刻我们很容易想到用多个感知器来进行组合（也就是可以多条线性的直线），以便获得更大的分类问题，好的，下面我们上图，看是否可行，如图5，</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/5.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 5</center><p>好的，我们已经得到了多感知器分类器了，那么它的分类能力是否强大到能将非线性数据点正确分类开呢~我们来分析一下：</p><p>我们能够得到</p><script type="math/tex; mode=display">y = W_{2-1}(w_{1-11}x_1+w_{1-21}x_2+b_{1-1}) +  W_{2-2}(w_{1-12}x_1+w_{1-22}x_2+b_{1-2}) + W_{2-3}(w_{1-13}x_1+w_{1-23}x_2+b_{1-3})</script><p>哎呀呀，不得了，这个式子看起来非常复杂，估计应该可以处理我上面的情况了吧，哈哈哈哈~不一定额，我们来给它变个形.上面公式合并同类项后等价于下面公式：</p><script type="math/tex; mode=display">y = x_1(w_{2-1}w_{1-11} + w_{2-2}w_{1-12} + w_{2-3}w_{1-13}) + x_2(w_{2-1}w_{1-21} + w_{2-2}w_{1-22} + w_{2-3}w_{1-23}) + w_{2-1}b_{1-1} + w_{2-2}b_{1-2} +  w_{2-3}b_{1-3}</script><p><strong>啧啧，估计大家都看出了，不管它怎么组合，最多就是线性方程的组合，最后得到的分类器本质还是一个线性方程，该处理不了的非线性问题，它还是处理不了。</strong></p><p><strong>就好像下图图6，直线无论在平面上如果旋转，都不可能完全正确的分开三角形和圆形点：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/6.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 6</center><p>既然是非线性问题，总有线性方程不能正确分类的地方，那么抛开神经网络中神经元需不需要激活函数这点不说，如果没有激活函数，仅仅是线性函数的组合解决的问题太有限了，碰到非线性问题就束手无策了.那么加入激活函数是否可能能够解决呢？</p><p>在上面线性方程的组合过程中（在加入阶跃激活函数的时候），我们其实类似在做三条直线的组合，如下图7：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/7.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 7</center><p>在这里就可以大声说出，激活函数就是来解决非线性因素的，没有太大的问题，就拿sigmoid例子说上面的场景，如图8sigmoid函数</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/8.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 8</center><p><strong>通过这个激活函数映射之后，输出很明显就是一个非线性函数！能不能解决一开始的非线性分类问题不清楚，但是至少说明有可能啊，上面不加入激活函数神经网络压根就不可能解决这个问题</strong></p><p>同理，扩展到多个神经元组合的情况时候，表达能力就会更强~对应的组合图9如下：（现在已经升级为三个非线性感知器在组合了）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/9.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 9</center><p>跟上面线性组合相对应的非线性组合如下，图10：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/10.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 10</center><p><strong>这看起来厉害多了，是不是~最后再通过最优化损失函数的做法，我们能够学习到不断学习靠近能够正确分类三角形和圆形点的曲线，到底会学到什么曲线，不知道到底具体的样子，也许是下面图11这个</strong>，那么随着不断训练优化，我们也就能够解决非线性的问题了。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/11.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 11</center><p><strong>所以到这里为止，我们就解释了这个观点，加入激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题。</strong></p><h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><ul><li><p><strong>Sigmoid函数</strong>，是比较常用的非线性激活函数</p><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script></li><li><p>值域：（0，1）；导数值域（0，0.25）</p></li><li><p>函数图像，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/12.jpg" alt=""></p></li><li><p>优点</p><ul><li>值域为(0，1），可以放到模型最后一层，作为模型的概率输出</li><li>特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1</li></ul></li><li><p>缺点</p><ul><li><p>在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。如果我们初始化神经网络的权值为 [ 0 , 1 ]  之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 ( 1 , + ∞ ) (1,+∞)(1,+∞) 区间内的值，则会出现梯度爆炸情况。</p></li><li><p>函数输出不是以0为中心的（不是zero-centered输出问题），这样会使权重更新效率降低。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：如</p><script type="math/tex; mode=display">f = w^Tx + b</script><p>那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。</p></li><li><p>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间</p></li></ul></li></ul></li><li><p><strong>tanh函数</strong></p><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script></li><li><p>值域：（-1，1）当|x|&gt;3时，函数容易饱和；导数值域（0，1）当|x|&gt;3时，梯度几乎为0</p></li><li><p>函数图像，如下图：（蓝色是Tanh原函数，紫色是导函数图像）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/13.jpg" alt=""></p></li><li><p>优点</p><ul><li>函数均值为0，一般作为图像生成的最后一层激活函数。</li><li>整个函数是以0为中心的，这个特点比sigmod的好，解决了Sigmoid函数的不是zero-centered输出问题</li></ul></li><li><p>缺点</p><ul><li>仍然存在梯度消失问题；涉及指数运算，复杂度高一些</li></ul></li></ul></li><li><p><strong>Relu函数</strong></p><ul><li><p>数学表达式</p><script type="math/tex; mode=display">\sigma(x) = max(0,x)</script></li><li><p>值域：当x<0时，函数值为0，当x>0时，函数值跟x线性增长; 当x<0时，导数值域：导函数值为0，当x>0时，导函数值为1</p></li><li><p>函数图像：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/14.jpg" alt=""></p></li><li><p>优点</p><ul><li>解决了gradient vanishing问题 (在正区间)</li><li>计算速度非常快，只需要判断输入是否大于0</li><li>收敛速度远快于sigmoid和tanh</li></ul></li><li><p>缺点</p><ul><li>当输入是负数的时候，ReLU是完全不被激活的，这就表明一旦输入到了负数，ReLU就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。</li><li>ReLU函数的输出要么是0，要么是正数，这也就是说，ReLU函数也不是以0为中心的函数。</li></ul></li></ul></li></ul><p>尽管存在这两个问题，<strong>ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！</strong></p><ul><li><p><strong>ELU函数</strong></p><ul><li><p>数学表达式</p><script type="math/tex; mode=display">f(n) = \begin{cases}x, & \text {x > 0}\\\alpha(e^x-1), & \text {otherwise}\end{cases}</script></li><li><p>函数图像：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/15.png" alt=""></p></li><li><p>ELU函数是针对ReLU函数的一个改进型，相比于ReLU函数，在输入为负数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。这样可以消除ReLU死掉的问题，不过还是有梯度饱和和指数运算的问题。不会有Dead ReLU问题和 输出的均值接近0，zero-centered。理论上虽然好于ReLU，但在实际使用中目前并没有好的证据证明ELU总是优于ReLU。</p></li></ul></li></ul><ul><li><p><strong>PReLU函数</strong></p><ul><li><p>函数表达式</p><script type="math/tex; mode=display">f = max(\alpha x,x)</script></li><li><p>函数图像：</p></li></ul></li></ul><pre><code>![](https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210824/16_1.png)</code></pre><ul><li><p>PReLU也是针对ReLU的一个改进型，在负数区域内，PReLU有一个很小的斜率，这样也可以避免ReLU死掉的问题。相比于ELU，PReLU在负数区域内是线性运算，斜率虽然小，但是不会趋于0，这算是一定的优势吧。</p><p>我们看PReLU的公式，里面的参数α一般是取0~1之间的数，而且一般还是比较小的，如零点零几。当α=0.01时，我们叫PReLU为Leaky ReLU，是PReLU的一种特殊情况吧</p></li></ul><ul><li><p><strong>Maxout函数</strong></p><ul><li>函数表达式：</li></ul><script type="math/tex; mode=display">\sigma(x) = max(W_1x+W_2x+b)</script></li></ul><ul><li>我们可以看到，Sigmoid函数实际上就是把数据映射到一个(−1,1)的空间上，也就是说，Sigmoid函数如果用来分类的话，只能进行二分类，而这里的softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。Softmax回归模型是logistic回归模型在多分类问题上的推广，在多分类问题中，待分类的类别数量大于2，且类别之间互斥。比如我们的网络要完成的功能是识别0-9这10个手写数字，若最后一层的输出为[0,1,0, 0, 0, 0, 0, 0, 0, 0]，则表明我们网络的识别结果为数字1。</li></ul><h3 id="应用中如何选择激活函数"><a href="#应用中如何选择激活函数" class="headerlink" title="应用中如何选择激活函数"></a>应用中如何选择激活函数</h3><ul><li>深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。</li><li>如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.</li><li>最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.</li><li>在较深层的神经网络中，选用relu激活函数能使梯度更好地传播回去，但当使用softmax作为最后一层的激活函数时，其前一层最好不要使用relu进行激活，而是使用tanh作为替代，否则最终的loss很可能变成Nan；</li><li>当选用高级激活函数时，建议的尝试顺序为ReLU-&gt;ELU-&gt;PReLU-&gt;MPELU，因为前两者没有超参数，而后两者需要自己调节参数使其更适应构建的网络结构。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 常用概念 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 激活函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络的来龙去脉</title>
      <link href="%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/"/>
      <url>%E4%BA%BA%E5%B7%A5%E5%8F%AA%E8%83%BD/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/</url>
      
        <content type="html"><![CDATA[<h2 id="神经网络的来龙去脉"><a href="#神经网络的来龙去脉" class="headerlink" title="神经网络的来龙去脉"></a>神经网络的来龙去脉</h2><p>神经，名呼其曰，就是动物的神经系统，从外界的条件触感和感知到大脑中枢的控制再到控制神经做出一系列的反应。</p><p>其实，在人工只能领域的神经网络而言，大部分的神经网络都可以用<strong>深度</strong> <strong>（depth）</strong>，和<strong>连接结构（connection）</strong>，但是具体的会具体说明。笼统的说，神经网络是可以分为有监督，无监督，半监督的神经网络，其实在这个分类下，忘忘也是你中有我我中有你的的一个局面，在学习的过程中有时候不必要去抠字眼。下面自己在浏览学习后，对神经网络的一点总结。</p><p>发展历程：</p><p>感知机 ==》多层感知机 ==》深度神经网络 ==》卷积神经网络</p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络即指人工神经网络，或称作连接模型，它是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。这种网络依靠系统的复杂程度，通过调整内部大量节点之间相互连接的关系，从而达到处理信息的目的。神经网络用到的算法是向量乘法，采用符号函数及其各种逼近。<strong>并行、容错、可以硬件实现以及自我学习特性</strong>，是神经网络的几个基本优点，也是神经网络计算方法与传统方法的<strong>区别所在</strong>。</p><h2 id="神经网络发展"><a href="#神经网络发展" class="headerlink" title="神经网络发展"></a>神经网络发展</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是Rosenblatt。但是，Rosenblatt的单层感知机有一个严重得不能再严重的问题，对于计算稍微复杂的函数其计算力显得无能为力。</p><h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人发明的多层感知机（multilayer perceptron)克服。多层感知机，顾名思义，就是有多个隐含层的感知机。</p><p>多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这就是我们现在所说的神经网络( NN)！多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。多层感知机给我们带来的启示是，<strong>神经网络的层数直接决定了它对现实的刻画能力——利用每层更少的神经元拟合更加复杂的函数。</strong></p><p>即便大牛们早就预料到神经网络需要变得更深，但是有一个梦魇总是萦绕左右。随着神经网络层数的加深，<strong>优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优</strong>。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，“<strong>梯度消失”现象更加严重</strong>。具体来说，我们常常使用 sigmoid 作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/575577-20190802141514879-1961811671.png" alt=""></p><center style="color:#000000;text-decoration:underline">图 1</center><p>传统意义上的多层神经网络包含三层：</p><ul><li>输入层</li><li>隐藏层</li><li>输出层</li></ul><p>其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适，多层神经网络做的步骤是：特征映射到值，特征是人工挑选。</p><h3 id="深度神经网络-（DNN）"><a href="#深度神经网络-（DNN）" class="headerlink" title="深度神经网络 （DNN）"></a>深度神经网络 （DNN）</h3><p>传统的人工神经网络（ANN）由三部分组成：输入层，隐藏层，输出层，这三部分各占一层。而深度神经网络的“深度”二字表示它的隐藏层大于2层，这使它有了更深的抽象和降维能力。</p><p>2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层(参考论文：Hinton G E, Salakhutdinov R R. Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006, 313(5786):504-507.)，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了 sigmoid，形成了如今 DNN 的基本形式。<strong>单从结构上来说，全连接的DNN和上图的多层感知机是没有任何区别的</strong>。值得一提的是，今年出现的高速公路网络（highway network）和深度残差学习（deep residual learning）进一步避免了梯度弥散问题，网络层数达到了前所未有的一百多层（深度残差学习：152层）</p><h3 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h3><p>如下图2所示，<strong>我们看到全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接</strong>，带来的潜在问题是<strong>参数数量的膨胀</strong>。假设输入的是一幅像素为1K<em>1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是<em>*通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。</em></em></p><p>对于图像，如果没有卷积操作，学习的参数量是灾难级的。CNN之所以用于图像识别，正是由于CNN模型限制了参数的个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被称为前向神经网络(Feed-forward Neural Networks)。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/e186f18d73fdafa8d4a5e75ed55ed4a3_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 2</center><p>下面，通过一个例子简单说明卷积神经网络的结构。假设如下图3，m-l 是输入层，我们需要识别一幅彩色图像，这幅图像具有四个通道ARGB（透明度和红绿蓝，对应了四幅相同大小的图像），假设卷积核大小为100 <em> 100，共使用100个卷积核w1到w100（从直觉来看，每个卷积核应该学习到不同的结构特征）。用w1在ARGB图像上进行卷积操作，可以得到隐含层的第一幅图像；这幅隐含层图像左上角第一个像素是四幅输入图像左上角100</em>100区域内像素的加权求和，以此类推。同理，算上其他卷积核，隐含层对应100幅“图像”。每幅图像对是对原始图像中不同特征的响应。按照这样的结构继续传递下去。CNN中还有max-pooling等操作进一步提高鲁棒性。</p><p>在这个例子里，我们注意到输入层到隐含层的参数瞬间降低到了100 <em> 100 </em> 100=10`6个！这使得我们能够用已有的训练数据得到良好的模型。题主所说的适用于图像识别，正是由于CNN模型限制参数了个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/440765dbaab356739fb855834f901e7d_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 3卷积神经网络隐含层（摘自Theano教程）</center><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/c71cd39abe8b0dd29e229f37058404da_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 4一个典型的卷积神经网络结构，注意到最后一层实际上是一个全连接层（摘自Theano教程）</center><p>典型的卷积神经网络由3部分构成：</p><ul><li><strong>卷积层</strong>：负责提取图像中的局部特征</li><li><strong>池化层</strong>：大幅降低参数量级(降维)</li><li><strong>全连接层</strong>：类似传统神经网络的部分，用来输出想要的结果。</li></ul><h4 id="1）卷积：提取特征"><a href="#1）卷积：提取特征" class="headerlink" title="1）卷积：提取特征"></a>1）卷积：提取特征</h4><p>卷积层的运算过程如下图，用一个卷积核扫完整张图片：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/v2-6428cf505ac1e9e1cf462e1ec8fe9a68_720w.gif" alt=""></p><p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p><p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/Inkedv2-08a3c438b08715ce15592c7bd0d923ae_720w_LI.jpg" alt=""></p><p><strong>总结：卷积层的通过卷积核的过滤提取出图片中局部的特征，跟上面提到的人类视觉的特征提取类似。</strong></p><h4 id="2）池化层（下采样）：数据降维，避免过拟合"><a href="#2）池化层（下采样）：数据降维，避免过拟合" class="headerlink" title="2）池化层（下采样）：数据降维，避免过拟合"></a>2）池化层（下采样）：数据降维，避免过拟合</h4><p>池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/v2-15e89ec6a866be1f7130655527079786_720w.gif" alt=""></p><p>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p><p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p><p><strong>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</strong></p><h4 id="3）全连接层：输出结果"><a href="#3）全连接层：输出结果" class="headerlink" title="3）全连接层：输出结果"></a>3）全连接层：输出结果</h4><p>这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。</p><p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/575577-20190802141514879-1961811671.png" alt=""></p><p>典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：</p><p><strong>卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/1628932130(1" alt="">.jpg)</p><h4 id="4）相关重点"><a href="#4）相关重点" class="headerlink" title="4）相关重点"></a>4）相关重点</h4><p><strong>1、卷积神经网络有2大特点</strong></p><ul><li>能够有效的将大数据量的图片降维成小数据量</li><li>能够有效的保留图片特征，符合图片处理的原则</li></ul><p><strong>2、卷积神经网络的擅长处理领域</strong></p><p>卷积神经网络 – 卷积神经网络最擅长的就是图片的处理</p><p><strong>3、卷积神经网络*解决了什么问题？*</strong></p><p>在卷积神经网络出现之前，图像对于人工智能来说是一个难题，有2个原因：</p><ul><li>图像需要处理的数据量太大，导致成本很高，效率很低</li><li>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高</li></ul><p><strong>A.需要处理的数据量太大</strong></p><p>图像是由像素构成的，每个像素又是由颜色构成的。现在随随便便一张图片都是 1000×1000 像素以上的， 每个像素都有RGB 3个参数来表示颜色信息。假如我们处理一张 1000×1000 像素的图片，我们就需要处理3百万个参数！</p><blockquote><p><strong>1000×1000×3=3,000,000</strong></p></blockquote><p>这么大量的数据处理起来是非常消耗资源的，而且这只是一张不算太大的图片！</p><p><strong>卷积神经网络 – CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。</strong></p><p><strong>更重要的是：我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。</strong></p><p><strong>B.保留图像特征</strong></p><p>假如一张图像中有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，<strong>图像的内容（本质）并没有发生变化，只是位置发生了变化</strong>。</p><p>所以当我们移动图像中的物体，用传统的方式的得出来的参数会差异很大！这是不符合图像处理的要求的。</p><p><strong>而 CNN 解决了这个问题，他用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。</strong></p><h3 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3><p>全连接的DNN还存在着另一个问题——无法对时间序列上的变化进行建模。然而，<strong>样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要</strong>。对了适应这种需求，就出现了题主所说的另一种神经网络结构——<strong>循环神经网络RNN。</strong></p><p>在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络（Feed-forward Neural Networks）。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！表示成图就是这样的</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/bef6a6073d311e79cad53eb47757af9d_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 5 RNN网络结构</center><p>我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/c2eb9099048761fd25f0e90aa66d363a_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 6 RNN在世间上展开</center><p>完美，<strong>（t+1）时刻网络的最终结果O（t+1）是该时刻输入和所有历史共同作用的结果！</strong>这就达到了对时间序列建模的目的。</p><p>不知题主是否发现，RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，<strong>“梯度消失”现象又要出现了，只不过这次发生在时间轴上</strong>。对于 t 时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。</p><p>为了解决时间上的梯度消失，机器学习领域发展出了<strong>长短时记忆单元LSTM，通过门的开关实现时间上记忆功能，并防止梯度消失</strong>，一个LSTM单元长这个样子：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210812/a8f4582707b70d41f250fdf0a43812fb_720w.jpg" alt=""></p><center style="color:#000000;text-decoration:underline">图 7 LSTM单元</center>]]></content>
      
      
      <categories>
          
          <category> 人工只能 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工只能 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>马在棋盘上的概率</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<h5 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h5><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/1.jpg" alt=""></p><p>大意的意思就是在一个棋盘上，马按照象棋中马走日的规则，可以选择走K此后，最后还是留在棋盘的概率。</p><h5 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h5><ul><li><p>首先一匹马在任意位置可以选择八个方向走动，称之为方向向量，分别为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int[] dx = &#123;-2, -1, 1, 2, 2, 1, -1, -2&#125;;    //方向导向数组</span><br><span class="line">int[] dy = &#123;1, 2, 2, 1, -1, -2, -2, -1&#125;;</span><br></pre></td></tr></table></figure></li><li><p>其次由于棋盘是有限的，所以如果选择的步伐超出了棋盘，则视为无效。</p></li><li><p>现在给个例子，模拟一下其的走动方位，N=4，k=3，r=0，c=0</p><ul><li><p>1、一开始的时候，这匹马所在的位置</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/2.jpg" alt=""></p></li><li><p>2、走第一步的时候后可以选择的落脚点：只有两个方向是可以选择的呢，其余的都是超出了棋盘的范围</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/1-2.jpg" alt=""></p><p>走完第一步后的矩阵如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/3.jpg" alt=""></p></li><li><p>3、仿照上述过程，第二步的走向如下</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/4.jpg" alt=""></p></li></ul></li></ul><pre><code>​     走完第二步后的矩阵如下：![](https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/5.jpg)​    </code></pre><ul><li><p>第三步可选的落脚点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/6.jpg" alt=""></p><p>走完第三步后的矩阵：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/7.jpg" alt=""></p></li><li><p>到这里，就已经完成了这匹马在棋盘上面的全部可能走法了，最后一个还有 2+2+6+6+2+2 = 20次还留着棋盘上，所以概率就为 20 / 8 <em> 8 </em> 8</p></li></ul><ul><li><p>状态转移方程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[r][c][steps]:表示马在位置（r，c）移动了 steps 次后还留在棋盘上的概率</span><br></pre></td></tr></table></figure><p>根据马的移动，得到如下递归方程 如下</p></li></ul><script type="math/tex; mode=display">dp[r][c][steps] = \sum_{dr，dc}dp[r+dr][c+dc][steps-1] / 8.0</script><p>dr，dc就是上面说的方向向量的数组，根据这个递归处理的方程，我们可以采取一个二维数组进行编写，即一新一旧，一步一步更新这些数组，最后求和即可</p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">//力扣上的主函数</span><br><span class="line">public double knightProbability(int N, int K, int r, int c) &#123;</span><br><span class="line">       double[][] dp_old = new double[N][N]; //dp_old数组，dp[x][y]表示第i次到达dp[x][y]的方案数</span><br><span class="line">       double[][] dp_new = new double[N][N]; //dp_new数组，dp[i][j]表示第i+1次到达dp[x][y]的方案数</span><br><span class="line">       //初始化</span><br><span class="line">       dp_old[r][c] = 1;   //一开始的位置</span><br><span class="line">       for(int i = 0; i &lt; K; i++)&#123; //K次</span><br><span class="line">           for (int x = 0; x &lt; N; x++) &#123;</span><br><span class="line">               for (int y = 0; y &lt; N; y++) &#123;</span><br><span class="line">                   //四面八方累加dp</span><br><span class="line">                   dp_new[x][y] = computeSumFromDirection(dp_old, x, y);</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           //更新两个数组</span><br><span class="line">           dp_old = dp_new;</span><br><span class="line">           dp_new = new double[N][N];</span><br><span class="line">       &#125;</span><br><span class="line">       //遍历这个数组的总和就是落在棋盘内所有格子的方案数</span><br><span class="line">       double in = 0;</span><br><span class="line">       for (int x = 0; x &lt; N; x++) &#123;</span><br><span class="line">           for (int y = 0; y &lt; N; y++) &#123;</span><br><span class="line">               in += dp_old[x][y];</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return in;</span><br><span class="line">   &#125;</span><br><span class="line">//每次的八个方向的走法，并且判断是否越界</span><br><span class="line">   public double computeSumFromDirection(double[][] dp_old, int x, int y)&#123;</span><br><span class="line">       double sum = 0;</span><br><span class="line">       int[] dx = &#123;-2, -1, 1, 2, 2, 1, -1, -2&#125;;    //方向导向数组</span><br><span class="line">       int[] dy = &#123;1, 2, 2, 1, -1, -2, -2, -1&#125;;</span><br><span class="line">       for (int i = 0; i &lt; dx.length; i++)&#123;</span><br><span class="line">           if(!check(dp_old, x + dx[i], y + dy[i])) continue;</span><br><span class="line">           sum += dp_old[x + dx[i]][y + dy[i]];</span><br><span class="line">       &#125;</span><br><span class="line">       return sum / 8.0;</span><br><span class="line">   &#125;</span><br><span class="line">//给定位置判断是否越界</span><br><span class="line">   public boolean check(double[][] dp_old, int x, int y)&#123;     //越界判断</span><br><span class="line">       return x &gt;= 0 &amp;&amp; x &lt; dp_old.length &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; dp_old[0].length;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>学习使我快乐！！！</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210524%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E7%9A%84%E6%A6%82%E7%8E%87/8.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最长递增子序列</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划—两道有趣的题目"><a href="#动态规划—两道有趣的题目" class="headerlink" title="动态规划—两道有趣的题目"></a>动态规划—两道有趣的题目</h2><h3 id="one：eetcode-300最长递增子序列"><a href="#one：eetcode-300最长递增子序列" class="headerlink" title="one：eetcode 300最长递增子序列"></a>one：eetcode 300最长递增子序列</h3><p><a href="https://leetcode-cn.com/problems/longest-increasing-subsequence/submissions/">https://leetcode-cn.com/problems/longest-increasing-subsequence/submissions/</a></p><h4 id="1、题目大意"><a href="#1、题目大意" class="headerlink" title="1、题目大意"></a>1、题目大意</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/1.jpg" alt=""></p><h4 id="2、分析"><a href="#2、分析" class="headerlink" title="2、分析"></a>2、分析</h4><p>看到题目，判断某一个数组的最长递增子序列，<strong>注意并不是连续的</strong>。</p><h4 id="3、方法1：完全递归"><a href="#3、方法1：完全递归" class="headerlink" title="3、方法1：完全递归"></a>3、方法1：完全递归</h4><ul><li><p>现在假设下标 <strong>i</strong> 结尾的数组的最唱递增子序列为 max，</p><p>若nums[i+1]&gt;nums[i] ; 则下标 <strong>i+1</strong> 结尾的数组的最唱递增子序列为 max+1，否则为 max</p></li><li><p>所以这个题目是可以拆解子问题的，有子问题最后堆砌到最终答案</p></li><li><p>设 函数  <strong>fun(n,nums)</strong>  : 表示在数组nums下，以n作为下标的最大递增序列</p><p>得到递归方程 ：<strong>fun(n,nums) = fun(j,nums)+1</strong> 其中 <strong>0&lt;=j&lt;i</strong> 并且 <strong>dp[j]&lt;dp[i] j为【0，i】</strong>里面的任意值，需要遍历</p></li><li><p>拿下图为递归树（简约哈）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/2.png" alt=""></p></li><li><p><strong>代码：</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">//方法一完全递归</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //直接遍历以每个下标的结尾的最大递增序列，再取其中的最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = fmax(ans,fun(i,nums));</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line">//定义递归函数</span><br><span class="line">//fun(n,nums) : 表示在数组nums下，以n作为下标的最大递增序列</span><br><span class="line">//fun(n,nums) = fun(j,nums)+1 其中 0&lt;=j&lt;i 并且 dp[j]&lt;dp[i] j为[0，i]里面的任意值，需要遍历</span><br><span class="line">int fun(int n,int* nums)&#123;</span><br><span class="line">    //最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    //递归出口,下标为0，即是返回1</span><br><span class="line">    if(n == 0)&#123;</span><br><span class="line">        ans = 1;</span><br><span class="line">        return ans;</span><br><span class="line">    &#125;</span><br><span class="line">    //按照递归方程开始，开始递归求解 </span><br><span class="line">    for(int i=0;i&lt;n;i++)&#123;</span><br><span class="line">        if(nums[i]&lt;nums[n])&#123;</span><br><span class="line">            ans = fmax(ans,fun(i,nums)+1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然，这样递归的结果就是，提交超时，hhhhhhh。但是没关系，思路对了，下面进行优化。</p><h4 id="4、方法2：记忆化加递归"><a href="#4、方法2：记忆化加递归" class="headerlink" title="4、方法2：记忆化加递归"></a>4、方法2：记忆化加递归</h4><ul><li><p>在方法一的基础上，我们可以记录一个记忆化的数组，在递归刚刚开始的时候去判断这个数组是否有值，有的话直接递归返回了，若没有，则进行递归</p></li><li><p>再拿上一张图片来看，比如递归到数字 2 的时候，我们记录好递归到数字 2 的记忆数组值，当下次在别的树枝上需要递归数字 2 的时候，便可以直接取了，而不用继续遍历了，效率会高很多</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/2.png" alt=""></p></li><li><p><strong>代码</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">//方法二，在方法1的基础上：变为 递归+记忆化</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //记忆化数组建立</span><br><span class="line">    int remenber[numsSize];</span><br><span class="line">    memset(remenber, -1,sizeof(int)* numsSize);</span><br><span class="line">    //直接遍历以每个下标的结尾的最大递增序列，再取其中的最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = fmax(ans,fun1(i,nums,remenber));</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line">//定义递归函数</span><br><span class="line">//fun1(n,nums) : 表示在数组nums下，以n作为下标的最大递增序列</span><br><span class="line">//fun1(n,nums) = fun1(j,nums)+1  其中 0&lt;j&lt;n</span><br><span class="line">int fun1(int n,int* nums,int *remenber)&#123;</span><br><span class="line">    //先判断记忆化数组里面是否有这个值，有直接返回，不用继续递归了</span><br><span class="line">    if(remenber[n]!=-1)return remenber[n];</span><br><span class="line">    //最大值</span><br><span class="line">    int ans = 1;</span><br><span class="line">    //递归出口,下标为0，即是返回1</span><br><span class="line">    if(n == 0)&#123;</span><br><span class="line">        ans = 1;</span><br><span class="line">        return ans;</span><br><span class="line">    &#125;</span><br><span class="line">    //按照递归方程开始，开始递归求解 </span><br><span class="line">    for(int i=0;i&lt;n;i++)&#123;</span><br><span class="line">        if(nums[i]&lt;nums[n])&#123;</span><br><span class="line">            ans = fmax(ans,fun1(i,nums,remenber)+1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //同时添加到记忆化数组</span><br><span class="line">    remenber[n] = ans;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这次提交，是可以通过的，足见时间效率上提高了可不少。</p><h4 id="5、方法3：动态规划"><a href="#5、方法3：动态规划" class="headerlink" title="5、方法3：动态规划"></a>5、方法3：动态规划</h4><ul><li><p>根据上述两个方法的分析的，可以很容易得到动态规划的状态转移方程</p></li><li><p>dp[i] : 表示以 i 为下标结尾的数组的最长递增字符串</p></li><li><p>状态转移方程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i] = max(dp[j]+1) 其中 0&lt;=j&lt;i 并且 dp[j]&lt;dp[i] j为[0，i]里面的任意值，需要遍历</span><br></pre></td></tr></table></figure></li><li><p><strong>代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">int lengthOfLIS(int* nums, int numsSize)&#123;</span><br><span class="line">    //最后的最大值</span><br><span class="line">    int ansMax = 1;</span><br><span class="line">    //1、建立dp数组</span><br><span class="line">    int dp[numsSize];</span><br><span class="line">    memset(dp, 0, sizeof(int) * numsSize);</span><br><span class="line">    //2、递归遍历，封装dp数组</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        //记录遍历j属于【0，i】里面的最大值。初始值为1，表示本身</span><br><span class="line">        int max = 1;</span><br><span class="line">        for (int j = 0; j &lt; i;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(nums[j]&lt;nums[i]) max = fmax(max, dp[j] + 1);</span><br><span class="line">        &#125;</span><br><span class="line">        //给dp赋值</span><br><span class="line">        dp[i] = max;</span><br><span class="line">        //每次比较，取最大</span><br><span class="line">        ansMax = dp[i] &gt; ansMax ? dp[i] : ansMax;</span><br><span class="line">    &#125;</span><br><span class="line">    return ansMax;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个是比较常规的方法了：时间复杂度是 <strong>O（N*N）</strong></p></li></ul><h4 id="6、方法4：贪心-二分查找"><a href="#6、方法4：贪心-二分查找" class="headerlink" title="6、方法4：贪心+二分查找"></a>6、方法4：贪心+二分查找</h4><ul><li>考虑一个简单的贪心，如果我们要使上升子序列尽可能的长，则我们需要让序列上升得尽可能慢，因此我们希望每次在上升子序列最后加上的那个数尽可能的小。</li><li>基于上面的贪心思路，我们维护一个数组 dp[i] ，表示长度为 <strong>i</strong> 的最长上升子序列的末尾元素的最小值，用 <strong>len</strong> 记录目前最长上升子序列的长度，起始时<strong>len =1，d[1]=nums[0]</strong>。</li></ul><ul><li><p>由定义知dp数组必然是一个递增数组,  对原数组<strong>nums</strong>进行迭代, 依次判断每个数<strong>num</strong>将其插入dp数组相应的位置:</p><ol><li><strong>num &gt; dp[len]</strong>, 表示num比所有已知递增序列的尾数都大, 将num添加入dp 数组尾部, 并将最长递增序列长度len加1</li><li><strong>dp[i-1] &lt; num &lt;= dp[i]</strong>, 只更新相应的dp[i]=num</li></ol></li><li><p>以 nums=[4,10,3,8,9]：</p><p>1)第一步插入4，则 dp=[4]</p><p>2) 第二步插入10，则dp=[4，10]</p><p>3) 第三步插入3，原数组4的位置更新为3 则dp=[4，10]==》dp=[3，10]</p><p>4) 第四步插入8，原数组10的位置更新为8 则dp=[4，10]==》dp=[3，8]</p><p>5) 第五步插入9，则dp=[3，8，9] </p><p>所以最后的答案为 <strong>len(dp) = 3</strong>;</p></li><li><p><strong>代码：</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int nums[] = &#123;10,9,2,5,3,7,101,18&#125;;</span><br><span class="line">    int numSize = sizeof(nums) / sizeof(nums[0]);</span><br><span class="line">    printf(&quot;%d&quot;,lengthOfLI3(nums,numSize));</span><br><span class="line">&#125;</span><br><span class="line">int lengthOfLI(int* nums, int numsSize)&#123;</span><br><span class="line">    //1、建立dp</span><br><span class="line">    int dp[numsSize];</span><br><span class="line">    memset(dp, 0, sizeof(int) * numsSize);</span><br><span class="line">    int index = 0;</span><br><span class="line">    for (int i = 0; i &lt; numsSize;i++)</span><br><span class="line">    &#123;   </span><br><span class="line">        //直接二分查找dp中的第一个大于等于nums[i]的值</span><br><span class="line">        int left = 0, right = index;</span><br><span class="line">        while(left&lt;right)&#123;</span><br><span class="line">            int mid = (left + right) / 2;</span><br><span class="line">            if(dp[mid]&lt;nums[i])left = mid + 1;</span><br><span class="line">            else right = mid;</span><br><span class="line">        &#125;</span><br><span class="line">        dp[left] = nums[i];</span><br><span class="line">        if(right == index) index++;</span><br><span class="line">    &#125;</span><br><span class="line">    return index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="two-：354-俄罗斯套娃"><a href="#two-：354-俄罗斯套娃" class="headerlink" title="two ：354 俄罗斯套娃"></a>two ：354 俄罗斯套娃</h3><p><a href="https://leetcode-cn.com/problems/russian-doll-envelopes/">https://leetcode-cn.com/problems/russian-doll-envelopes/</a></p><h4 id="1、题目大意-1"><a href="#1、题目大意-1" class="headerlink" title="1、题目大意"></a>1、题目大意</h4><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/3.jpg" alt=""></p><h4 id="2、分析-1"><a href="#2、分析-1" class="headerlink" title="2、分析"></a>2、分析</h4><ul><li><p>根据题目的要求，如果我们选择了 k 个信封，它们的</p></li><li><p>宽度依次为 w0, w1, ···, w k-1 高度依次为 h0, h1,···, h k-1 ，那么需要满足如下了两个条件：</p><script type="math/tex; mode=display">W0<W1<···<Wk-1</script><script type="math/tex; mode=display">h0<h1<···<hk-1</script><p>同时控制 w 和 h 两个维度并不是那么容易，因此我们考虑固定一个维度，再在另一个维度上进行选择。例如，我们固定 w 维度，那么我们将数组<strong>envelopes</strong> 中的所有信封按照 w 升序排序。这样一来，我们只要按照信封在数组中的出现顺序依次进行选取，就一定保证满足：</p><script type="math/tex; mode=display">W0≤W1≤···≤Wk-1</script></li><li><p>然而小于等于 ≤ 和小于 &lt;还是有区别的，但我们不妨首先考虑一个简化版本的问题：</p><p>如果我们保证所有信封的 w 值互不相同，那么我们可以设计出一种得到答案的方法吗？</p><p>在 w 值互不相同的前提下，小于等于≤ 和小于 &lt; 是等价的，那么我们在排序后，就可以完全忽略 w 维度，只需要考虑 h 维度了。此时，我们需要解决的问题即为：</p><p>给定一个序列，我们需要找到一个最长的子序列，使得这个子序列中的元素严格单调递增，即上面要求的：</p><script type="math/tex; mode=display">h0<h1<···<hk-1</script><p>那么这个问题就是经典的「最长严格递增子序列」问题，问题得到解决，</p></li><li><p>当我们解决了简化版本的问题之后，我们来想一想使用上面的方法解决原问题，会产生什么错误？当 w 值相同时，如果我们不规定 h 值的排序顺序，那么可能会有如下的情况：</p><p>排完序的结果为 [(w, h)] = [(1, 1), (1, 2), (1, 3), (1, 4)][(w,h)]=[(1,1),(1,2),(1,3),(1,4)]，由于这些信封的 w 值都相同，不存在一个信封可以装下另一个信封，那么我们只能在其中选择 1 个信封。然而如果我们完全忽略 w 维度，剩下的 h 维度为 [1, 2, 3, 4][1,2,3,4]，这是一个严格递增的序列，那么我们就可以选择所有的 4 个信封了，这就产生了错误。</p><p>因此，我们必须要保证对于每一种 w 值，我们最多只能选择 1 个信封。</p><p>我们可以将 h 值作为排序的第二关键字进行降序排序，这样一来，对于每一种 w 值，其对应的信封在排序后的数组中是按照 h 值递减的顺序出现的，那么这些 h 值不可能组成长度超过 1 的严格递增的序列，这就从根本上杜绝了错误的出现。</p></li><li><p>因此我们就可以得到解决本题需要的方法：</p><ul><li><p>首先我们将所有的信封按照 w 值第一关键字升序、h 值第二关键字降序进行排序；</p></li><li><p>随后我们就可以忽略 w 维度，求出 h 维度的最长严格递增子序列，其长度即为答案。</p></li></ul></li></ul><ul><li>至此分析完了，归根到底就是<strong>最长递增子序列</strong>的问题了</li></ul><h4 id="3、代码"><a href="#3、代码" class="headerlink" title="3、代码"></a>3、代码</h4><p><strong><em>直接show code no say say</em></strong></p><ul><li><p>常规动态规划</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int* p[4];</span><br><span class="line">    int points[4][4] = &#123;&#123;5,4&#125;,&#123;6,4&#125;,&#123;6,7&#125;,&#123;2,3&#125;&#125;;</span><br><span class="line">    p[0] = &amp;points[0][0];</span><br><span class="line">    p[1] = &amp;points[1][0];</span><br><span class="line">    p[2] = &amp;points[2][0];</span><br><span class="line">    p[3] = &amp;points[3][0];</span><br><span class="line">    int envelopesSize = 4;</span><br><span class="line">    int envelopesColSize[] = &#123;2,2,2,2&#125;;</span><br><span class="line">    int ans = maxEnvelopes1(p,envelopesSize,envelopesColSize);</span><br><span class="line">    printf(&quot;%d&quot;, ans);</span><br><span class="line">&#125;</span><br><span class="line">//方法一：普通动态规划</span><br><span class="line">//dp[i]：表示以下标i为结尾的最大增序列  再次遍历取其最大</span><br><span class="line">//状态转移方程 dp[i] = dp[j]+1 其中 j&lt;i 并且排好序的envelopes中envelopes[j][1]     &lt;envelopes[i][1] </span><br><span class="line">int maxEnvelopes(int** envelopes, int envelopesSize, int* envelopesColSize)&#123;</span><br><span class="line">    //1、先排序，首先按照第一列升序排序，若第一列的值相同，则按照第二列的值降序排序</span><br><span class="line">    qsort(envelopes, envelopesSize, sizeof(int*), compare);</span><br><span class="line">    //2、构建dp数组</span><br><span class="line">    int dp[envelopesSize+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0]) * (envelopesSize + 1));</span><br><span class="line">    //3、根据状态转移方程，递推求dp</span><br><span class="line">    for (int i = 1; i &lt;= envelopesSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        //遍历【1，i】位置 找符合条件的最大dp[j]+1</span><br><span class="line">        int max = 1;</span><br><span class="line">        for (int j = 1; j &lt; i;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(envelopes[i-1][1]&gt;envelopes[j-1][1]) max = fmax(max, dp[j] + 1);</span><br><span class="line">        &#125;</span><br><span class="line">        //赋值给dp</span><br><span class="line">        dp[i] = max;</span><br><span class="line">    &#125;</span><br><span class="line">    //4、取其最大</span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 0; i &lt; sizeof(dp) / sizeof(dp[0]); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = dp[i] &gt; ans ? dp[i] : ans;</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//第一个参数升序 第二个参数降序</span><br><span class="line">int compare(const void *a, const void *b)&#123;</span><br><span class="line">    int* num1 = *(int**)a;</span><br><span class="line">    int* num2 = *(int**)b;</span><br><span class="line">    if(num1[0]==num2[0]) return num2[1] - num1[1];</span><br><span class="line"> else  return num1[0] - num2[0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>基于二分查找的动态规划</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;</span><br><span class="line">    int* p[4];</span><br><span class="line">    int points[4][4] = &#123;&#123;5,4&#125;,&#123;6,4&#125;,&#123;6,7&#125;,&#123;2,3&#125;&#125;;</span><br><span class="line">    p[0] = &amp;points[0][0];</span><br><span class="line">    p[1] = &amp;points[1][0];</span><br><span class="line">    p[2] = &amp;points[2][0];</span><br><span class="line">    p[3] = &amp;points[3][0];</span><br><span class="line">    int envelopesSize = 4;</span><br><span class="line">    int envelopesColSize[] = &#123;2,2,2,2&#125;;</span><br><span class="line">    int ans = maxEnvelopes1(p,envelopesSize,envelopesColSize);</span><br><span class="line">    printf(&quot;%d&quot;, ans);</span><br><span class="line">&#125;</span><br><span class="line">//方法二：基于二分查找的动态规划</span><br><span class="line">int maxEnvelopes1(int** envelopes, int envelopesSize, int* envelopesColSize) &#123;</span><br><span class="line">    if (envelopesSize == 0)  return 0;</span><br><span class="line">    //1、先排序，首先按照第一列升序排序，若第一列的值相同，则按照第二列的值降序排序</span><br><span class="line">    qsort(envelopes, envelopesSize, sizeof(int*), compare);</span><br><span class="line">    //2、构建dp数组</span><br><span class="line">    int dp[envelopesSize], indexSize = 0;</span><br><span class="line">    dp[indexSize++] = envelopes[0][1];</span><br><span class="line">    for (int i = 1; i &lt; envelopesSize; ++i) &#123;</span><br><span class="line">        int num = envelopes[i][1];</span><br><span class="line">        if (num &gt; dp[indexSize - 1])  dp[indexSize++] = num;</span><br><span class="line">        else &#123;</span><br><span class="line">            //在dp中寻找第一个大于等于num的值的下标，进而替换她</span><br><span class="line">            int index = lower_bound(dp, indexSize, num);</span><br><span class="line">            dp[index] = num;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return indexSize;</span><br><span class="line">&#125;</span><br><span class="line">//二分查找</span><br><span class="line">int lower_bound(int* arr, int arrSize, int val) </span><br><span class="line">&#123;</span><br><span class="line">    int left = 0, right = arrSize - 1;</span><br><span class="line">    while (left &lt;= right) &#123;</span><br><span class="line">        int mid = (left + right) &gt;&gt; 1;</span><br><span class="line">        if (val &lt; arr[mid])  right = mid - 1;</span><br><span class="line">        else if (val &gt; arr[mid]) left = mid + 1;</span><br><span class="line">        else  return mid;</span><br><span class="line">    &#125;</span><br><span class="line">    if (arr[left] &gt;= val)  return left;</span><br><span class="line">    return -1;</span><br><span class="line">&#125;</span><br><span class="line">//第一个参数升序 第二个参数降序</span><br><span class="line">int compare(const void *a, const void *b)&#123;</span><br><span class="line">    int* num1 = *(int**)a;</span><br><span class="line">    int* num2 = *(int**)b;</span><br><span class="line">    if(num1[0]==num2[0])&#123;</span><br><span class="line">        return num2[1] - num1[1];</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return num1[0] - num2[0];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="summary："><a href="#summary：" class="headerlink" title="summary："></a>summary：</h3><p>套路：遇到这种数组这种问题，经常想到以 <strong>下标i</strong> 为结尾作为的子问题，直接定义 <strong>dp【i】</strong>：为以 <strong>i</strong> 作为下标结尾的数组怎么怎么。。。。 </p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/20210523%E5%AD%90%E5%BA%8F%E5%88%97/4.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
            <tag> 子序列问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BFS和DFS模板</title>
      <link href="%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/BFS%E5%92%8CDFS%E6%A8%A1%E6%9D%BF/"/>
      <url>%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/BFS%E5%92%8CDFS%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<p><strong>首先，总结一定的模板做题是十分有作用的，善于总结才是我们加强算法能力的表现。做总结可以提高我们的代码能力，可以比较快速解决算法问题，也会更加清晰算法的流程！！十分有必要！！</strong></p><p><strong>BFS的模板：</strong></p><ul><li>1、如果不需要确定当前遍历到了哪一层，BFS 模板如下。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while queue 不空：</span><br><span class="line">    cur = queue.pop()</span><br><span class="line">    for 节点 in cur的所有相邻节点：</span><br><span class="line">        if 该节点有效且未访问过：</span><br><span class="line">            queue.push(该节点)</span><br></pre></td></tr></table></figure><ul><li>2、如果要确定当前遍历到了哪一层，BFS 模板如下。 这里增加了 level 表示当前遍历到二叉树中的哪一层了，也可以理解为在一个图中，现在已经走了多少步了。size 表示在当前遍历层有多少个元素，也就是队列中的元素数，我们把这些元素一次性遍历完，即把当前层的所有元素都向外走了一步。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">level = 0</span><br><span class="line">while queue 不空：</span><br><span class="line">    size = queue.size()</span><br><span class="line">    while (size --) &#123;</span><br><span class="line">        cur = queue.pop()</span><br><span class="line">        for 节点 in cur的所有相邻节点：</span><br><span class="line">            if 该节点有效且未被访问过：</span><br><span class="line">                queue.push(该节点)</span><br><span class="line">    &#125;</span><br><span class="line">    level ++;</span><br></pre></td></tr></table></figure><p><strong>DFS模板（回溯）</strong></p><ul><li>1、最本质的法宝是“画图”，千万不能偷懒，拿纸和笔“画图”能帮助我们更好地分析递归结构，这个“递归结构”一般是“树形结构”，而符合题意的解正是在这个“树形结构”上进行一次“深度优先遍历”，这个过程有一个形象的名字，叫“搜索”；我们写代码也几乎是“看图写代码”，所以“画树形图”很重要。</li><li>2、然后使用一个状态变量，一般我习惯命名为 path、pre ，在这个“树形结构”上使用“深度优先遍历”，根据题目需要在适当的时候把符合条件的“状态”的值加入结果集；这个“状态”可能在叶子结点，也可能在中间的结点，也可能是到某一个结点所走过的路径。</li><li>3、在某一个结点有多个路径可以走的时候，使用循环结构。当程序递归到底返回到原来执行的结点时，“状态”以及与“状态”相关的变量需要“重置”成第 1 次走到这个结点的状态，这个操作有个形象的名字，叫“回溯”，“回溯”有“恢复现场”的意思：意即“回到当时的场景，已经走过了一条路，尝试走下一条路”。第 2 点中提到的状态通常是一个列表结构，因为一层一层递归下去，需要在列表的末尾追加，而返回到上一层递归结构，需要“状态重置”，因此要把列表的末尾的元素移除，符合这个性质的列表结构就是“栈”（只在一头操作）。</li><li>4、当我们明确知道一条路走不通的时候，例如通过一些逻辑计算可以推测某一个分支不能搜索到符合题意的结果，可以在循环中 continue 掉，这一步操作叫“剪枝”。“剪枝”的意义在于让程序尽量不要执行到更深的递归结构中，而又不遗漏符合题意的解。因为搜索的时间复杂度很高，“剪枝”操作得好的话，能大大提高程序的执行效率。“剪枝”通常需要对待搜索的对象做一些预处理，例如第 47 题、第 39 题、第 40 题、第 90 题需要对数组排序。“剪枝”操作也是这一类问题很难的地方，有一定技巧性。总结一下：“回溯” = “深度优先遍历” + “状态重置” + “剪枝”，写好“回溯”的前提是“画图”。因此，非要写一个模板，我想它可能长这个样子：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def backtrack(待搜索的集合, 递归到第几层, 状态变量 1, 状态变量 2, 结果集):</span><br><span class="line">    # 写递归函数都是这个套路：先写递归终止条件</span><br><span class="line">    if 可能是层数够深了:</span><br><span class="line">        # 打印或者把当前状态添加到结果集中</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    for 可以执行的分支路径 do           //分支路径</span><br><span class="line">        </span><br><span class="line">        # 剪枝</span><br><span class="line">        if 递归到第几层, 状态变量 1, 状态变量 2, 符合一定的剪枝条件:</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line">        对状态变量状态变量 1, 状态变量 2 的操作（#）</span><br><span class="line">   </span><br><span class="line">        # 递归执行下一层的逻辑</span><br><span class="line">        backtrack(待搜索的集合, 递归到第几层, 状态变量 1, 状态变量 2, 结果集)</span><br><span class="line"></span><br><span class="line">        对状态变量状态变量 1, 状态变量 2 的操作（与标注了 # 的那一行对称，称为状态重置）</span><br><span class="line">        </span><br><span class="line">    end for</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小tips </tag>
            
            <tag> DFS </tag>
            
            <tag> BFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>背包问题</title>
      <link href="%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
      <url>%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划（Dynamic Programming，简称DP）动态规划常常适用于有<strong>重叠子问题</strong>和<strong>最优子结构</strong>性质的问题，动态规划方法所耗时间往往远少于朴素解法。</p><p>动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。 通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量： 一旦某个给定子问题的解已经算出，则将其<strong>记忆化</strong>存储，以便下次需要同一个子问题解之时直接查表。 这种做法在重复子问题的数目关于输入的规模呈<strong>指数增长</strong>时特别有用。虽然抽象后进行求解的思路并不复杂，但具体的形式千差万别，找出问题的子结构以及通过子结构重新构造最优解的过程很难统一，为了解决动态规划问题，只能靠多练习、多思考了。</p><p><strong><em>\</em>动态规划问题满足三大重要性质**</strong></p><p><strong>最优子结构性质：</strong>如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态规划算法解决问题提供了重要线索。</p><p><strong>子问题重叠性质：</strong>子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。</p><p><strong>无后效性</strong>：将各阶段按照一定的次序排列好之后，对于某个给定的阶段状态，它以前各阶段的状态无法直接影响它未来的决策，而只能通过当前的这个状态。换句话说，每个状态都是过去历史的一个完整总结。这就是无后向性，又称为无后效性。</p><p><strong><em>重点</em></strong>   dp数组的含义 + 状态转移方程 （具体问题具体分析）</p><p><strong>首先背包问题是我们接触动态规划比不可取的经典问题，重要的问题说三遍，经典经典经典。</strong></p><ul><li>0-1背包问题</li><li>完全背包问题</li><li>多重背包问题</li></ul><h2 id="0-1背包问题"><a href="#0-1背包问题" class="headerlink" title="0-1背包问题"></a>0-1背包问题</h2><h3 id="1、题目描述"><a href="#1、题目描述" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p>​    这里你有N件物品和一个容量为M的背包，这N件物品的第 i 件物品的 价值是V[i] ，重量是W[i].问题是拿取这N件物品的哪几件时，使得背包可以装下（<u>意思就是物品的重量总和小于或等于M</u>）且价值最大。</p><p><strong>关键问题</strong>：其实这堆物品在你选择的时候无非就是两种路子可以选择：<strong>选 or 不选</strong></p><p>第一步：构建dp数组的含义，<code>dp[i][j]</code>  ：代表的是前 <strong>i</strong> 个物品加入容量为 <strong>j</strong> 的背包里面价值总和的最大值</p><p>第二步：分析状态转移方程</p><p>​    对于一个物品来说：要么选要么不选，</p><ul><li><p>选择这个物品：就是第<strong>i</strong>件物品放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j-w[i]] + V[i]   （j&gt;W[i]）</span><br></pre></td></tr></table></figure></li><li><p>不选择这个物品：就是舍弃第<strong>i</strong>件物品，不放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j]</span><br></pre></td></tr></table></figure><p>经过这两步的分析，可以得出这个问题的状态转移方程（即为重要，very very important）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i-1][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 4; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,2,4,3,7&#125;; //物品的价值 前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,5,5&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(j&lt;W[i])&#123;</span><br><span class="line">                dp[i][j] = dp[i - 1][j];  //装不下第i件物品，只能不要咯</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                dp[i][j] = fmax(dp[i - 1][j], dp[i - 1][j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、深入分析理解"><a href="#3、深入分析理解" class="headerlink" title="3、深入分析理解"></a>3、深入分析理解</h3><p>这个代码就是自下而上的方法，思路也是比较简单，就是不断遍历，不断填充dp表：</p><p>第一：初始化时候的表格：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti1.jpg" alt=""></p><p>第二：当 <strong>i</strong>=1的时候，只有物品1能够选择，如果背白容量够的话，那么此时的最大价值就是物品1的价值了</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti2.jpg" alt=""></p><p>第三：当<strong>i</strong>=2的时候，根据状态转移方程，此时取<strong>i</strong>=2，<strong>j</strong>=3的时候有如下转换：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti3.jpg" alt=""></p><p>最后，根据这样的规则：逐一填表得：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti4.jpg" alt=""></p><p>这样，就可以得到最后的结果：13了，我们也可以根据状态转移方程方向得到选择的物品是第1 2 4号物品。</p><p>到此，分析时间复杂度为填表的时间为<strong>O(N*M)</strong>  ， 空间复杂度为<strong>O(N*M)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti5.jpg" alt=""></p><h3 id="4、优化"><a href="#4、优化" class="headerlink" title="4、优化"></a>4、优化</h3><p>在这个问题上，其实时间上没什么好优化的了，只能从空间上进行一点优化 ，</p><p>首先我们再看看状态转移方程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i-1][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti6.jpg" alt=""></p><p>可以明显看出，在填<strong>i+1</strong>行的数据的时候，只用到了第<strong>i</strong>行的数据，根本就没有用到<strong>i-1</strong>行的数据，换句话说，填某一行的数据的时候只与其前一行有关，根据这个规律，我们就可以使用将二维dp降为一维dp，缩减空间。此所谓滚动数组。</p><p><strong>总结</strong>：<code>dp[i][j]</code>所依赖的值必须是没有更新的，所以后到前。（ps：完全背包正好相反）</p><p>此时状态转移方程  <code>dp[j]</code> : 表示容量不超过 <strong>j</strong> 的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[j] = Max(dp[j],dp[j-w[i]] + V[i])  j&gt;W[i]</span><br></pre></td></tr></table></figure><p><strong>代码实现：</strong></p><p>和上面的代码有一点区别，在填充dp数组的第二层循环的时候，不应该从前到后（左到右），而应该从后到前（右到左），因为如果选择从前到后（左到右），会导致前面的值被修改，而后面的的值确实依赖前面的值的，要保证后面值得依赖是不变了。所以在第二轮扫描得时候需要从后到前扫描。（下图看做一行滴数据哈）</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti7.jpg" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 4; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,2,4,3,7&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,5,5&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化得</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = M; j &gt;= W[i]; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时时间复杂度 <strong>O(N*M）</strong>    空间复杂度<strong>O(M)</strong>   空间复杂度优化了挺多哦</p><h2 id="完全背包问题"><a href="#完全背包问题" class="headerlink" title="完全背包问题"></a>完全背包问题</h2><h3 id="1、题目描述-1"><a href="#1、题目描述-1" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p> 有N种物品和一个容量为M的背包，每种物品都就可以选择任意多个，第<strong>i</strong>种物品的价值为 V[i]，重量是 W[i]，求解：选哪些物品放入背包，可因使得这些物品的价值最大，并且体积总和不超过背包容量。</p><p><strong>分析：</strong></p><p>完全背包问题是在0-1背包问题的基础上略有不同，不同的是在0-1背包问题中，某一件物品要么取一件要么不取，但是在完全背包的问题中，某一件物品可以无限（任意）的取。</p><p>从物品的选择角度说也不是 <strong>选 OR 不选 </strong>的问题了，而是选 0 1 2 3 4 ，，，件的问题了，</p><p>默默嘀咕：曾经的我刚刚 接触的时候，贪心（有手就行），事实证明我还是年轻，打扰了，贪心解决不了。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti8.jpg" alt=""></p><p><strong>动态规划方法：</strong></p><p>第一步：构建dp数组的含义，<code>dp[i][j]</code>  ：代表的是前 <strong>i</strong> 个物品加入容量为 <strong>j</strong> 的背包里面价值总和的最大值</p><p>第二步：分析状态转移方程</p><p>​    对于一件新的物品来说：可以选可以不选，</p><ul><li><p>选择这个新物品：此时（拿了 <strong>i</strong> 号物品，我们还是继续拿 <strong>i</strong> 号物品）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i][j-w[i]] + V[i]   （j&gt;W[i]）</span><br></pre></td></tr></table></figure></li><li><p>不选择这个物品：就是舍弃全部的第 <strong>i</strong> 件物品，不放入背包中，此时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = dp[i-1][j]</span><br></pre></td></tr></table></figure><p>经过这两步的分析，可以得出这个问题的状态转移方程（即为重要，very very important）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = Max(dp[i-1][j],dp[i][j-w[i]] + V[i])</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、代码实现-1"><a href="#2、代码实现-1" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,5,8&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//没有优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(j&lt;W[i])&#123;</span><br><span class="line">                dp[i][j] = dp[i - 1][j];  //装不下第i件物品，只能不要咯</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                //可以装下，可以选择 拿或者不拿  只是将 0-1背包问题中的 i-1 改为 i</span><br><span class="line">                dp[i][j] = fmax(dp[i - 1][j], dp[i][j - W[i]] + V[i]); </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注解</strong> ： 其实这边也还可以利用另外一个状态转移方程解决（自个摸索把）直接给出答案</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">f[i][j]=max(f[i-1][j-k*V[i]]+k*W[i],f[i][j])     0&lt;=k*c[i]&lt;=j</span><br><span class="line"></span><br><span class="line">代码：</span><br><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;5,8&#125;; //物品的价值</span><br><span class="line">    int W[] = &#123;5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//f[i][j]=max(f[i-1][j-k*V[i]]+k*W[i],f[i][j])     0&lt;=k*c[i]&lt;=j</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">            for (int j = 0; j &lt;= M; j++)&#123;</span><br><span class="line">                for (int k = 0; k * V[i] &lt;= j; k++)&#123;</span><br><span class="line">                    dp[i+1][j] = fmax(dp[i+1][j], dp[i][j-k * V[i]] + k * W[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、深入分析理解-1"><a href="#3、深入分析理解-1" class="headerlink" title="3、深入分析理解"></a>3、深入分析理解</h3><p>这边也可以画出表格来一步一步填这个表格的问题，自底向上，</p><p>第一步：初始化时的表格：</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti9.jpg" alt=""></p><p>第二步：在第 <strong>i</strong> 个物品的时候，我们其实可以选择上一层中的几个位置中价值最高的那一个，在这里M=10，所以只需要将两个数值进行比较，如果M大于10，那么就需要将取0个、1个和两个i2物品的情况进行比较，然后选出最大值.</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti10.jpg" alt=""></p><p>到此，分析时间复杂度为填表的时间为<strong>O(N*M)</strong>  ， 空间复杂度为<strong>O(N*M)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti11.jpg" alt=""></p><h3 id="4、优化-1"><a href="#4、优化-1" class="headerlink" title="4、优化"></a>4、优化</h3><p>优化思路和0-1背包问题一模一样的，就不在这里赘述了，直接上状态方程和代码。</p><p><strong>总结</strong>：<code>dp[i][j]</code>所依赖的值必须是已经更新的，所以前到后。（ps：0-1背包正好相反）</p><p><strong>状态转移方程为</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f[j]=max(f[j-w[i]]+c[i], f[j]);</span><br></pre></td></tr></table></figure><p><strong>代码实现</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 2; //物品的数量</span><br><span class="line">    int M = 10; //背包的容量</span><br><span class="line">    int V[] = &#123;0,5,8&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,5,7&#125;; //物品的重量</span><br><span class="line">    int re = seekMax1(N, M, V, W);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化得</span><br><span class="line">int seekMax(int N,int M,int V[],int W[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = W[i]; j &lt;= M; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - W[i]] + V[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时时间复杂度 <strong>O(N*M）</strong>    空间复杂度<strong>O(M)</strong>   空间复杂度也是变成线性的了。</p><p><strong>注解</strong>：其实还有一个小优化</p><p>比如两件物品 ：<strong>i</strong>   <strong>j</strong>    当  <strong>i</strong> 物品的重量比 <strong>j</strong> 的重，但是 <strong>i</strong> 的价值确比 <strong>j</strong> 的低，那我们岂不是可以直接跳过 <strong>i</strong> 了，直接选择 <strong>j</strong> 物品了。道理很简单，难道这世界上会有人去买一个又贵又难吃的东西？（富豪除外）</p><h2 id="多重背包问题"><a href="#多重背包问题" class="headerlink" title="多重背包问题"></a>多重背包问题</h2><h3 id="1、题目描述-2"><a href="#1、题目描述-2" class="headerlink" title="1、题目描述"></a>1、题目描述</h3><p>有N种物品和一个容量为V的背包。第i种物品最多有n[i]件可用，每件费用是w[i]，价值是c[i]。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。</p><p><strong>分析</strong></p><p>这里既不像0-1背包每种物品只有1件，也不像完全背包那样每种物品有无数件，而是限定来了每种物品的数量，并不是你想取多少就取多少，得看看人家有没有。</p><p>经过前面两个的分析，这个多重背包问题得状态转移方程和完全背包的状态转移方程岂不是一个爹娘的样子，<strong>就是K多了一个限制</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i,j] = max(dp(i-1, j - V[i] * k) + P[i] * k); （0 &lt;= k * V[i] &lt;= j &amp;&amp; 0 &lt;= k &lt;= n[i]）</span><br></pre></td></tr></table></figure><h3 id="2、代码实现-2"><a href="#2、代码实现-2" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[],int n[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 3; //不同物品的数量</span><br><span class="line">    int M = 15; //背包的容量</span><br><span class="line">    int V[] = &#123;3,4,5&#125;; //物品的价值</span><br><span class="line">    int W[] = &#123;2,3,4&#125;; //物品的重量</span><br><span class="line">    int n[] = &#123;4, 3, 2&#125;;//每种物品的个数</span><br><span class="line">    int re = seekMax(N, M, V, W,n);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//没有优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[],int n[])</span><br><span class="line">&#123;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[N+1][M+1];</span><br><span class="line">    memset(dp, 0, sizeof(dp[0][0]) * (N + 1) * (M + 1)); //先全部置为0，因为c中会给随机值，很烦</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][0] = 0;  //背包容量为0，你拿不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">     for (int j = 0; j &lt; N ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[0][j] = 0;  //你不拿物品价值最大都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 0; i &lt; N; i++)&#123;</span><br><span class="line">            for (int j = 0; j &lt;= M; j++)&#123;</span><br><span class="line">                //k限制了条件 不加限制就是我上面讲的完全背包问题中在我没有优化的时候提出的另外一个方程的解</span><br><span class="line">                for (int k = 0; k &lt;= n[i] &amp;&amp; k * V[i] &lt;= j; k++)&#123;  </span><br><span class="line">                    dp[i+1][j] = fmax(dp[i+1][j], dp[i][j-k * V[i]] + k * W[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return dp[N][M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分析就不再分析了，和前面的一模一样的</p><h3 id="3、优化"><a href="#3、优化" class="headerlink" title="3、优化"></a>3、优化</h3><ul><li>优化1：这边优化也可以项0-1背包和完全背包的样子，将dp二维数组改为一维的dp数组，改为滚动数组，这里不再赘述</li><li>优化2：这里有一个比较巧妙的方法，完美将多重背包问题顺利转为0-1背包的问题了。下面会详细讲解这个优化（鄙人比较pick）</li></ul><p>举个例子：比如有一种物品，她一共有8件，我们再取的时候可以取得 0 1 2 3 4 5 6 7 8 件这九种情况，但是我们在取得时候是不知道该取多少件得，这时候我们可以把这八件物品分堆，使得我们可以取得上述得九种情况的任意一种，所以分堆便是重点了，这里分堆采用2进制的方法进行分堆</p><p><strong>统一</strong>：分为 1 2 4 8 。。。总的减去前面的总和（因为最后一个并不一定是2的整数次幂）</p><script type="math/tex; mode=display">n = 2^0 + 2^1 + 2^2 + 2^3...+ 2^h+(n-2^c+1)       （其中 h=c-1）</script><p>例如八件同一件物品：分为大小为 1 2 4 1 的四个堆即可，任意组合可以得到上述选择的九种可能，此时将这四个堆想象成0-1背包问题种的不一样的物品即可。 </p><p>状态转移方程和0-1背包相同</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;math.h&quot;</span><br><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">int seekMax(int N, int M, int V[], int W[],int n[]);</span><br><span class="line">int main()&#123;</span><br><span class="line">    int N = 3; //不同物品的数量</span><br><span class="line">    int M = 15; //背包的容量</span><br><span class="line">    int V[] = &#123;0,3,4,5&#125;; //物品的价值  前面的0就是占位的，方便遍历</span><br><span class="line">    int W[] = &#123;0,2,3,4&#125;; //物品的重量</span><br><span class="line">    int n[] = &#123;0,4,3,2&#125;;//每种物品的个数</span><br><span class="line">    int re = seekMax(N, M, V, W,n);</span><br><span class="line">    printf(&quot;%d&quot;, re);</span><br><span class="line">&#125;</span><br><span class="line">//优化的</span><br><span class="line">int seekMax(int N,int M,int V[],int W[],int n[])</span><br><span class="line">&#123;</span><br><span class="line">    //创建分堆后的价值和重量数组 这个大小可以根据题目给的数据范围来确定</span><br><span class="line">    int newW[N * 20];</span><br><span class="line">    int newV[M * 20]; </span><br><span class="line">    newW[0] = 0;</span><br><span class="line">    newV[0] = 0;</span><br><span class="line">    //先分堆 完善上面两个数组</span><br><span class="line">    int number = 0; //分堆后的总堆数</span><br><span class="line">    for (int i = 1; i &lt;= N;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 1; j &lt;= n[i];j *= 2)</span><br><span class="line">        &#123;</span><br><span class="line">            number++;</span><br><span class="line">            newW[number] = W[i] * j;</span><br><span class="line">            newV[number] = V[i] * j;</span><br><span class="line">            n[i] -= j;</span><br><span class="line">        &#125;</span><br><span class="line">        //最后那个</span><br><span class="line">        if(n[i]&gt;0)&#123;</span><br><span class="line">            number++;</span><br><span class="line">            newW[number] = W[i] * n[i];</span><br><span class="line">            newV[number] = V[i] * n[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //1、创建dp数组</span><br><span class="line">    int dp[M+1];</span><br><span class="line">    //2、初始化dp数组</span><br><span class="line">     for (int j = 0; j &lt;= M  ; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[j] = 0;  //初始全部都为0</span><br><span class="line">    &#125;</span><br><span class="line">    //3、开始根据状态转移方程地推填满dp数组</span><br><span class="line">    for (int i = 1; i &lt;= number; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = M; j &gt;= newW[i]; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[j] = fmax(dp[j], dp[j - newW[i]] + newV[i]); //可以装下，可以选择 拿或者不拿</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return dp[M];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此三个经典的背包问题解决啦。</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti13.jpg" alt=""></p><h2 id="混合背包问题"><a href="#混合背包问题" class="headerlink" title="混合背包问题"></a>混合背包问题</h2><p>所谓混合背包的问题无非就是前面三种背包的杂糅操作，比如有的物品符合0-1背包（只能够取1件或者不取），有的物品符合完全背包问题（一件物品能够取任意件），有的物品符合多重背包问题（一种物品只能怪取限定件）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">伪代码：</span><br><span class="line">for(i = 0 ;i&lt;N;i++)&#123;</span><br><span class="line">if i属于0-1背包问题 </span><br><span class="line">采用0-1解决方法</span><br><span class="line">else if i属于完全背包问题</span><br><span class="line">采用完全解决方法</span><br><span class="line">else if i属于多重背包问题</span><br><span class="line">采用多重解决方法</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti12.jpg" alt=""></p><p>上述三种经典背包问题可谓了解动态规划的经典之作了，其实三种背包问题 都是有着异曲同工之妙呀，认真解读会让自己了解的更加深刻，舒服。其实关于背包问题的变形变异还有很多类似的题目，后续加以继续撸。。。鄙人不才，若有误望指正，本文章也是采取一些其他博客的思路，谢谢各路大神。</p><p>解决拥有子问题的问题，可以有三个方法</p><ul><li>朴素递归 （效率很低）</li><li>递归 + 记忆化 （效率较高）</li><li>递推完善dp数组（效率最高）动态规划常用</li></ul><p><strong><em>重点</em></strong>   dp数组的含义 + 状态转移方程 （具体问题具体分析）</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><p>[]: <a href="https://blog.csdn.net/woshi250hua/article/details/7636866">https://blog.csdn.net/woshi250hua/article/details/7636866</a></p><p>这位好心的博主列举了一些背包模型的例子，可以利于自己继续练习</p><p><img src="https://cdn.jsdelivr.net/gh/xiewende/blog_img/bag_problem/wenti21.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 动态规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
            <tag> 背包问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程中无穷大常量(ox3f3f3f3f)的设定技巧</title>
      <link href="%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B%E4%B8%AD%E6%97%A0%E7%A9%B7%E5%A4%A7%E5%B8%B8%E9%87%8F-ox3f3f3f3f-%E7%9A%84%E8%AE%BE%E5%AE%9A%E6%8A%80%E5%B7%A7/"/>
      <url>%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B%E4%B8%AD%E6%97%A0%E7%A9%B7%E5%A4%A7%E5%B8%B8%E9%87%8F-ox3f3f3f3f-%E7%9A%84%E8%AE%BE%E5%AE%9A%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h4 id="编程中无穷大常量的设定技巧"><a href="#编程中无穷大常量的设定技巧" class="headerlink" title="编程中无穷大常量的设定技巧"></a>编程中无穷大常量的设定技巧</h4><p>首先，在做某一些算法的时候，会很常求最大最小值一类的问题，通常我们会设置一个初始一个answer最大int类型的最大或者最小，然后每次比较取大取小即可。</p><p>其实如果问题中各数据的范围明确，那么无穷大的设定不是问题，在不明确的情况下，很多程序员都取0x7fffffff作为无穷大，因为这是32-bit int的最大值。如果这个无穷大只用于一般的比较（比如求最小值时min变量的初值），那么0x7fffffff确实是一个完美的选择，但是在更多的情况 下，0x7fffffff并不是一个好的选择。理由如下：</p><p>很多时候我们并不只是单纯拿无穷大来作比较，而是会运算后再做比较，例如在大部分最短路径算法中都会使用的松弛操作：<br>if (d[u]+w[u][v]&lt;d[v]) d[v]=d[u]+w[u][v];<br>我们知道如果u,v之间没有边，那么w[u][v]=INF，如果我们的INF取0x7fffffff，那么d[u]+w[u][v]会溢出而变成负数， 我们的松弛操作便出错了，更一般的说，0x7fffffff不能满足“无穷大加一个有穷的数依然是无穷大”，它变成了一个很小的负数。</p><p>计算机不会表示出“无穷大”的概念，所以我们只能以一个定值来表示“最大”。那么使用什么值呢？</p><p>32-bit int举例，我们选择的最大应该满足两个条件</p><ul><li><strong>这个最大值真的很大，是和定义的最大值是同一个数量级的</strong></li><li><strong>这个最大值+这个最大值并不会溢出的，也就是无穷大嘉无穷大依然是无穷大</strong></li></ul><p>所以我们需要一个更好的家伙来顶替 0x7fffffff ，最严谨的办法当然是对无穷大进行特别处理而不是找一个很大很大的常量来代替它（或者说模拟 它），但是这样会让我们的编程过程变得很麻烦。</p><p>在我看的大佬上面，最精巧的无穷大常量取值是 0x3f3f3f3f，我不知道是谁最先开始使用这个精妙的常 量来做无穷大，自己也是学以致用，你还别说发现非常好用，而当我对这个常量做更深入的分析时，就发现它真的是非常精巧了。</p><p> 第一、0x3f3f3f3f的十进制是1061109567，也就是10^9级别的（和 0x7fffffff一个数量级），而一般场合下的数据都是小于10^9的，所以它可以作为无穷大使用而不致出现数据大于无穷大的情形。</p><p>第二、由于一般的数据都不会大于10^9，所以当我们把无穷大加上一个数据时，它并不会溢出（这就满足了“无穷大加一个有穷的数依然是无穷 大”），事实上 0x3f3f3f3f + 0x3f3f3f3f = 2122219134，这非常大但却没有超过32-bit int的表示范围，所以 0x3f3f3f3f 还满足了我们“无穷大加无穷大还是无穷大”的需求。</p><p>第三、0x3f3f3f3f还能给我们带来一个意想不到的额外好处：如果我们想要将某个数组清零，我们通常会使用 <code>memset(a,0,sizeof(a))</code>这样的代码来实现（方便而高效），但是当我们想将某个数组全部赋值为无穷大时（例如解决图论问题时邻接矩阵的 初始化），就不能使用memset函数而得自己写循环了（写这些不重要的代码真的很痛苦），我们知道这是因为 memset 是按字节操作的，它能够对数组清 零是因为0的每个字节都是0，现在好了，如果我们将无穷大设为 0x3f3f3f3f，那么奇迹就发生了，0x3f3f3f3f 的每个字节都是0x3f！所 以要把一段内存全部置为无穷大，我们只需要<code>memset(a,0x3f,sizeof(a))</code>。所以在通常的场合下，0x3f3f3f3f 真的是一个非常棒的选择。</p><p>补充：memset以字节为单位进行填充，可以全部置为0，-1，和某个int类型的值四个字节都是一样的表示的数值，别问问就是巧合！！</p>]]></content>
      
      
      <categories>
          
          <category> 算法总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小tips </tag>
            
            <tag> 常量设定 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tcp详解</title>
      <link href="%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcp%E8%AF%A6%E8%A7%A3/"/>
      <url>%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/tcp%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、tcp协议的特点"><a href="#1、tcp协议的特点" class="headerlink" title="1、tcp协议的特点"></a>1、tcp协议的特点</h4><p>TCP是在不可靠的IP层之上实现的可靠的数据传输协议，它主要解决传输的可靠、有序、无丢失和不重复问题。TCP 是TCP/IP 体系中非常复杂的一个协议，主要特点如下：</p><ul><li><p>TCP 是面向连接的传输层协议。</p></li><li><p>每条TCP 连接只能有两个端点，每条TCP 连接只能是点对点的（一对一）。</p></li><li><p>TCP 提供可靠的交付服务，保证传送的数据无差错、不丢失、不重复且有序。</p><ul><li><p>如何保证数据无差错、不丢失、不重复且有序的？有哪些机制来保证？</p><p>答：TCP 使用了校验、序号、确认和重传等机制来达到这一目的。</p></li></ul></li><li><p>TCP 提供全双工通信，允许通信双方的应用进程在任何时候都能发送数据，为此TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据。</p><ul><li><p>为什么需要设置缓存，缓存的作用？ </p><p>答：发送缓存用来暂时存放以下数据：1.发送应用程序传送给发送方TCP 准备发送的数据；2.TCP已发送但尚未收到确认的数据。</p><p>接收缓存用来暂时存放以下数据：1.按序到达但尚未被接收应用程序读取的数据；2.不按序到达的数据。</p></li></ul></li><li><p>TCP是面向字节流的，虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅视为一连串的无结构的字节流。</p><ul><li>一个字节占一个序号，每个报文段用第一个字节的序号来标识,例如，一报文段的序号字段值是301, 而携带的数据共有l00B, 表明本报文段的数据的最后一个字节的序号是400, 因此下一个报文段的数据序号应从401开始，也就是期望的下一个序号（确认号）。</li></ul></li></ul><h4 id="2、tcp报文段格式"><a href="#2、tcp报文段格式" class="headerlink" title="2、tcp报文段格式"></a>2、tcp报文段格式</h4><p><img src="https://www.hualigs.cn/image/60a3a18937d44.jpg" alt=""></p><p>部分字段解释：</p><p>1) 序号字段（就是seq）：序号字段的值指的是本报文段所发送的数据的第一个字节的序号。</p><p>2) 确认号字段（就是ack）：是期望收到对方的下一个报文段的数据的第一个字节的序号。若确认号为N, 则表明到序号N-1为止的所有数据都已正确收到。（累积确认）</p><p>3) 确认位ACK：只有当ACK=1时确认号字段才有效。当ACK=0时，确认号无效。TCP 规定，在连接建立后所有传送的报文段都必须把ACK置1。</p><p>4) 同步位SYN。同步SYN=1表示这是一个连接请求或连接接收报文。当SYN= 1, ACK=0 时，表明这是一个连接请求报文，对方若同意建立连接，则在响应报文中使用SYN= 1, ACK=1。即SYN=1表示这是一个连接请求或连接接收报文。</p><p>5) 终止位FIN (Finish) 。用来释放一个连接。FIN=1表明此报文段的发送方的数据已发送完毕了并要求释放传输连接。</p><h4 id="3、tcp连接管理"><a href="#3、tcp连接管理" class="headerlink" title="3、tcp连接管理"></a>3、tcp连接管理</h4><p>TCP 是面向连接的协议，因此每个TCP 连接都有三个阶段：连接建立、数据传送和连接释放。TCP 连接的管理就是使运输连接的建立和释放都能正常进行。</p><p>在TCP 连接建立的过程中，要解决以下三个问题：</p><p>1) 要使每一方都能够确知对方的存在。</p><p>2) 要允许双方协商一些参数（如最大窗口值、是否使用窗口扩大选项、时间戳选项及服务质量等）。</p><p>3) 能够对运输实体资源（如缓存大小、连接表中的项目等）进行分配。</p><p>每条TCP 连接唯一地被通信两端的两个端点（即两个套接字）确定。 端口拼接到IP地址即为套接字，tcp的连接采用的是客户机/服务器方式，主动发起连接建立的应用进程称为客户机，而被动等待连接建立的应用进程称为服务器。</p><h4 id="4、tcp连接的建立"><a href="#4、tcp连接的建立" class="headerlink" title="4、tcp连接的建立"></a>4、tcp连接的建立</h4><p><img src="https://www.hualigs.cn/image/60a3a1bfa6710.jpg" alt=""></p><p>1) 第一次握手：客户机的TCP首先向服务器的TCP发送一个连接请求报文段。这个特殊的报文段中不含应用层数据，其首部中的SYN标志位被置为1。另外，客户机会随机选择一个起始序号seq = x（连接请求报文不携带数据，但要消耗一个序号）。</p><p>2) 第二次握手：服务器的TCP 收到连接请求报文段后，如同意建立连接，就向客户机发回确认，并为该TCP连接分配TCP缓存和变量。在确认报文段中，SYN 和ACK 位都被置为1, 确认号字段的值为x+1,并且服务器随机产生起始序号seq= y( 确认报文不携带数据，但也要消耗一个序号）。确认报文段同样不包含应用层数据。</p><p>3) 第三次握手：当客户机收到确认报文段后，还要向服务器给出确认，并且也要给该连接分配缓存和变量。这个报文段的ACK 标志位被置1, 序号字段为x+1, 确认号字段ack=y+1。该报文段可以携带数据，若不携带数据则不消耗序号 http中的tcp连接的第三次握手的报文段中就捎带了客户对万维网文档的请求 。成功进行以上三步后，就建立了TCP 连接，接下来就可以传送应用层数据。TCP 提供的是全双工通信，因此通信双方的应用进程在任何时候都能发送数据。</p><p><strong>【总结】</strong></p><ul><li>1) SYN = 1,ACK = 0,seq = x;</li><li>2) SYN = 1,ACK = 1,seq = y,ack = x+1;</li><li>3) SYN = 0,ACK = 1,seq = x+1,ack=y+1。</li></ul><h4 id="5、tcp释放连接"><a href="#5、tcp释放连接" class="headerlink" title="5、tcp释放连接"></a>5、tcp释放连接</h4><p><img src="https://www.hualigs.cn/image/60a3a1dfde76f.jpg" alt=""></p><p>1) 第一次握手：客户机打算关闭连接时，向其TCP发送一个连接释放报文段，并停止发送数据，主动关闭TCP 连接，该报文段的FIN 标志位被置1, seq= u, 它等于前面已传送过的数据的最后一个字节的序号加1 (FIN 报文段即使不携带数据，也要消耗一个序号）。TCP是全双工的，即可以想象为一条TCP 连接上有两条数据通路。发送FIN 报文时，发送FIN 的一端不能再发送数据，即关闭了其中一条数据通路，但对方还可以发送数据。</p><p>2) 第二次握手：服务器收到连接释放报文段后即发出确认，确认号是ack = u + 1, 而这个报文段自己的序号是v, 等千它前面已传送过的数据的最后一个字节的序号加1 。此时，从客户机到服务器这个方向的连接就释放了，TCP连接处千半关闭状态。但服务器若发送数据，客户机仍要接收，即从服务器到客户机这个方向的连接并未关闭。</p><p>3) 第三次握手：若服务器已经没有要向客户机发送的数据，就通知TCP释放连接，此时其发出FIN=1的连接释放报文段。</p><p>4) 第四次握手：客户机收到连接释放报文段后，必须发出确认。在确认报文段中，ACK字段被置为1, 确认号ack= w + 1, 序号seq= u + 1 。此时TCP连接还未释放，必须经过时间等待计时器设置的时间2MSL（最长报文段寿命）后，A才进入连接关闭状态。</p><p><strong>【总结】</strong></p><ul><li><p>1) FIN = 1,seq = u;</p></li><li><p>2) ACK = 1,seq = v,ack = u+1;</p></li><li><p>3) FIN = 1,ACK = 1,seq = w,ack =u+1;(确认第一次的u)</p></li><li><p>4) ACK = 1,seq = u+1,ack = w+1。</p></li></ul><p><strong>question one : 什么是SYN洪泛攻击？（三次握手机制有什么问题？）</strong></p><p>答：由于服务器端的资源是在完成第二次握手时分配的，而客户端的资源是在完成第三次握手时分配的，攻击者发送TCP的SYN报文段，SYN是TCP三次握手中的第一个数据包，而当服务器返回ACK后，该攻击者就不对其进行再确认，那这个TCP连接就处于挂起状态，也就是所谓的半连接状态，服务器收不到再确认的话，还会重复发送ACK给攻击者。这样更加会浪费服务器的资源。攻击者就对服务器发送非常大量的这种TCP连接，由于每一个都没法完成三次握手，所以在服务器上，这些TCP连接会因为挂起状态而消耗CPU和内存，最后服务器可能死机，就无法为正常用户提供服务了。</p><p><strong>question two :为什么不采用“两次握手”建立连接呢？</strong></p><p>答：这主要是为了<strong>防止两次握手情况下已失效的连接请求报文段突然又传送到服务器而产生错误</strong>。考虑下面这种情况。客户A 向服务器B 发出TCP 连接请求，第一个连接请求报文在网络的某个结点长时间滞留， A 超时后认为报文丢失，于是再重传一次连接请求， B 收到后建立连接。数据传输完毕后双方断开连接。而此时，前一个滞留在网络中的连接请求到达服务器B, 而B 认为A又发来连接请求，此时若使用“三次握手”，则B 向A 返回确认报文段，由于是一个失效的请求，因此A 不予理睬，建立连接失败。若采用的是“两次握手”，则这种情况下B 认为传输连接已经建立，并一直等待A 传输数据，而A 此时并无连接请求，因此不予理睬，这样就造成了B的资源白白浪费。</p><p><strong>question three :如果已经建立了连接，但是客户端突然出现故障了怎么办?</strong></p><p>答：TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。</p><p><strong>question four :为什么连接的时候是三次握手，关闭的时候却是四次握手?</strong></p><p>答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次握手。</p><p><strong>question four :为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？</strong></p><p>答：1)虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL(Maximum SegmentLifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。2)防止出现“已失效的连接请求报文段“（和上面的为啥不用二次握手类似）。A 在发送最后一个确认报文段后，再经过2MSL可保证本连接持续的时间内所产生的所有报文段从网络中消失.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tcp协议 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="all/hello-world/"/>
      <url>all/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
